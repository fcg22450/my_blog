{"pages":[],"posts":[{"title":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; &lt;– more –&gt;More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2022/11/30/hello-world/"},{"title":"关于一个spark项目的详细拆分解析","text":". Spark项目详细解析（该项目并未上传，系广汽新能源相关项目 PS： 关于这个功能中涉及到的一些知识，其实并没有太过复杂， kafka方面使用的是scala基础的kafka交互模块（org.apache.kafka） 数据库使用的java.sql直接编写sql语句进行数据库数据写入 逻辑模块更简单，基础的一些类型操作，for循环使用，if条件判断使用 比较好的一点是各个功能是分开独立的，方便与后期对功能判断条件的一些细微调整 相应的注释我都写在了代码之中，进行了一些简单的标注 spark相关配置 pom中配置较少，这边需要导入的只有一个mysql-connector-java的包 12345678&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.25&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 这次写的时候使用的环境是scala-2.12.12，spark-3.1.1（如果需要使用其他版本的spark的话，对应scala版本需要根据官网提供的文档进行选择，必须使用要求的环境版本，否则在一些模块的调用上将会出现问题） kafka与zookeeper环境没有什么要求，能用就行 代码部分 首先要做的是构建kafka数据消费函数， 用于从kafka中消费数据并把这些数据拿过来进行处理 直接上代码吧 12345678910val prop = new Propertiesprop.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;)prop.put(&quot;group.id&quot;, &quot;group01&quot;)prop.put(&quot;auto.offset.reset&quot;, &quot;earliest&quot;)prop.put(&quot;key.deserializer&quot;,Class.forName(&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;))prop.put(&quot;value.deserializer&quot;,Class.forName(&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;))prop.put(&quot;enable.auto.commit&quot;, &quot;true&quot;)prop.put(&quot;session.timeout.ms&quot;, &quot;60000&quot;)val kafkaConsumer = new KafkaConsumer[String,String](prop)kafkaConsumer.subscribe(Collections.singletonList(&quot;test&quot;)) 详细解读 new properties 首先我们需要构建一个变量用来存储我们的配置属性， 这里配置基本使用的是默认配置需要修改的地方也不多，建议根据自身需要修改一下 bootstrap.server,这个属性对应是我们后台搭建好的kafka的地址，可以使用本地路径，也可以使用外网路径，主要看自身的kafka配置情况，酌情使用 group.id 这个属性并不会有太大的影响，依然自行决定是否使用 session.timeout.ms 设置连接超时时间 new KafkaConsumer 创建一个新的kafka连接，需要传入我们准备好的配置属性（如上述代码） kafkaConsumer.subscribe(Collections.singletonList(“test”)) test为我们接下来要访问的topic的名称，这里我使用的是自己本机创建的一个名为test的topic，实际使用过程中根据实际情况来进行调整 继续上代码 12345678910while(true){ val msgs:ConsumerRecords[String, String] = kafkaConsumer.poll(3000) val it = msgs.iterator() while (it.hasNext) { // 读取kafka中保存的数据 val msg = it.next().value() val vSome = JSON.parseFull(msg) val vMap = vSome.get.asInstanceOf[Map[String,String]] }} 通过while循环来构建一个时刻执行的任务，来确保在获取到kafka数据的时候能够进行对应的处理，无数据产生时将会自动等待 kafkaConsumer.poll(3000) 将最新接收到的kafka数据抽出来，3000是设定超时时间timeout msgs.iterator 将获取到的数据转化为迭代器来进行下一步的迭代 hasNext 用于查看是否还有数据， 还有数据则继续循环，无数据则退出内层循环进入外层循环 使用next（）来迭代一条数据，我们要使用的数据包含在这个数据中的value中 使用JSON.parseFull方法来将我们拿到的数据转化为一个map类型的数据，这个数据目前还不能直接用，因为它自动的在map外面套了一层some来进行包装 所以我们还需要对这个some数据进行处理转化成一个map类型的数据 map类型的数据有点像字典，不过它的表现形式并不是python字典中的键值对（key：value）形式，而是类似于指针一样的表现形式，（key -&gt; value） 一个map类型的数据在定义的时候需要设定一下key和value的数据类型 如上述代码中， 我设定是数据类型为[String, String]，也就是key，value 的类型全部为string类型 完成这一步之后，我们就可以对这些数据来进行逻辑运算，书写我们之前设定好的规则了 考虑到这部分代码如果拿过来的话会比较多，所以就不进行完整的展示了，我会拿一种一部分来进行一个展示，因为这部分逻辑判断中，绝大部分都是这一小部分进行服用得到的，所以也没必要去全部看一遍 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081// 考虑到这部分相对来说比较多，就不写在外面了，直接在里面通过注释的形式来书写def AEB_Computational_logic(data:Map[String,String]): Map[String,String] = { // 构建一个list来存储这个功能涉及到的一些信号名，有些功能需要用到多个信号来进行判断，所以这里以嵌套List的方式来进行构建 val signal_list:List[Any] = List( List(&quot;SAS_SteeringAngle&quot;, &quot;SAS_SteeringAngleSpd&quot;), &quot;EMS_GasPedalActPstforMRR&quot;, &quot;MRR_AEBOffSt&quot;, List(&quot;BCS_VDCActiveSt&quot;, &quot;BCS_VDCOff&quot;), List(&quot;BCS_TCSActiveSt&quot;, &quot;BCS_TCSOff&quot;), &quot;BCS_HDCCtrlSt&quot;, &quot;BCS_VehSpd&quot;, &quot;MRR_AEBLVehHoldReq&quot;, &quot;BCS_VehSpd&quot;, &quot;IFC_CameraBlockageSt&quot;, &quot;VCU_CrntGearLvl&quot;, List(&quot;ADAS_SensorBlockedSts&quot;, &quot;ADAS_SensorFailure&quot;) ) // 定义最终返回的一个数据，这个数据同样使用map类型 var result_data:Map[String,String] = Map() // 这个地方的作用是用来接收结果，同时用来作为终止循环的条件，因为我们这里只需要考虑到一个条件出错即可，所以在已经获取到一个结果的情况下，后续的数据就不需要进行处理了 var cause_of_error = 0 // 开启循环，读取信号列表中的信号用来获取对应的数据 for(index &lt;- Range(0,signal_list.length) if cause_of_error == 0) { val signal = signal_list.apply(index) // 这里根据信号的数据类型来进行区分，正常信号的数据类型为String类型，但有一部分为List类型，所以我们需要先区分出来 if (signal.isInstanceOf[String]){ // 从数据流中获取对应信号的值，如果该信号不存在，则不进行处理 val signal_data:String = data.get(signal.toString).getOrElse(&quot;&quot;) if (signal_data!=&quot;&quot;){ // 对应获取到的数据进行切片转化成一个List类型并进行反转， 这里的反转是因为原定的数据给我们的是最新的在前面，而先获取到的数据在后面，所以我们需要把它进行一个反转操作，确保时间顺序是从左往右的 val signal_data_list = signal_data.split(',').toList.reverse // 根据信号的不同来对数据进行不同的条件判断 if (signal == &quot;EMS_GasPedalActPstforMRR&quot;){ // &gt;85% // 这里使用的exists方法的作用是对列表中的每一项进行判断，只要有一项满足即返回True，并修改cause_of_error的值为对应的错误原因编号， 该原因编号根据数据库中确定好的规则表来进行填写，当规则表发生重大变动时，则需要来这里进行对应的调整 if(signal_data_list.exists(x =&gt; x.toInt &gt; 85) || (signal_data_list.apply(signal_data_list.length-1).toInt - signal_data_list.apply(0).toInt).abs &gt; 200){ cause_of_error = 2 } }else if(signal == &quot;MRR_AEBOffSt&quot;){ if (signal_data_list.exists(x =&gt; x == &quot;1&quot;)){ cause_of_error = 3 } } } } // 对多条件的错误原因进行处理 else{ // 使用自带的asInstanceOf对格式进行一下转化，因为这边无法自动判断出signal是什么格式，默认是any， 而any格式是不能使用apply方法的 val this_signale_list:List[String] = signal.asInstanceOf[List[String]] val data_1 = data.get(this_signale_list.apply(0).toString).getOrElse(&quot;&quot;) val data_2 = data.get(this_signale_list.apply(1).toString).getOrElse(&quot;&quot;) // 这边如果还是通过信号判断的话可能不太准确，所以这里直接使用索引来进行区分，相较于单个信号，多信号的判断相对来说比较绕，费神一点，不过还好，基本的逻辑和用法都大差不差 if (index == 0){ if (data_1 != &quot;&quot; &amp;&amp; data_2 != &quot;&quot;){ val data_3 = data_1.split(',').toList.reverse val data_4 = data_2.split(',').toList.reverse if(data_3.exists(x =&gt; x.toInt &gt; 60) || data_4.exists(x =&gt; x.toInt &gt; 150)){ cause_of_error = 1 } }else if(data_1 != &quot;&quot;){ val data_3 = data_1.split(',').toList.reverse if(data_3.exists(x =&gt; x.toInt &gt; 60)){ cause_of_error = 1 } }else if(data_2 != &quot;&quot;){ val data_4 = data_2.split(',').toList.reverse if(data_4.exists(x =&gt; x.toInt &gt; 150)){ cause_of_error = 1 } } } } } // 在完成了上述的判断之后，我们得到了一个cause_of_error，将这个参数和功能编号一起写入到我们之前定义的result_data中去，使用return来返回 result_data += (&quot;function&quot; -&gt; &quot;1&quot;) result_data += (&quot;reason&quot; -&gt; cause_of_error.toString) return result_data } 完成了上述部分之后， 基本可以说已经完成了大部分的工作，接下来要做的就是对我们要存储的一些没有涉及到计算的数据进行添加 这部分的工作并不多 1234567891011121314151617181920212223 // 基础字段处理var base_data: Map[String,String] = Map()// 车辆vin码base_data += (&quot;vin&quot; -&gt; vMap.get(&quot;vin&quot;).getOrElse(&quot;&quot;))// 信号录制时间base_data += (&quot;time&quot; -&gt; vMap.get(&quot;sampleTime&quot;).getOrElse(&quot;&quot;))// 车系base_data += (&quot;vehicle_series&quot; -&gt; vMap.get(&quot;vehicleSeries&quot;).getOrElse(&quot;&quot;))// 车型base_data += (&quot;vehicle_model&quot; -&gt; vMap.get(&quot;vehicleModelCode&quot;).getOrElse(&quot;&quot;))// 经度base_data += (&quot;TEL_LongitudeDeg&quot; -&gt; vMap.get(&quot;vehicleModelCode&quot;).getOrElse(&quot;&quot;))// 纬度base_data += (&quot;TEL_LatitudeDeg&quot; -&gt; vMap.get(&quot;vehicleModelCode&quot;).getOrElse(&quot;&quot;))// 省份//城市// 县，区//刹车踏板base_data += (&quot;brake_pedel_status&quot; -&gt; vMap.get(&quot;VCU_EMS_BrkPedalSt&quot;).getOrElse(&quot;&quot;)) 基本上就是获取一下这些固定的数据，这里面需要进行处理的数据只有这个经纬度、省市县、刹车踏板这几个数据，处理起来比较轻松 最终我们得到了一个base_data，里面会包含除了id，功能名称，错误原因三个意外的全部字段数据 当然，这部分高德经纬度处理的东西正在研究怎么整，因为深入看了一下代码之后发现kafka传过来的经纬度数据还需要处理，他是分成度分秒来传回来的，需要对这些数据进行拼接之后才能使用 完成这些之后， 我们需要对逻辑函数返回的错误条件与基础的base_data里面的数据加到一起，使用的是map数据相加的方法 ++ 当然， 在++之前记得判断一下返回的错误条件的情况，按照之前的设置， 如果没有出现错误，返回的结果中reason的值为0（这里的0为字符串的0，注意别弄错了） 完成数据准备之后就到了我们的数据添加部分，这部分负责将我们的数据添加到数据库表中去 首先是构建数据库连接，开头的时候已经说过， 我们用的是最基础的mysql-connetcor-java包进行的连接 1234567val url: String = db_urlval driver: String = &quot;com.mysql.cj.jdbc.Driver&quot;val username: String = &quot;root&quot;val password: String = &quot;rbac2021&quot;var conn: Connection = nullconn = DriverManager.getConnection(url, username, password)val st = conn.createStatement 上述为基础的mysql连接方式，基本上各种语言中的连接都大同小异 构建添加数据方法 1234567891011121314151617181920212223242526272829def sql_conn_create(data: Map[String,String]): Unit = { try { Class.forName(driver) val sql = &quot;&quot; + &quot;insert into function_abort_reason&quot; + &quot;(&quot; + &quot;vin, vehicle_series, vehicle_model, config_vehicle, park_vehicle, abort_time, &quot; + &quot;longitude, latitude, province, city, county,function, reason, speed,hrake_pedel_status,related_signals) &quot; + &quot;values(&quot; + s&quot;${vin},&quot; + s&quot;${vehicle_series},&quot; + s&quot;${vehicle_model},&quot; + s&quot;${config_vehicle},&quot; + s&quot;${park_vehicle},&quot; + s&quot;${abort_time}, &quot; + s&quot;${longitude}, &quot; + s&quot;${latitude}, &quot; + s&quot;${province}, &quot; + s&quot;${city}, &quot; + s&quot;${county}, &quot; + s&quot;${function}, &quot; + s&quot;${reason}, &quot; + s&quot;${speed}, &quot; + s&quot;${hrake_pedel_status}, &quot; + s&quot;${related_signals} &quot; + &quot;)&quot; val rs = st.executeQuery(sql) } 感觉这个也没啥好讲的。。。使用格式化输入的方式来写一个sql语句出来，最后使用executeQuery方法添加数据，完事 PS-1：以上就是关于部分功能的具体逻辑， 后续的测试需要等待经纬度方面的处理代码写好之后才能进行，目前正在看这方面的文章，姑且就这些吧PS-2: 这部分的代码不算太完善，并没有涉及到spark方面的深度使用，说实话对于这个框架依然很不熟练，基本上写的过程中都是一边写一遍查询遇到的问题PS-3： 那么就这些吧，有什么有疑问的地方随时找我","link":"/2022/11/30/Study-notes-spark%E9%A1%B9%E7%9B%AE%E7%9B%B8%E5%85%B3%E6%B3%A8%E9%87%8A/"},{"title":"wf-lol（半成品）","text":"无聊写的，随便看看吧。 “天象在涌动。” 瑞兹抱着他的卷轴，双眸之中，倒映着夜空中那颗璀璨的流星，“没有侦测到符文能量，但是，那是什么东西？” “必须加快进度了。”紧了紧背后的大卷轴，流浪的奥术师再次踏上了他的旅途。 。。。。。。 这颗流星相当的璀璨，而且直坠入恕瑞玛————古老文明的残余荒漠。几乎所有观测到这颗流星的人都闻风而动。流星，往往以为着财富。天外的金属、珍惜的矿物，毫无疑问，无论是商人还是佣兵，都不会拒绝这样的一个宝物。沙漠这么大，说不定被自己找到呢？怀揣着这样的想法，大量的雇佣兵、商人汇聚而来。诺克萨斯同样得知了天降陨石的消息————他们的战争石匠几乎无处不在————考虑到自身正处于与艾欧尼亚开战的节骨眼上，最终被派过来的，是正在竞技场丢斧子的德莱文————优秀的清算者，战斗力也还算是不错，同样带来的， 还有他手下的一批崇拜者组成的飞斧手队伍————300人的规模，在这种冷兵器的战场上，也算是极其强大的兵力了。而此时此刻，恕瑞玛的深处，布满了地瘤艾卡西亚地底。久违的光从穹顶的大洞射入，让卡莎有些难以适应。长期的地底生活让她觉得阳光颇为刺眼，不过此时此刻，比阳光更加刺眼的，是面前的撞击坑中的东西。五分钟前，流星坠落在了艾卡西亚，强大的冲击力击破了本就已经被虚灵啃食出空洞的岩层，坠入了这个地底世界。五分钟的时间并不足以令流星完全冷却下来，不过，连番的撞击以及大气层的摩擦，已经将原本包裹在核心外的不明物质剥离干净，此时此刻，呈现在卡莎面前的，是一个如同蚕蛹一般紧闭的银灰色金属物体，正在向周围的空气肆无忌惮的释放着自身的热量。卡莎小心的避开仍在散发着高温的陨石碎片，靠近了过来，朝上的位置，是一块明黄色的半透明晶体，透过晶体，隐约可以看到平躺于其中的身影。人型，但身体表面似乎包覆着一层奇异的暗红色甲壳，关节处还有着向外伸展的尖刺。也就是看到这个人型的瞬间，卡莎的伴生甲壳，突然骚动了起来。亢奋的情绪在传导，激烈的欲望在蔓延，向着卡莎传递着细碎的信息。想要！！饥饿！！吞掉他！！皱了皱眉头，早已在无穷尽的虚空侵蚀中无比坚韧的精神强行将骚动的甲壳压制住，转而开始在其表面摸索着，寻找着可能存在的开关。单凭自己的体力，将这个明显不轻的大块头背走慢慢研究有点不太现实，还是试试直接打开吧。正此时，窸窸萃萃的声音逐渐在地底中变大，黑暗的地底中，有微弱的紫色光芒闪动了起来，并逐渐从四面八方汇聚而来。已经在地底呆了多年的卡莎很清楚，这是游荡于艾卡西亚的梦魇，虚空的爪牙，最低级的虚灵，诞生于虚空之中的他们，是虚空大军中最初级的兵种，也是那些特殊的虚空生物进化的起点，而在地底世界的时光中，她已经杀了很多这样的虫子。不过现在，卡莎并不像和它们起冲突。皱了皱眉头，伴生甲壳在卡莎念头升起的一刻便已经覆盖住了她的头颅，同时双臂之上，紫色的光刃延伸而出，强行刺入了面前的休眠舱的水晶盖子中，猛力一划。“嗤……”漏气的声音，伴随着休眠舱内侧闪烁着的红光，卡莎一把将里面的人捞起，紫光闪动间，消失在了黑暗之中。留在原地的休眠舱闪烁了一阵之后，便仿佛能量耗尽一般，暗淡了下来，不再有反应。虫群涌入搜寻一阵之后，终于散去。……卡莎并没有走多远。从刚刚将其从休眠舱拎出来之后，卡莎便敏锐的察觉到，对方开始活动了起来，不再是如之前一般的熟睡状态，更像是一种深度睡眠之后试图醒过来的迷蒙的状态。于是卡莎直接找了一处封闭的溶洞钻了进去，将其放在中间，自身则隐入了黑暗之中，远远的观察着。……白堂感觉自己好像喝醉了一样，但又好像是困倦到极致的早起，整个人都处于一种醒了，但没有完全醒过来的混沌状态。他只觉得自己在最初醒过来的时候似乎是被人拦腰抱着跑，过了一会直接被丢在了地上，迷迷糊糊之间，白堂只看到了隐约的石壁。意识在恢复，触觉清楚的传递了回来，告诉白堂他现在正躺在地上，而且是石头地面。似乎是光溜溜的贴在地上。白堂打了个激灵，愈发清醒了。一手拍头一手撑地，白堂坐了起来，努力的睁开眼睛，扫视了一圈。奇怪的灰白色视角，似乎是夜视，也就是说周边应该很暗。然后，手的触感有点不对劲。仿佛宕机的脑袋重启，白堂开始检查自己的身体情况。覆盖全身的甲壳，并非是完全长在皮肤上，而是类似衣服的存在，内里甚至是白堂很眼熟的内裤，但很奇怪的拥有着皮肤一般的触感。同款的头盔，目前附带的功能只有夜视这一条，不排除别的功能存在的可能性，至于防御力，白堂不太确定。这身甲壳乍一看防御力应该还可以，但毕竟算是衣服，应该不容乐观。倒是整体来看，衣服表面凹凸不平仿佛没有皮肤包裹的肌肉一般的造型让白堂越看越是疑惑，因为实在是太眼熟了。眼熟到之前的白堂几乎每天都会看到————虽然是隔着屏幕。如果白堂没认错的话，身上穿的这身，应该是某个游戏里面的一套装饰性的衣服，也就是俗称的皮肤，而且，这套皮肤的来源是季票奖励获取的。使徒套装！这样的话，排除他人的恶作剧，唯一的可能性就是，他白堂，穿越了？还是穿的自己经常玩的游戏的世界？那这个明显是地底的地方又是哪里，克隆尼的地下矿洞吗？战甲呢？武器呢？茫然的挠了挠头，白堂只好先看向了视线所及的唯一一个生物————躲在石头后面的卡莎。头盔带来的夜视效果在面对生物时似乎顺便接收到了生物的红外线，以至于白堂毫不费力的就看到了红外线勾勒出曲线的卡莎。“喂~那边的小姐，这里是哪里啊？”想了想，白堂索性直接开口喊了，毕竟自己刚刚迷迷糊糊明显是被人搬过来的，周围又只有她一个，应该不至于对自己下手，毕竟昏迷的时候已经有的是时间了。“你是什么人？”卡莎在白堂开口后并未放松警惕，反而架起了背上的虚空荚囊。虽然渴望回到地面上，回到人类世界，但多年的地底厮杀依然让她保持着极大的警惕性。“咦？是中文啊！”白堂倒是高兴起来，对方的中文明显字正腔圆，“我是白堂。”“是你带我来这里的吗？这里是哪里啊？”卡莎点了点头：“这里是艾卡西亚的地底，我在陨石坑中捡到了你。”“艾卡西亚？”挠了挠头，又是一个熟悉的名字，“符文大陆？”“是的。”“这可真是……英雄联盟啊（小声嘀咕）“”那个，虽然有点冒昧，可以带我去你说的陨石坑吗？“白堂挠了挠头，顺便摘下了头盔。虽说这个头盔带着破有安全感，但该有的憋闷还是有的，更何况，摘下头盔也算是获取信任的一种方式，以免对方以为自己还在准备攻击，毕竟这身衣服的表象，还是颇为狰狞的。实际上，看到白堂摘下头盔露出来的有点稚嫩的清秀脸庞，卡莎确实是松了口气，毕竟，相比起只有人型的生物，人类的内力还是很友善的，尤其是在这样暗无天日的地底世界。于是，俩人开始返回最开始的地方。在地底寻路这件事，卡莎可谓是轻车熟路，一路上，白堂也询问了一下陨石坑的情况。如果猜测正确的话，那枚陨石应该是迫降的指挥官休眠舱，里面也许会残留一些其他的线索。而白堂想要的，就是这些可能存在的线索。“到了。”陨石降落点并不远。两人到达这里时，虚灵早已经散去，原地只留下沉寂的休眠舱和遍地的足迹————虚灵的四肢如同刀锋一般尖锐，以至于在地面上爬行时留下了一个个浅浅的凹坑。戴上头盔扫视了一圈，确认没有生命迹象之后，白糖靠了过去，手放在了休眠舱上。下一刻，仿佛系统被激活了一般，蔚蓝色的光亮蔓延而出，然后，休眠舱化为光芒消失了。“……”愣了一愣，白堂忽然感觉体内多了什么，本能驱使之下，白堂触动了一下体内多出来的东西。光芒再次亮起，不过这一次是在白堂的身上。而此刻，白堂的意识，已经进入了一处奇特的场景。是记忆！ 恢弘而高耸的建筑面前，一个和白堂一模一0样的人跪坐在金色的台座之上，头颅微垂，不同于白糖身上的使徒套装，这个人身上穿着的，是一套有着繁杂金色装饰的华美铠甲，优美而不失峥嵘，面部佩戴着的，是遮挡住眼睛的半透明水晶面罩，面罩之后，亮金色的眸子闪烁着耀眼的光芒。 “恭喜你，白糖，今天，我以平等的身份来向你致意。” prime版白堂的对面，同样有一个金色的台座，跪坐于其上的，正是游戏中白堂的导师，奥罗金侍卫————泰辛》“祥麟瑞凤现，今天真是个好日子啊。”“你现在已经可以在任何中继站使用至崇宗师祝福之泉，并祝福在场的所有人，为他们送上增益，让他们也能追随你的步伐，迈向宗师之路。”“我很荣幸，为你主持至崇宗师升格典仪。”“我的弟子啊。”“我的荣幸，老师。”声音逐渐模糊，白堂已经无法看清后续的事情，周围的一切光怪陆离起来，伴随着怪异的嗡鸣声，愈发的扭曲。然后，白堂的意识回归了体内，然后，便看到了右手上多出来的东西。“灼烂歼鬼 · 炮！”增幅器的信息出现在白堂的脑海之中，让他不由得捂脸，中二期起的名字现在看可真是太羞耻了。这东西，正是白堂游戏中获得的第一把自制增幅器————223，在当初游戏更新陷入停滞的时代，这把增幅器，可是当时版本的天花板。抖了抖手，增幅器化作光点融入了手臂，白堂看了看旁边的卡莎：“好了，我搞定了。”“你有啥想问的吗？”白堂早就看到卡莎在增幅器出现时欲言又止的表情了。“你是……什么？”犹豫了一下，卡莎问出了自己憋着的问题，“是人类吗？”“我的话，大概算是旅行者吧……”挠了挠头，白堂自己也有点不确定，现在能确认的是自己确实穿越了，而且之前记忆闪回看到的也是游戏里的剧情，只是奇怪的是原本的游戏角色的脸居然是自己的，而且他从来没有这段记忆。9身边唯一与游戏相关的东西就只有身上的使徒套装和刚刚拿到的增幅器了。“总之可以确认的是，我之前并不是符文大陆的，不过，我对于符文大陆也算是有一定的了解。”毕竟老召唤师了。“是嘛。”卡莎也不知道该说啥，对于她而言，唯一在意的，也就是毁掉村庄的虚空先知的手下，村庄坠入地底的恐惧，孤立无援的绝望，令卡莎几乎只剩下了复仇的欲望，也导致了，第一次遇到陌生人的卡莎无比的茫然无措。肤甲的存在让卡莎一直不敢靠近人类，即使她早已经有能力能够离开地底世界，重返人间。白堂同样有点不知道该去干啥，毕竟自己刚刚确认自己是个穿越者，索性这个世界并不是现实世界，不然还得先想办法搞定身份问题。虽然很想搞清楚穿越的问题还有之前闪回的记忆片段的由来，但毕竟完全没有头绪，只能走一步看一步了，第一次记忆片段的激发源自于休眠舱，下一次触发就不清楚了，携带休眠舱的陨石在飞行过程中基本上已经燃烧殆尽，没有利用价值了。原本游戏中的战甲和圣装倒是一个没看见，衣服只有身上的使徒。","link":"/2022/11/30/Inspiration-%E6%98%9F%E9%99%85%E6%88%98%E7%94%B2-001/"},{"title":"哪吒","text":"永劫无间英雄设计大赛 - 哪吒 英雄：哪吒 职业：仙 背景： 哪吒本立于天上。世间神仙大抵如此，兢兢业业苦修一世，终究是为了立于天上。即使世人皆知，天条森严，天上的生活，依然为人向往。哪吒其实并不喜欢天上，并不仅仅是因为他太年轻，性格跳脱，更重要的是，身在天庭，总归有些东西得忍着，有些安排，得受着。 “威灵显赫大将军何在？” ”聚窟洲反魂花开，阴神隐现，恐生变故，故着令汝下界查探，若有变故，可先斩后奏。“ ”哪吒领命。“ 无聊的”出差“任务。但总归好过在天上发呆。 于是哪吒便来了。 ”天庭有令，霍乱天下者，杀无赦！“ ”大胆妖孽，敢在本座面前放肆！“ ”杀手？鼠辈而已！“ ”秃驴！再敢碍事，本座送你上西天见如来！“ ”喂！那个蛮子，你也用火？来， 让我看看。“ 技能： f1：乾坤圈 哪吒将他的乾坤圈扔向目标，在击中敌人或物品后会弹跳最多5次后返回，火圈会对碰到的敌人造成火焰伤害，并标记对方10秒，被标记的敌人会收到60%额外伤害，并在死亡时有50%几率掉落一个回复50%生命力的生命球/50%几率掉落一个回复25%奥义能量的能量球 乾坤圈飞行过程中轻按技能键可传送至乾坤圈所在位置，如果该位置在墙内或者离墙过近，则无法传送。 f2：乾坤圈-风火飞轮 哪吒将他的乾坤圈扔向目标，在击中敌人或物品后会弹跳最多5次后返回，火圈会对碰到的敌人造成火焰伤害，并标记对方10秒，被标记的敌人会收到50%额外伤害，并在死亡时有50%几率掉落一个回复30%生命力的生命球/50%几率掉落一个回复15%奥义能量的能量球. 长按F健可以为乾坤圈蓄力，提高额外火焰伤害，并且不再弹跳，而是以直线往返飞行，飞行无视阻碍 乾坤圈飞行过程中轻按技能键可传送至乾坤圈所在位置，如果该位置在墙内或者离墙过近，则无法传送，激活传送时，会在落点产生爆炸，并造成火焰伤害。 f3：乾坤圈-无尽飞轮 哪吒将他的乾坤圈扔向目标，在击中敌人或物品后会弹跳最多5次后返回，火圈会对碰到的敌人造成火焰伤害，并标记对方10秒，被标记的敌人会收到50%额外伤害，并在死亡时有50%几率掉落一个回复30%生命力的生命球/50%几率掉落一个回复15%奥义能量的能量球。集中目标后，乾坤圈将会分裂成两个，最多分裂三次。 长按F健可以为乾坤圈蓄力，提高额外火焰伤害，并且不再弹跳，而是以直线往返飞行，飞行无视阻碍，蓄力乾坤圈无法分裂。 乾坤圈飞行过程中轻按技能键可传送至乾坤圈所在位置，如果该位置在墙内或者离墙过近，则无法传送，激活传送时，会在落点产生爆炸，并造成火焰伤害。 当发生分裂时，将无法使用传送。 v1：混天绫 召唤一个火焰圆环围绕自己旋转。火环基础生命值为500，在此基础上，会增加哪吒全部护甲值的175%额外生命值。在激活之后，火环将会使哪吒无敌三秒，在此期间，哪吒受到的任何伤害都会按照250%的倍率附加到混天绫的生命值中去，进入哪吒1.25米范围的敌人将会被环绕的混天绫触碰导致踉跄并且每秒受到100点的物理伤害。混天绫存在期间，哪吒所受伤害的90%将会由混天绫吸收，且在此期间，哪吒将获得对异常状态的完全免疫，包括燃烧、中毒、跌倒、击飞等。混天绫生命值耗尽后，会保有1s的无敌时间，之后技能进入冷却。 v2： 召唤一个火焰圆环围绕自己旋转。火环基础生命值为500，在此基础上，会增加哪吒全部护甲值的175%额外生命值。在激活之后，火环将会使哪吒无敌三秒，在此期间，哪吒受到的任何伤害都会按照250%的倍率附加到混天绫的生命值中去，进入哪吒1.25米范围的敌人将会被环绕的混天绫触碰导致踉跄并且每秒受到100点的物理伤害。混天绫存在期间，哪吒所受伤害的90%将会由混天绫吸收，且在此期间，哪吒将获得对异常状态的完全免疫，包括燃烧、中毒、跌倒、击飞等。混天绫生命值耗尽后，向外发射一圈热量，对周围的敌人造成火焰伤害，并使哪吒无敌1秒，之后技能进入冷却。 若技能发动期间承受的最后一次伤害足以将哪吒杀死（伤害的10%大于哪吒的当前护甲和伤害减免后的实际有效生命值），那么无敌效果便不会发动，而哪吒也会直接进入濒死/死亡状态。 v3： 召唤一个火焰圆环围绕自己旋转。火环基础生命值为500，在此基础上，会增加哪吒全部护甲值的175%额外生命值。在激活之后，火环将会使哪吒无敌三秒，在此期间，哪吒受到的任何伤害都会按照250%的倍率附加到混天绫的生命值中去，进入哪吒1.25米范围的敌人将会被环绕的混天绫触碰导致踉跄并且每秒受到100点的物理伤害。混天绫存在期间，哪吒所受伤害的90%将会由混天绫吸收，且在此期间，哪吒将获得对异常状态的完全免疫，包括燃烧、中毒、跌倒、击飞等。混天绫生命值耗尽后，会保有1s的无敌时间，之后技能进入冷却。 哪吒可以提前释放混天绫以召唤火尖抢，破地而出的长矛会将附近的敌人挑起，在此期间，敌人无法移动或者攻击，再次释放技能会将敌人拍回地面，并造成击倒效果","link":"/2022/11/30/Inspiration-%E6%B0%B8%E5%8A%AB%E6%97%A0%E9%97%B4%E8%8B%B1%E9%9B%84%E8%AE%BE%E8%AE%A1-%E5%93%AA%E5%90%92/"},{"title":"射命丸文","text":"永劫无间英雄设计大赛 - 射命丸文 英雄：射命丸文 种族：鸦天狗 背景：平常在山里居住。喜欢小道消息、总是购买信息、和天狗们交换情报。情报每次经过交换都会有所改变、最后大部分传言都会变为大话。天狗中有很多喜欢收集幻想乡女孩子的传言。因为可以自己去观察，也可以从同伴那里获得等，所以没有新情报的情况几乎就不存在。只是这些情报里混杂了大量谎言 [2] 。不论人类或是妖怪，如果有机会写成报导，她就会以绅士的态度进行采访；但如果有人阻挠，她就会以强硬姿态排除。近期反魂花将开，文文追寻着反魂花香而来，准备搞个大新闻。 伴生武器： 团扇 技能： f1：天狗的立风露 在空中利用交叉风制造立足点，并借由例子点向任意方向再次跳跃，并向立足点下方释放跳跃产生的反作用力进行攻击。 f2：天狗的下爆流 缠绕着风迅速向下踢打进攻，着陆时产生风流吹飞周围的任何东西。 f3：天狗的太鼓 从所在场所跳跃并用力踩向敌人，命中之后将会向后跳跃。 v1：疾走风靡 缠绕着风向前方进行高速突击，由于以风做庇护所以移动过程中会偏移远程攻击。 v2：疾走优美 疾走风靡的变形版，降低了速度，取消了偏转远程攻击的能力，但是可以自由转换进攻方向 v3：镰风面纱 在自己周围缠绕出风之铠的特殊技能，铠会跟随文文一起移动，当接触到敌人时会造成伤害","link":"/2022/11/30/Inspiration-%E6%B0%B8%E5%8A%AB%E6%97%A0%E9%97%B4%E8%8B%B1%E9%9B%84%E8%AE%BE%E8%AE%A1-%E5%B0%84%E5%91%BD%E4%B8%B8%E6%96%87/"},{"title":"小野冢小町","text":"永劫无间英雄设计大赛 - 小野冢小町 英雄：小野冢小町 种族：死神 背景：小町是三途河的船夫，她的工作是将死者从此岸（现世），运送到阎魔所在的彼岸，根据她心情的变化，河的宽度和深度也会发生相应的变化。死神是阎魔的部下，小町是负责幻想乡的四季映姬·亚玛撒那度的部下。因此她的船运送的人，也大多是幻想乡的人类和妖怪。反魂花的开放影响了死者的转生，小町特意过来解决这个问题。 伴生：镰刀 技能： f1：死歌-八重雾中渡 用满载幽灵的船撞向对手（指向性技能） f2：舟符-宛若河流般 召唤出少许三途河的水，并利用其驾船向别人突进的豪爽技能。（突进技） f3：地狱-无间之狭间 斩击敌人的同时创造出一个能阻止对手横向移动的区域。效果时间内小町便可以完全掌握住距离。（掌控距离即为自身对敌人而言是最远距离，敌人对自身而言是最近距离） v1：命符-余命无几许 削减敌人一半当前最大体力的恐怖技能。可见越早命中效果越大。 v2：魂符-生魂流离之镰 放出怨灵汇集而成的刀刃，切到敌人就会飞出带有对方生命力的怨灵，小町可以回收之以间接吸收体力。 v3：命符-“不惜身命，可惜身命” 前所未闻的可以交换双方体力的技能。无论多么紧迫的危机也可能在一瞬之间扭转为优势。","link":"/2022/11/30/Inspiration-%E6%B0%B8%E5%8A%AB%E6%97%A0%E9%97%B4%E8%8B%B1%E9%9B%84%E8%AE%BE%E8%AE%A1-%E5%B0%8F%E9%87%8E%E5%86%A2%E5%B0%8F%E7%94%BA/"},{"title":"行者","text":"永劫无间英雄设计大赛 - 行者 英雄：行者 种族：人族 背景：行走于大地之上的苦修者，沉默寡言，内心坚如磐石，信奉大地的力量。 技能： f1：山崩 行者以大地之力打出的一拳，攻击两米内的一个敌人，并对敌人身体周围两米内的其余敌人造成50%溅射伤害。 技能冷却时间：23秒 被石化的敌人会在击败之后掉落一块碎石，捡起碎石会获得石质护甲，每块碎石增加50护甲值，当护甲值堆叠到400时，使用山崩将没有CD。 f2：山崩 - 全力一击 技能冷却时间：25秒 行者以大地之力打出的一拳，攻击两米内的一个敌人，并对敌人身体周围两米内的其余敌人造成50%溅射伤害。 被石化的敌人会在击败之后掉落一块碎石，捡起碎石会获得石质护甲，每块碎石增加50护甲值，当护甲值堆叠到400时，使用山崩将没有CD。 山崩将获得额外附加伤害，伤害数值由当前选择的武器以及攻击性魂玉加成来变化 f3：山崩 - 大地之力 技能冷却时间：25秒 行者以大地之力打出的一拳，攻击两米内的一个敌人，并对敌人身体周围两米内的其余敌人造成50%溅射伤害。 被石化的敌人会在击败之后掉落一块碎石，捡起碎石会获得石质护甲，每块碎石增加50护甲值，当护甲值堆叠到400时，使用山崩将没有CD。 山崩的突进会留下一道持续12秒的痕迹，可以石化敌人3秒。 v1：聚沙成壁 行者重塑周围的地面，并在她面前堆砌出一道大地壁垒来抵御来自前方的攻击，再大地壁垒被制造出来后，他将会拥有2000点基础生命值，行者护甲值的500%将会计入大地壁垒的生命值中去。 再次激活时，行者会锤击面前的壁垒， 并将其变成一个滚石，使其滚向准星所指方向，滚石会对撞到的敌人造成伤害，滚石超过15m或者触碰到场景中的障碍物时，会直接爆炸，对周围两米的敌人造成伤害 v2：聚沙成壁 - 裂缝 行者重塑周围的地面，堆砌出三道大地壁垒，但是无法变成滚石 v3：聚沙成壁 - 裂缝 行者重塑周围的地面，堆砌出一道大地壁垒。使用山崩攻击大地壁垒，将会向前方扇形区域释放溅射伤害，波及距离10m，伤害随山崩变化，该伤害在释放时会消耗大地壁垒的生命值。","link":"/2022/11/30/Inspiration-%E6%B0%B8%E5%8A%AB%E6%97%A0%E9%97%B4%E8%8B%B1%E9%9B%84%E8%AE%BE%E8%AE%A1-%E8%A1%8C%E8%80%85/"},{"title":"西行寺幽幽子","text":"永劫无间英雄设计大赛 - 西行寺幽幽子 ​ 英雄：西行寺幽幽子 ​ 种族：幽灵 ​ 背景：华胥的亡灵，瀛洲冥界白玉楼的西行寺家的大小姐，是罕见的长寿亡灵。因为她并未心怀怨恨，同时同于幽灵的能力收到当地阎罗王赏识，因此奉命管理幽灵。交换条件是她可以永远住在冥界。注：据其本人所言，似乎是因为过于饥饿不小心吃了好多幽魂，因此被阎罗王赶出来了，真是个任性的幽灵啊。 ​ 技能部分： ​ f1: 生者必灭之理-死蝶： ​ 发动后召唤大量蝴蝶环绕翩翩起舞的符卡，蝴蝶会自动攻击周围敌人并造成少量伤害。 ​ 发动后无法使用武器攻击。 ​ 按下左键触发分支技能：埋骨于弘川-神灵（蝴蝶数量呈现爆发性增长） ​ 按下右键出发分支技能：黄泉平阪行路（打开与冥界沟通的简易门进行短距传送，传送结束后在落脚点召唤三只幽灵对周围的敌人发动一次攻击） ​ f2: 生者必灭之理-毒蛾： ​ 发动后召唤大量蝴蝶环绕翩翩起舞的符卡，蝴蝶会自动攻击敌人并造成少量伤害，附带毒素效果。 ​ 发动后无法使用武器攻击。 ​ 按下左键触发分支技能：凤蝶纹的死枪（从背后张开扇子，向前方乱射出光束和蝶弹） ​ 按下右键出发分支技能：蝶之羽风暂留此世（蝴蝶的数量减少为五，蝴蝶分散到四方十米的位置转化为会释放蝶弹的幽灵，该幽灵会持续到血量耗尽） ​ f3: 生者必灭之理-眩惑: ​ 发动后召唤大量蝴蝶环绕，蝴蝶不会进行攻击，但附带一个群体魅惑效果，并在持续时间内抵御一定的物理攻击。 ​ 发动后无法使用武器进行攻击。 ​ 按下左键触发分支技能： 死亡的诱蛾灯（重置魅惑效果持续时间，同时附带持续伤害。 ​ 按下右键触发分支技能： 华胥的永眠（眩晕受到魅惑的敌人，并造成一次伤害） ​ v1： 反魂蝶 ​ 在周围延伸出光带以及飞舞的蝶弹。 ​ v2： 反魂蝶-八分咲 ​ 当场上存在蝴蝶时，复活全体队友。当自身死亡时，如果场上存在蝴蝶，同时该技能准备就绪，将自动触发并复活自己（此时无法对队友生效） ​ v3： 西行寺无余涅槃 ​ 召唤西行妖投影，投影范围内f技能无冷却限制。自身所有伤害转变为无差别攻击。","link":"/2022/11/30/Inspiration-%E6%B0%B8%E5%8A%AB%E6%97%A0%E9%97%B4%E8%8B%B1%E9%9B%84%E8%AE%BE%E8%AE%A1-%E8%A5%BF%E8%A1%8C%E5%AF%BA%E5%B9%BD%E5%B9%BD%E5%AD%90/"},{"title":"魂魄妖梦（半成品）","text":"永劫无间英雄设计大赛 - 魂魄妖梦（未完成） 英雄：魂魄妖梦 种族：半灵 背景：魂魄家并非纯粹的幽灵，而是半幽灵半人类的家系。外表和普通人类无差别，思考方式也和人类一样，据说体温要比普通人低一些，寿命也比人类长很多。妖梦住在冥界的白玉楼，每日巡视庭园。白天修建庭园和修行剑术，有事务就出门。夜晚再巡视一遍，在白玉楼的主人就寝后才去休息。入侵者无论是人是妖，一律斩无赦。由于庭园的主人幽幽子的离开，妖梦于是跟了过来。外表是年幼的少女。 伴生武器： 太刀-楼观剑，短剑-白楼剑 技能： f1：人符-现世斩 v1：六道剑-一念无量劫","link":"/2022/11/30/Inspiration-%E6%B0%B8%E5%8A%AB%E6%97%A0%E9%97%B4%E8%8B%B1%E9%9B%84%E8%AE%BE%E8%AE%A1-%E9%AD%82%E9%AD%84%E5%A6%96%E6%A2%A6/"},{"title":"废稿-001","text":"随便写写 （一） 男孩与女孩的爱情观似乎是不同的。 之所以用似乎， 是因为重明也不确定。 18之前的重明，从来没有想过感情这种东西。 18岁时，重明被老爹丢进去部队，挨了两年打，出来之后，他更加不明白了。 2年的军旅生涯，带给重明的改变是极其巨大的，但同时，也让他对于感情这种东西愈发的迷茫。 不过，20岁的少年也不太在乎这些，他现在想的，是如何去玩，如何把失去的两年玩乐时光补回来。 踏上回家的火车的重明，就是这么想的，同行的兄弟们也这么想，于是他们在列车员路过的时候一人买了一罐啤酒——禁酒令的实行影响还是很大的，比如重明退伍之前排里的兄弟们聚餐的时候，喝的是饮料…… 于是重明他们被带队的排长训了，然后勒令他们把酒退","link":"/2022/11/30/Inspiration-%E7%A8%BF%E4%BB%B6-01/"},{"title":"废稿-星际战甲","text":"星际战甲随笔 - 其一 睁开眼时，看到的并不是自家天花板，而是正向着两侧划开的门板——大概是门板吧，看不出材质。 身上穿着奇怪质地的衣服，有点像角质化甲壳的贴身甲衣。 洛风皱了皱眉头，眼前的一切透着熟悉的味道。 暗金色的，装饰着亮金色纹样的，不到一米高的台座排成一排，充当着栏杆，边上还摆放着成堆的玩偶，造型怪异，鲜明而有特点。胎座的顶上，还顶着粉红色的爱心气球，在冷色调的环境中透着艳丽的光。 ‘确实是我的船。’ 也是看到这些东西，洛风终于确定了这件事。 飞船，衣服，台座，爱心气球。 以上的种种，全部都在告诉洛风这个事实——他现在，正站在自己的飞船之中，自己游戏之中，所使用的飞船！ “奥迪斯，你在吗？” 洛风试着喊了一声。 “奥迪斯并不需要睡觉，指挥官。即使我也想试试睡觉的感觉。” 熟悉的环绕音响起。 “什么时间了？”确定了自己确实是在游戏中的飞船里面，洛风松了口气——好吧半口，毕竟游戏归游戏，实际上这个世界还是相当的危险的——当务之急，洛风想先确定这个世界和之前游戏的区别。 “现在时间是一点三十分——我是说下午。” “有几位天诺战士发来的信息，指挥官。要接入吗？” 洛风一惊，但很快镇定下来，“接入吧，让我看看。” “林呗：阿布来陪我中断，搞快点搞快点。” “未曾见过你：垃圾车来不来？” “未曾见过你：？人呢？” 洛风陷入了沉默，洛风被击沉了。 这不是自己的游戏好友吗？？？怎么还能接到信息？ 难道他其实不是穿越到游戏的世界了， 而是穿越到了游戏里面？ 那么游戏菜单呢？ 这样想着，洛风的眼前，浮现出了一个熟悉的界面。 “物资仓，军械库，铸造厂，振幅晶体，孵化仓，虚空遗物，指挥官，通讯” ”奥迪斯，回复未曾见过你：收到信息速回，有急事，很急“洛风的呼吸有点急促，，如果能联系上的话，也许能让他帮忙处理一下现实的事情，毕竟如果一声不吭的走了的话，可能会出问题。 ”未曾见过你：啥事？我在刷垃圾。“ 新的消息似乎直接被脑海中的游戏菜单接管了，直接显示了出来。 ”很急，十万火急，快点快点。“","link":"/2022/11/30/Inspiration-%E7%A8%BF%E4%BB%B6-%E5%85%B3%E4%BA%8E%E5%A5%A5%E7%BD%97%E9%87%91%E7%9A%84%E4%B8%80%E4%BA%9B/"},{"title":"VTD路网编辑器相关","text":"路网编辑器(ROD) ROD可以使用无限制数量的车道线、复杂路口、交通牌和信号灯等，用于设计道路和轨道网络。 虚拟世界可以徒手设计，或者由已经存在的道路面片拼合而成。 ROD本身内置了多种导入和导出格式、大量的三维模型以及依据国家分类的交通标志和交通灯，更加方便我们的使用 逻辑数据被导出成OpenDRIVE格式，并可以被链接到道路逻辑数据库中。","link":"/2022/11/30/Study-notes-VTD-VTD%E8%B7%AF%E7%BD%91%E7%BC%96%E8%BE%91%E5%99%A8/"},{"title":"VTD(Virtual Test Drive)相关安装流程以及注意事项","text":"一、准备软件安装环境 Linux 系统 从Ubuntu官网上下载系统镜像并制作启动u盘 使用启动u盘进行系统安装 英伟达（NVIDIA）显卡与驱动安装 由于软件的特性，需要使用高端的图形显卡，目前的话客服那边推荐使用2080或者性能更加强大的3080、3090等 Linux系统里面通常会自带英伟达驱动程序以及software update， 打开后将会自动搜索匹配的驱动文件，供我们自行选择。 相关依赖模块安装 sudo apt-get install xterm sudo apt-get install freeglut3 sudo apt-get install openssh-server sudo apt-get install nfs-common sudo apt-get install mesa-utils sudo apt-get install xfonts-75dpi sudo apt-get install libusb-0.1-4 sudo apt-get install python sudo apt-get install mesa-common-dev sudo apt-get install libgl1-mesa-dev sudo apt-get install lsb sudo apt-get update 二、安装软件 1. 安装License管理工具 - sudo ./msc_licensing_hulium_linux64.bin - 注意将本地存储默认目录中的” “替换成”_” - 这里需要我们准备一个license文件来进行安装，license文件需要联系软件销售人员进行获取，获取的试用license有效时长为45天 2. 安装VTD","link":"/2022/11/30/Study-notes-VTD-VTD%E5%AE%89%E8%A3%85/"},{"title":"初识C++","text":"1. 初识C++1.1 创建新项目 打开Visual Studio 创建新项目 1.2 创建新文件 点击添加按钮即可选择要添加的文件类型以及名称 1.3 编写代码12345678# include &lt;iostream&gt;using nanespace std;int main(){ cout &lt;&lt; &quot;hello world&quot;&gt;&gt; endl; system(&quot;pause&quot;); return 0;} 1.4 运行程序","link":"/2022/11/30/Study-notes-C-%E5%9F%BA%E7%A1%80-001-%E5%88%9D%E8%AF%86C/"},{"title":"常量","text":"1 常量1.1 常量用于记录程序中某些不可更改的数据1.2 常量的定义方式 宏常量 #define 常量名 = 常量值 #define num 10; 被修饰的变量 const const int num = 1","link":"/2022/11/30/Study-notes-C-%E5%9F%BA%E7%A1%80-004-%E5%B8%B8%E9%87%8F/"},{"title":"注释","text":"1. 关于注释1.1 单行注释 // 完成注释之后，目标行代码将会变成绿色， 运行代码时将会忽略这一行的代码 注释的用处时用来对代码进行标注1.2 多行注释 /**/ 多行注释支持换行，它可以详细描述多行代码，或者注释一整个代码块","link":"/2022/11/30/Study-notes-C-%E5%9F%BA%E7%A1%80-002-%E6%B3%A8%E9%87%8A/"},{"title":"变量","text":"1.变量1.1 变量存在的意义变量的存在能够方便我们管理内存空间 1.2 变量创建语法 数据类型 变量名 = 变量初始值 int a = 10;1.3 变量的类型 int float double string char long longlong","link":"/2022/11/30/Study-notes-C-%E5%9F%BA%E7%A1%80-003-%E5%8F%98%E9%87%8F/"},{"title":"输入输出","text":"1. 输出cout用法： 1cout &lt;&lt; &quot;输出&quot; &lt;&lt; endl; 2. 输入cin 用法： 1234char a;cin &gt;&gt; acout &lt;&lt; &quot;输出输入的数据&quot; &lt;&lt; a &lt;&lt; endl;","link":"/2022/11/30/Study-notes-C-%E5%9F%BA%E7%A1%80-005-%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/"},{"title":"武器大师 - 001","text":"武器大师 - 001 青石镇。 作为青石城周围通往边关的贸易镇之一，今日依然颇为热闹，各国来来往往的行商不少都途径此地，倒也使得小镇经济愈发蓬勃，当然，这主要得力于国家的强横，作为大陆三大帝国之一的大乾，自然是不需要怎么担心边关稳定，如若不然，凭青石镇紧靠边关的地理位置，怕是早就在战火中覆灭了。 此时此刻，正有一个十二三岁的稚嫩少年从镇子外归来。身上裹着的皮甲上，还点缀着狐绒，明显是精心打理过的，狐绒随风飘摇着，显得颇为俊逸。 “小少爷又去林子里玩了？” 林老头笑着，从自家的蒸笼中摸出来一个大包子，塞给少年：“来， 刚出笼的包子，赶紧趁热吃。” “谢谢林柏。”礼貌的道了声谢，王崇明接过包子，也不顾手上沾着的些许浅绿色草叶，就往嘴里塞，脚下确实不停，径直往家里走去。 王府在这镇上也称得上体面，王老太爷本是一介行商，年轻时候走街串巷，行走于诸国之间，倒卖特产为生，后来年纪大了，便来这青石镇安了家，起初的时候，这镇上还颇为破落，王老太爷到了这里，带着村民一块折腾起了镇子的特产，兼之各种茶楼驿站，慢慢有了名气，便开始有商人途经此地，借此机会，村里便慢慢好起来了，也是因此，镇里的老人大都对王家心怀感激，平日里见着这位小少爷，也是顺手塞点糖果点心，王崇明自个都吃惯了，反倒不是很喜欢坐在家里吃饭，故而总是乱跑，反正这镇里也饿不着他。 家里本身也算宽松，老太爷念叨了几次，看他听不进去，也就不再说啥了，只是嘱咐要注意安全，莫要跑去丛林深处。 毕竟这世道，林中可不是只有野兽。 还有野妖。 何为野妖？ 如同野兽一般，驯化的野兽便不再是野兽，而是看家护院的宠物，同理，妖有野妖，自然也有在野的妖物。 大乾下立镇妖司，妖星阁。 镇妖司由人族组成，妖星阁由妖族组成，这里的妖族，乃是在大乾登记造册，等同于国民的存在，而非野外的妖怪。 大乾有律法，凡野外自行觉醒灵智的妖兽，由妖星阁收拢教导，诵念四书五经，学习人理纲常，待到由妖化人，便可行走于世。 至于在野妖兽屠戮人类的，由镇妖司直接出手，罪过较轻者，可酌情交与妖星阁教导，情节严重者，当斩。 而通常情况下，像镇外的密林，便是野妖常见的出没场所，这些野妖大都保留着野兽心性，茹毛饮血，于懵懂间吸收月华，其中杀性重者，更是为祸一方，肆意杀戮取乐。 扯远了，总之，家里对于王崇明出门晃悠还是看得开的，不过， 像这次一般夜不归宿，还是头一次。 王府。 摸了一把石狮子，王崇明迈步向院内走去，一路穿过前院，前厅走廊，七拐八拐一阵，方才回到了自己的小院。 插好门窗，解开皮甲，扯下昨夜损毁的内里衣衫，王崇明站在铜镜前，看着胸前的爪痕皱了皱眉。 ”系统，出来“ 眼前的光景在变幻，光芒汇聚之间，化为了一张浅蓝色的面板。 没有智能，没有声音，没有新手教程，没有礼包。 昨天晚上半夜跑出去，还和一个野妖打了一架，王崇明得到的，就是这个看着像是半成品的系统。 是的， 王崇明并非此界中人。 确切的说，是他的前世。 即使前世之事模糊至极，只剩下一些隐隐约约的片段留存，但存留的那部分记忆中的一些东西，确实直白的让王崇明了解到，自己是转世者。 前世因何缘故去世，现在已经想不起来了，只记得高楼大厦，记得白底黑字的对联，记得和自己的肉身面对面，只是那脸上仿若蒙了一层迷雾，看不真切。 一晃已经十二年了，若非昨夜在林中捡到了系统，王崇明都已经快忘记了自己的曾经。 此时此刻，系统的菜单上，只有寥寥几个功能选项。 个人，背包，技能，***，成就，地图 目光在***上停顿了一下，王崇明的注意力， 集中到了个人上。 姓名：王崇明（***） 骨龄：12 灵根：无 特异点：*** 天赋：*** 血脉：人族 战斗力：7（你可以和刚刚开启灵智的野妖碰一碰，虽然死战的话你大概率只能重开，但毕竟勇气可嘉） 物品。 ”这里什么也没有（剩余栏位：1）“ 技能。 ”这里什么也没有“ 成就。 ”转生（已达成）：成就点+99“ ”尿床（已达成）：成就点+1“ ”学步（已达成）：成就点+1“ …… 王崇明脸一黑，不再细看成就信息，点开了角落的成就点商城。 入目就是一道金光。 ”传奇：将一个技能直接提升至最大等级，无其他任何消耗。（PS：慎用！慎用！！！技能等级的提升会带来大量对应的感悟、细节等信息，请考虑清楚再进行使用。）售价：1000成就点。“ ”空白技能卡：将一个技能等级+1，该技能卡只适用于0-1级技能。售价：100成就点“ “…”王崇明看了看自己的150点成就点，陷入了沉默，算上转生的99点成就点，十二年才150点……虽说这也是因为之前没系统在没有刻意去摸索成就，但也属实艰难了一点。 ”算了算了，先不看了。“挠了挠头，王崇明决定暂且放一放，虽说这成就点商店似乎还有不少好东西，但凭自己的点数，明显买不到什么好东西，倒是能买一张技能卡，但自己现在连个技能都没有，自然也就用不上了。 ”不过，也许可以试试这个。”翻了翻抽屉，将一本古旧的书取了出来，“莽牛拳！” 这是前些日子一个行商的货物，王崇明路过的时候正好碰上那商人在摆摊，看到这本书就随手买了，翻了翻发现看不懂那些乱七八糟的穴位，就随手丢屋里了——他屋里大部分小物件都这么来的。 只是这次翻开这本书，却出现了不一样的情况，微弱的蓝色光芒如流水一般，从王崇明的手中流淌而出，流转于书页之上。 “检测到新的可用技能。“ 系统界面弹出，蓝色光芒回流，最终凝成了一行字。 ”莽牛劲：经系统修正的莽牛劲，强身健体，奠基之法。（PS：莽牛×铁憨憨√）等级：0“ “铁憨憨……”王崇明决定忽略这句话，转而专心接受莽牛劲这个技能的相关信息。 0级的信息并不多，只有一些入门的修行方法，打熬肉身，锤炼筋骨之法，王崇明只用了几分钟就已经整理完毕，借着系统的力量，这些记忆被完整的记了下来。 然后他犹豫了一下，打开成就商店，买下了一张技能卡，然后拍在了莽牛劲上。 下一刻，疼痛如潮水般从身体各处爆发出来，猛地涌向了大脑，与此同时，相关的修行经验冲击着王崇明的意识，保持着大脑的清醒。 一级的提升不算太大，痛苦也只是持续了一分钟的样子，便平稳了下来，即便如此，王崇明也已经满身大汗，毕竟再怎么早熟，现在的他，依然只是个十二岁的孩子而已。","link":"/2022/11/30/Inspiration-%E7%A8%BF%E4%BB%B6-%E6%AD%A6%E5%99%A8%E5%A4%A7%E5%B8%88-1-%E8%8E%BD%E7%89%9B%E5%88%9D%E5%BC%80/"},{"title":"武器大师 - 002","text":"武器大师 - 002 正当王崇明摸索身体变化之时，脚步声响起，未几，有人推门。 回来之时，他便已将门反锁上，来人顿了顿，抬手敲了敲门。 “明哥儿，夫人差我来带你去吃早茶。” 却是王崇明母亲的贴身丫鬟小月。 小月还是婴儿时被人放在了王府门口，身无长物，只有个写着月字的纸条，那时王崇明才刚满月，其母看小月颇为可爱，便收养了她，虽说是丫鬟的名头，却也是视如己出，也是与王崇明从小玩到大的青梅。 “吱呀”一声，王崇明拉开了门，清晨的阳光斜斜的落在赤裸的上身，将少年结实的身材打上了一层光。 “诶呀，你怎么不穿衣服？” 虽说见过几次，小月还是羞红了脸，推着王崇明往屋里走，催他去穿衣服。","link":"/2022/11/30/Inspiration-%E7%A8%BF%E4%BB%B6-%E6%AD%A6%E5%99%A8%E5%A4%A7%E5%B8%88-2-%E5%BC%82%E5%9B%BD%E5%95%86%E4%BA%BA/"},{"title":"武器大师 - 简介","text":"武器大师 - 简介 何为武器大师？ 将心、技、体融为一体，盖世无双者是也。 王崇明起于微末，灵根低下，却意外得到了来自天外的至宝。 从此以后，万般兵器，诸般法门，尽握于手中。 灵根微末？简单，肉身横推，强行招引灵气即可。","link":"/2022/11/30/Inspiration-%E7%A8%BF%E4%BB%B6-%E6%AD%A6%E5%99%A8%E5%A4%A7%E5%B8%88-%E7%AE%80%E4%BB%8B/"},{"title":"新坑 - 001","text":"”先帝创业未半而中道崩殂……“ 迷迷糊糊之间，朗朗读书声传入耳中，慢慢清晰了起来。 方知有点懵逼的揉了揉眼睛，后知后觉的发现，此时的自己，并不是在自己入睡时的公寓之中，而是在一间教室之中，这教室看上去还挺眼熟。 刚睡醒还不是很清醒的脑袋努力的转动着，在记忆深处找到了眼前教室的信息。 2013年，初三。 熟悉的小组制座位。 方知抬头看了眼教师后方黑板上方挂的表——7：30。 似乎是早读时间？ 大致确定了一下时间，方知皱起了眉头。 自己这是，穿越了？还是重生了？ 信息太少不好确定，不过，初三的自己已经是走读生，每天中午都会回家吃饭，到时候再摸索一下情况。 先前的记忆仍停留在下班回家，莫名的困得不行，于是便直接上床睡觉了，灯都没关。 而且，大拇指上多了一个记忆中并没有的扳指。 灰紫色的戒身，其上镶嵌着一个黄铜色的装饰物。 这个东西，有点眼熟啊。 思索了一下，方知伸手抚摸了一下这个戒指，下一刻，一缕信息传入方知脑海。 黑暗封印： +1法术强度（当前荣耀层数1，击杀同级别对手叠加一层，每层永久增加法术强度1，最多叠加十层，每叠加十层增加一点灵魂） 等级：1（0/1) 虽然不知道这东西怎么来的，但毫无疑问，这东西应该就是某个游戏中的装备，以击杀敌人作为增益来源的特殊装备——对敌人具有相当高的嘲讽性。 “下课时间已到……” 收起探寻的心思，方知照着记忆里的印象向着食堂走去。 方知初中是在镇上的学校上的，离家近，走路大概也就二十分钟就到家了，而且半数是土路，每天早上七点十分开始早读，到七点四十下课，八点开始第一节课，而七点四十到八点之间，则是预留的吃饭时间——初一时候方知曾获得过学校发放的奖学金，一张八百多的饭卡，也是这样，家里从来不用管他吃饭的事情。 只不过前世的方知后来逐渐贪玩了起来，开始肆意挥霍。 重活一世，方知自然不会像以往那般幼稚，经历了社会上的各种诱惑的社畜，面对着学校的食堂，属实有些不感兴趣。 草草的吃了些饭菜，方知慢悠悠的度过了上午的四节课，期间好好回顾了一下班里的各个同学的名字，相比起后来的他们，此刻这些人还青涩稚嫩的多——虽说此刻的方知也是。 放学铃声响起的时候，方知已经踩着点从后门摸了出去，一路直奔大门，此时的他，迫切的需要一些私人空间整理一下自己的思绪。 于是，意外出现了。 并非是撞车，也不是路边的小混混。 这意外，是真正意义上的，方知没想到的特殊意外。 灰白色的身影，飘在半空之中，随风晃悠着，即使啥也没干，也依然让方知不由得停下了脚步。 似乎是个……恶灵？ 而且这个位置…… 忘记提了，在方知初二的时候，老家路上有了新的规划，政府的施工队直接开山，硬生生地打通了两条公路之间的小山，拉了一条直溜溜的公路出来。 而原本在半山腰上的一条大渠，也因此被悬空撑了起来。 而此刻方知遇见着鬼影的地方，正是这条渠的下方。 只是这灵，看起来没什么攻击性的样子。 方知前世在这条路走了三年，什么时间段都走过了，却从来没见过这东西，如今刚重生回来就撞上了，难免不让方知多想。 “是因为黑暗封印？”眯了眯眼睛，方知摸了摸戒指，走上了前去。 这灵看上去没有攻击性，试一下应该也无所谓。 方知抬起右手，小心的凑过去，用手上的扳指碰了一下。 下一刻，灵体崩散，蓝色的光点纷纷扬扬地涌向了戒指。 “荣耀+1，法术强度+1” “黑暗封印首次激活，随机抽取技能……” “抽取完毕，获得技能：肉食者。” ”肉食者（初级）：通过进食获取能量，强化肉身。（消化能力得到极大增强。）“ ”黑暗封印曾被大量强者所使用，它记录了一些强者的技艺。“ ”黑暗封印可进化为梅贾的窃魂卷，并拥有更强的增幅和更多的技能选择。“ “黑暗封印：+2法术强度（当前荣耀层数2，击杀同级别对手叠加一层，每层永久增加法术强度1，最多叠加十层，每叠加十层增加一点灵魂）”","link":"/2022/11/30/Inspiration-%E7%A8%BF%E4%BB%B6-%E6%96%B0%E5%9D%91-001/"},{"title":"空气炸锅对比表格","text":"6款空气炸锅对比表格 品牌 价格 大小 按钮 温度 优点 缺点 美的 199 3 1.不能控制温度2.只有一个时间按钮 139℃-175℃（不可控）温差36℃ 1.外壳做工最差2.烘烤耗时长，效果勉强3.不好清洗 九阳 179 3 温度和时间两个按钮一个正面一个顶部 设定200℃，实际169-231，温差62 1.加热管带遮挡 1.火力过猛不稳定2.清洗简单，不沾 山本 199 4.2 两个按钮在正面，一本菜谱 设定200℃，加热需一分钟，170-230，温差60 清洗简单，不沾 韩国现代 155 4.5 两个按钮在顶部 设定200℃，加热需2分半，160-220，温差60 1.外壳做工较差2.沾底不好洗，但比美的强 苏泊尔 279 5 两个按钮在正面 设定200℃，加热需2分半，175-207，温差30 清洗简单，不沾 飞利浦 1000+ 3 液晶屏控制 设定200℃，加热需4分半，193-201，温差10","link":"/2022/11/30/Inspiration-%E7%A8%BF%E4%BB%B6-%E6%96%B0%E5%9D%91-002/"},{"title":"flask开端","text":"—————————–flask开端——————————– flask安装: ​ pip install flask 在命令提示符中输入pip指令进行安装 ​ 使用pycharm安装flask 在file选项中找到settings 找到project选项,点击进入porject下的project interpreter选项中 点击此处+号,进入搜索 搜索到之后打勾,点击install package ,开始下载 flask使用: ​ 初次创建flask工程 from flask import Flask app = Flask(__name__) @app.route('/') def hello_world(): return 'hello world!' if __name__ = '__main__': app.run(debug=True) 注意,在2018之后版本的pycharm中,设定debug=true并不能开启debug模式,想要开启debug模式需要在pycharm中进行设定 点击运行按钮左边的选项,在台出的页面中选择edit configurations 进入后勾选FLASK_DEBUG","link":"/2022/11/30/Study-notes-Python-Flask-01-flask/"},{"title":"flask-router","text":"flask_route 创建app主体 ```pythonfrom flask import Flaskapp = Flask(name) 12345- 绑定路由 - ```python @app.route('/') ```python@app.route(‘/‘,methods=[‘GET’,’PORT’]) 12345678 - 绑定路由时,如果不设定methods方法,将会默认为get方法,这种情况下,其他方法无法访问- 设定路由的执行函数 - ```python def hello_world(): return 'hello world!' 路由的执行函数通过return来实现,在不返回HTML页面的情况下,可以将要返回的数值直接返回,但是只能返回字符串类型 在设定玩路由之后,只需要开辟一个程序入口进行运行,就构成了一个完整的flask if __name__ = '__main': app.run(debug=True)","link":"/2022/11/30/Study-notes-Python-Flask-02-flask-route/"},{"title":"flask_migrate","text":"flask_migrate初试: 模块导入 ```pythonfrom flask_migrate import Migrate,MigrateCommand 1234567891011121314151617181920212223242526272829 - Migrate用于migrate对象实例化通过app以及db来构筑完整的migrate - MigrateCommand是一个对象,其中封存着要使用的command对象(此对象使用方法等同于flask_script的构筑方法) - 此处还可以导入flask_script模块- 具体实现步骤 - migrate文件内的构造 - ```python #导入flask_script模块 from flask_script import Manager #导入flask_migrate模块 from flask_migrate import Migrate,MigrateCommand #导入app from app import app #导入数据库链接实例db from exts import db #实例化manager对象 manager = Manager(app) #开始创建app与数据库连接 Migrate(app,db) #在创建好的对象中添加script子命令 manager.add_command('db',MigrateCommand) exts文件中的构造 ```pythonfrom flask_sqlalchemy import SQLALchemy db.SQLALchemy() 123456789101112131415161718- app文件中的构造- ```python from flask import Flask from exts import db import config app = Flask(__name__) app.config.from_object(config) db.init_app(app) @app.route('/') def hello_world(): return 'Hello World!' if __name__ == '__main__': app.run() models文件中的构造 ```pythonfrom exts import dbclass person(db.Model): __tablename__ = 'person' id = db.column(db.Integer,primary_key=True,autoincrement=True) name = db.Column(db.String(100)) 12345678910------## flask_migrate在命令行中的使用:- 进入文件夹,打开命令窗口- ```python python 文件名 db(子命令名称) init #创建一个migrations文件夹 创建一个迁移文件 ```pythonpython 文件名 db(子命令名称) migrate#########################################注意,这里有一个坑,如果在创建migrate文件的时候,没有传入你的models文件中的表(类)#那么将会出现无法找到表的情况#具体表现形式为“””INFO [alembic.runtime.migration] Context impl MySQLImpl.INFO [alembic.runtime.migration] Will assume non-transactional DDL.INFO [alembic.env] No changes in schema detected.“””#在这种情况下,需要在migrate中导入表form models import Person######################################## 123456- 将创建好的迁移文件映射到数据库中- ```python python 文件名 db(子命令名) upgrade #完成数据库映射 python 文件名 db(子命令名) downgrade python 文件名 db(子命令名) head python 文件名 db(子命令名) history 基本上所有的方法对应着flask_alembic中的方法 可以使用 –help方法查看可以使用的全部方法","link":"/2022/11/30/Study-notes-Python-Flask-03-flask-migrate/"},{"title":"flask 数据库迁移","text":"flask数据库迁移 123456789101112131415161718192021from sqlalchemy import *from sqlalchemy.ext.declarative import *from sqlalchemy.orm.orm import *from sqlalchemy.dialects.mysql import LONGTEXT,DECIMAL,DATETIMEHost = '127.0.0.1'Port = '3306'User = 'root'Password = '000000'Database = 要使用的数据库名称DB_URI = 'mysql+mysqlconnector://{user}:{password}@{host}:{port}/{database}'.format(user=User,password=Password,host=Host,port=Port,database=Database)engine = create_engine(DB_URI)Base = declarative_base(engine)session = sessionmaker(engine)()class person(Base): __tablename__ = 'person' id = Column(Integer,primary_key=True) name = Column(String(100))Base.metadata.drop_all()Base.metadata.create_all() 创建一个模型 1alembic init alembic 创建新的alembic文件(此操作在命令提示符中完成) 修改alembic文件内容 打开alembic.ini文件 修改***sqlalchemy.url***为: ​ 1sqlalchemy.url = 'mysql+mysqlconnector://root:password@localhost:port/database' 打开alembic文件夹 打开env.py文件 在其中添加以下代码 ```pythonimport sys,osimport appsys.path.append(os.path.dirname(os.path.dirname(file))) 12345678910- 修改文件代码 - ```python #修改前 target_metadata = None #修改后 target_metadata = app.Base.metadata 1234567*alembic revision --autogenerate -m 'first commit'创建新的迁移脚本(此操作在命令提示符中完成)alembic revision --autogenerate -m 'add age column'更新迁移脚本(此操作在命令提示符中完成) alembic upgrade head数据库迁移开启(此操作在命令提示符中完成) alembic常用字段 ​ init 新建一个仓库 revision 创建一个新的版本文件 –autogenerate 自动将当前模型的修改,生成迁移文件 –m 本次迁移做了哪些修改,可以通过指定这个参数,进行回顾 upgeade 将置定版本的迁移文件映射到数据库之中,如果有多个迁移文件没有映射,那么会执行多个迁移脚本 [head] 代表最新的迁移脚本的版本号 一般指向最新版 ‘first commit’ 生成第一个迁移脚本时的映射参数 add age column 修改数据库时使用的映射参数 history 列出所有的迁移版本及其信息 alembic history (展示所有的版本更新的版本号) heads 展示head指向的脚本文件的版本号 这个版本号通常为正在使用的版本号 downgrade 执行指定版本迁移文件中的downgrade函数 与upgrade对应,使用方法为 alembic downgrade 版本号 版本号可以在文件中查看,即为对应迁移文件中的revision的值 current 展示当前数据库中的版本号 使用方法 在数据库中会生成一个额外的表,alem_version ,这个表中存储着当前的版本号 在命令提示符中输入 alembic current 会得到当前正在使用的版本号 alembic在flask中的使用 前期操作方法相同 在创建好仓库时,按照以下步骤设定 找到alembic.ini文件 修改其中的sqlalchemy_uri的值为要使用的数据库链接 找到alembic文件夹下的env.py 在其中进行如下操作 import sys,os sys.path.append(os.path.dirname(os.path.dirname(__file__))) import app * target_metadata = app.db.Model.metadata 之后的操作和之前并无差别","link":"/2022/11/30/Study-notes-Python-Flask-010-flask%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%81%E7%A7%BB/"},{"title":"flask_script","text":"flask_script安装: ​ 安装方法 ```pip install flask-script12345678910111213141516------## flask_script初次使用:1. ```python from flask_script import Manage from app import app manage = Manage(app) @manage.command def func(): print('hello world') #最简单的flask_script创建 #无参数 ```python@manage.option(‘-u’,’–username’,dest=’’)def func(username): print(&quot;你输入的账号为%s&quot;%username) #有参数 12345673. 进入命令提示符,进入代码所在的文件夹 1. ```python python (文件名称) func #无参数 ```pythonpython (文件名称) func -u “root”#有参数1234567891011121314151617181920212223242526------## flask_script进阶操作:1. 额外创建一个script文件2. 在文件中引入要集成的文件 1. ```python from flask_script import Manager from app import app,db,Person db_script = Manager() @db_script.command def func1(): print('仓库创建成功') @db_script.command def func2(): print('数据迁移成功') @db_script.command def func3(): print('数据映射成功') 在主文件中注册附属的script文件 ```pythonfrom flask_script import Managerfrom app import app,db,Personfrom db_script import db_scriptmanager = Manager(app)manager.add_command(‘db’,db_script)12345674. 在命令提示符中使用 1. ```python python 文件名 db func1 python 文件名 db func2 python 文件名 db func3","link":"/2022/11/30/Study-notes-Python-Flask-04-flask-script/"},{"title":"Flask文件上传","text":"文件上传: #导入必须的模块包 from wtform import Form,StringField,PasswordField,SubmitField,BooleanField,FileField,SelectField #导入验证器包 from wtform.validators import EqualTo,Length,UUID,URL,ValidatorsError,InputRequired,Email #从flask_wtf中导入file.FileRequired(是否为空验证)和file.FileAllowed(验证是否符合要求的格式,格式要求的范围作为参数进行传递,切记传入的参数必须为一个列表) from flask import Flask,request,render_template,send_from_directory from werkzeug.datastructures import CombinedMultiDict from flask_wtf.file import FileRequired,FileAllowed #导入文件名过滤包 from werkzeug.utils import secure_filename import os app = Flask(__name__) upload_url = os.path.join(os.path.dirname(__file__),'image') class userForm(Form): #创建文件上传约束 avater = FileField(validators=[FileRequired(),FileAllowed(['jpg','png','gif'])]) @app.route('/',methods=['GET','POST']) dedf func(): if request.method == 'GET': return render_template('add.html') else: #使用ConbinedMultiDict模块对传输的两个不同的值进行合并,一起传入wtf文件传输判定中进行验证 userform = userForm(CombinedMultiDict([request.form,request.files])) if userform.validate(): avater = userform.avater.data filename = secure_filename(avater.filename) avater.save(os.path.join(upload_url,filename)) return '上传成功'","link":"/2022/11/30/Study-notes-Python-Flask-05-flask%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0/"},{"title":"Flask App上下文","text":"app上下文 应用上下文和请求上下文都是存放到一个’LocalStack’的栈中 和应用app相关的操作就必须用到应用上下文,比如通过current_app获取当前这个app 和请求相关的操作就必须用到请求上下文,比如url_for反转视图函数 视图函数中不需要考虑上下文的问题 应用上下文:12345678from flask import current_app#current_app模块的作用是获取当前文件的栈#使用的方法current_app.name#获取当前文件的名字#此选项需要在路由函数中才能使用#在路由函数外直接使用将会报错 手动推入应用上下文: 第一种方法: 123456#在路由函数外调用current_app模块#需要先设定好应用上下文app.context = app.app_context()app.context.push()print(current_app.name) 第二种方法: 12with app.app_context(): print(corrent_app.name) 请求上下文:123456789101112131415#请求上下文最常见的一个是url_for#在视图函数中@app.route('/')def func(): #查看url_for打印出来的路由地址 print(url_for('func1')) return 'hello world'@app.route('/test/')def func1(): return 'hello user'print(url_for('func1'))#在路由函数外直接调用url_for 查看路由地址将会报错,原因是请求上下文问题#此问题无法使用应用上下文的方法解决 手动推入请求上下文: ​ 1234#在路由函数之外调用with app.test_requert_context(): print(url_for('func1'))#注意,test_request_context这个函数会在推送请求上下文之前先检查有没有应用上下文,如果没有,将会先推送一个应用上下文过来,在推送一个请求上下文,也就是说,在一定程度上等同于app_context的加强版","link":"/2022/11/30/Study-notes-Python-Flask-app%E4%B8%8A%E4%B8%8B%E6%96%87%E6%9C%BA%E5%88%B6/"},{"title":"Flask WTF 模块","text":"flask_wtf简介: ​ flask_wtf模块下载安装 pip install flask-wtf 使用命令行形式安装flask_wtf模块 flask_wtf实际使用: ```pythonfrom wtforms import Form,StringField,PasswordField,SubmitFieldfrom wtforms.validators import EqualTo,Length#创建一个全新的form表单类,该类继承自Formclass RegistForm(Form): #username属性设定 username = StringField(validators=[Length(min=3,max=8)]) #password属性设定 password= PasswordField(validators=[Length(min=6,max=10)]) #EqalTo作用,添加EqualTo的字段必须与其参数的值相同,相当于绑定 password_1 = PasswordField(validators=[Length(min=6,max=10),EqualTo('password')]) sub = SubmitField() app = Flask(name) @app.route(‘/‘)def hello_world(): return 'Hello World!' #创建form表单提交页面@app.route(‘/regist’,methods=[‘GET’,’POST’])def regist(): #判断请求方式并给出相应回复 if request.method == 'GET': return render_template('regist.html') else: #实例化表单类 regist_form = RegistForm(request.form) #判断类中的validate是否正常,即验证是否通过 #根据判定结果进行不同的返回情况 if regist_form.validate(): return 'True' else: print(regist_form.errors()) return &quot;false&quot; 12345- 在length函数中,可以添加一个新的参数,message,用来设定报错之后的- ```python username = StringField(validators=[Length(min=3,max=8,message='您所输入的账户名称不符合要求.请重新输入')]) 这个message所储存的参数将会体现在类的errors方法里面,可以通过直接输出来查看 flask_wtf常用验证器:验证器导入方法: 123456789from wtforms.validators import EqualTofrom wtforms.validators import Lengthfrom wtforms.validators import NumberRangefrom wtforms.validators import Emailfrom wtforms.validators import InputRequiredfrom wtforms.validators import Regexpfrom wtforms.validators import URLfrom wtforms.validators import UUID EqualTo 绑定另一个字段,使两个字段保持一致 Length 限定内容长度,以及自定义报错提醒 NumberRange 限定数字的大小所在区间,和length一样有max和min两个值限制,只有在这两个值之剑才满足 Email 邮箱验证器,用于验证邮箱,不需要参数,内置了邮箱格式验证 InputRequired 对输入的信息进行验证,有值即为True,没有则为false Regexp 正则表达式,自动将输入的内容于Regexp的内容匹配对照 URL 验证输入的是否为正确的url地址 UUID 验证是否为UUID UUID导入方法 ```pythonform uuid import uuid4print(uuid4())12345678------## 自定义验证器:- ```python def validate_字段名 (self,filed): #通过这个方法定义的验证器,将会自行调用进行判断, 自定义验证器的名字为validate_加上要绑定的字段名 validate_字段名 需要设定一个形参,不限制 flask_wtf常用字段: IntegerField 整形字段,该输入框只支持输入数字类型 最终拿到的值也是整形,即int 同一属性的还有FloatField StringField 字符串形式,该输入框支持输入字符串类型 PasswordField 密码字段,如果使用该字段创建表单,其生成的将会是密码输入框 SubmitField 此字段对应的是submit提交按钮 BooleanField 单选框模板 FileField 文件框 SelectField 下拉框 tags = SelecrField('标签名',choices=[('1','python'),('2','ios')])","link":"/2022/11/30/Study-notes-Python-Flask-flask-WTF/"},{"title":"Flask session","text":"flask_session /cookies cookie创建:1234567891011121314151617181920from flask import Responseres = Response('示例')res.set_cookie('username','root', max_age=60, expires=datetime(year,month,day,hour,minute,second), domain='.fcg.com')#创建cookie时需要导入以下几项,# 1.cookies名字 + cookies值 (必传)# 2.max_age从当前开始计算的过期时间,单位为秒,不添加的时候默认过期时间为关闭浏览器过期# 3.expires设定过期时间,以格林尼治标准时间计算,换算到北京时间需要减八,这个参数如果设置的话,需要传入六个参数分别是--(year,month,day,hour,minute,second)# 4.domain设定cookie作用范围,在主域名前面加上.即为将此cookie应用于所有子域名# 5.设定作用范围,需要先绑定好主域名和子域名#cookie删除res = Response('删除')res.delete_cookie('username') session创建:12345678910111213141516171819202122from flask import Flask,sessionapp = Flask(__name__)@app.route('/')def func(): #创建并设置session.cookie session['username'] = '123' #设定session过期时间 session.permanent=True return 'hello world'#session.permanent默认为一个月#可以通过设定修改,#app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(剩余过期的时间)#session删除session.pop('username')#相当于字典删除session.clear()#将session中储存的数据全部清空del session[key] 请求函数的装饰器使用方法:1decorators 设置请求函数过滤用装饰器 #从functools 中导入模块wraps from functools import wraps from flask import session,redirect,url_for def login_required(func): #设定从functools里面导入的装饰器,此装饰器的参数为外部函数传入的值 @wraps(func) #定义内层函数,该内从函数中执行运算与判定 def wrapper(*args,**kwargs): #获取session username = session.get('username') #判断是否存在session if username: #存在session时,正常调用外层函数传入的func参数 return func(*args,**kwargs) else: #当session中不存在对应的cookie时,重定向当前网址到登陆界面 return redirect(url_for('login')) #正常的闭包函数用法,返回内层函数的引用 return wrapper","link":"/2022/11/30/Study-notes-Python-Flask-flask-session/"},{"title":"Flask 数据库操作","text":"创建flask数据库链接 12345678910111213141516171819from flask import Flaskfrom flask_sqlalchemy import SQLALchemyapp = Flask(__name__)app.comfig['SQLALCHEMY_DATABASE_URI'] = 'mysql+mysqlconnector://root:password@localhost:port/database'db = SQLALchemy(app)class person(db.Model): __tablename__ = 'person' id = db.COlumn(db.Integer,primary_key=True,autoincrement=True) name = db.Column(db.String(100))db.drop_all()db.create_all()@app.route('/')def hello_world(): return 'hello world!'if __name__ = '__main__': app.run() 关于导包: flask_sqlalchemy中把sqlalchemy的所有模块整合到了一起,所以不需要像原生sqlalchemy一样导入一大堆模块,在这一方面做的比原生SQL alchemy好得多 关于分页查询:paginateper_page:设定每页展示的数据 page:当前的页数 has_prve:判断是否具有上一页 has_next:判断是否具有下一页 prve_num:上一页的页码 next_num:下一页的页码 iter_pages: 所有页码,可遍历","link":"/2022/11/30/Study-notes-Python-Flask-flask-sqlalchemy/"},{"title":"Flask 常用钩子函数","text":"常用钩子函数 errorhandler 错误报告专用 在装饰器中传入参数进行对应错误代码进行捕获 定义错误捕获函数进行捕获,此函数中定义参数为error 此错误报告可以返回 ‘html’ 页面,并使用js对其进行修饰 使用abort模块进行错误跳转,即,当访问的界面不符合条件时,直接返回对应的错误, ```pythonfrom flask import abort…..@app.route(‘/)def func():if....: return render_template('...html') else: #不满足条件直接返回404错误 abort(404) 12345678910 - ```python @app.errorhandler(404) def func1(error): return '404' @app.errorhandler(500) def func2(error): return '500' context_processor 上下文声明专用 在声明的函数中直接返回要定义的上下文内容 @app.context_processor def function(): return {&quot;current&quot;:&quot;&quot;}","link":"/2022/11/30/Study-notes-Python-Flask-flask%E5%B8%B8%E7%94%A8%E9%92%A9%E5%AD%90%E5%87%BD%E6%95%B0/"},{"title":"Flask 文件上传","text":"flask–05文件上传: #导入必须的模块包 from wtform import Form,StringField,PasswordField,SubmitField,BooleanField,FileField,SelectField #导入验证器包 from wtform.validators import EqualTo,Length,UUID,URL,ValidatorsError,InputRequired,Email #从flask_wtf中导入file.FileRequired(是否为空验证)和file.FileAllowed(验证是否符合要求的格式,格式要求的范围作为参数进行传递,切记传入的参数必须为一个列表) from flask import Flask,request,render_template,send_from_directory from werkzeug.datastructures import CombinedMultiDict from flask_wtf.file import FileRequired,FileAllowed #导入文件名过滤包 from werkzeug.utils import secure_filename import os app = Flask(__name__) upload_url = os.path.join(os.path.dirname(__file__),'image') class userForm(Form): #创建文件上传约束 avater = FileField(validators=[FileRequired(),FileAllowed(['jpg','png','gif'])]) @app.route('/',methods=['GET','POST']) dedf func(): if request.method == 'GET': return render_template('add.html') else: #使用ConbinedMultiDict模块对传输的两个不同的值进行合并,一起传入wtf文件传输判定中进行验证 userform = userForm(CombinedMultiDict([request.form,request.files])) if userform.validate(): avater = userform.avater.data filename = secure_filename(avater.filename) avater.save(os.path.join(upload_url,filename)) return '上传成功'","link":"/2022/11/30/Study-notes-Python-Flask-flask%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0/"},{"title":"Flask 文件下载","text":"文件下载模块 send_filereturn send_file(文件路径,,==as_attachment=True==,==cache_timeout=1==)","link":"/2022/11/30/Study-notes-Python-Flask-flask%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD/"},{"title":"Flask 信号机制","text":"flask信号 信号使用详解: 信号使用分为三步 定义信号 ```pythonfrom flask import Flaskfrom signal import visit_signalapp = Flask(name)@app.route(‘/‘)def func1():username = request.args.get('username') if username: g.user = username visit_signal.send() else: return 'hello world' 123456789- ```python from blinker import Namespace my_pace = Namepace() visit_signal = my_pace.signal('visit-signal') def func(sender): from flask import g print(g.user) visit_signal.connect(func) 监听信号 ```pythonvisit_signal.send()#设置监听12345678910111213141516171819202122------### flask自带的信号机制:#### 模板渲染时传出的信号:- ```python #正常信号监听 from flask import Flask,render_template,template_rendered app = Flask(__name__) *********************************************************** #其一 def template_rendered_func(sender,template,context): print('sender:',sender) print('template:',template) print('context',context) template_randered.context(template_rendered_func) *********************************************************** @app.route('/') def func1(): return render_template('index.html') #异常信号监听 from flask import Flask,render_template,template_rendered,got_request_exception app = Flask(__name__) *********************************************************** #其二 def request_exception_log(sender,*args,**kwargs): print(sender) print(args) print(kwargs) got_request_exception.connect(request_exception_log) *********************************************************** @app.route('/') def func1(): return render_template('index.html')","link":"/2022/11/30/Study-notes-Python-Flask-%E4%BF%A1%E5%8F%B7%E6%9C%BA%E5%88%B6/"},{"title":"Flask g定向","text":"### g定向使用 from flask import Flask,g app = Flask(__name__) @app.route('/') def func(): g.username = 'root' return 'hello world' g的声明方法和local相同 使用g定向声明过的变量在整个flask文件中通用,及如果你在app中调用了另一个外部文件的函数,在这个函数中你是可以直接传入g来使用你在app中声明的变量的 这样可以不传入参数","link":"/2022/11/30/Study-notes-Python-Flask-g%E5%AE%9A%E5%90%91/"},{"title":"Flask 前端分页与下载","text":"Flask 前端分页与下载 123456789{% if goods.has_prev %} &lt;a href=&quot;/show?page={{ goods.prev_num }}&quot;&gt;上一页&lt;/a&gt;{% endif %}{% for foo in goods.iter_pages() %} &lt;a href=&quot;/show?page={{ foo }}&quot;&gt;{{ foo }}&lt;/a&gt;{% endfor %}{% if goods.has_next %} &lt;a href=&quot;/show?page={{ goods.next_num }}&quot;&gt;下一页&lt;/a&gt;{% endif %} 123456@app.route('/show/')def show(): page = int(request.args.get('page',1)) goods = GOOds.query.paginate(page,per_page=3) # app_list = app_sql.session.query(app_sql.Subordinate_forces).offset(num * 10).limit(10).all() return render_template('show.html',goods=goods) 下载判断 @page.route(“/goodsdown”)def download(): goodslist = Goods.query.all() data = [] for goods in goodslist: temp = {} temp[“id”] = goods.id temp[“name”] = goods.name temp[“price”] = goods.price temp[“img”] = goods.img data.append(temp) # 每次下载 向数据库中写入count count = Count.query.first() print(&quot;count&quot;, count) if not count: count = Count(count=1) db.session.add(count) db.session.commit() else: count.count += 1 db.session.commit() with open(&quot;static/data.json&quot;, &quot;w&quot;) as f: jstr = json.dumps(data, ensure_ascii=False) f.write(jstr) return send_file(&quot;static/data.json&quot;, as_attachment=True, cache_timeout=1)","link":"/2022/11/30/Study-notes-Python-Flask-%E5%88%86%E9%A1%B5/"},{"title":"Django 自动创建api接口","text":"自动创建api接口的文档 配置:setting文件配置: 下载模块 1pip install corsapi 配置SCHEMA_CLASS 123REST_FRAMEWORK = { 'DEFAULT_SCHEMA_CLASS': 'rest_framework.schemas.AutoSchema'} 在主路由中注册使用 12345from rest_framework.decumentation import include_docs_urlsfrom django.url import path,includeurlpatterns = [ path('docs/',include_docs_urls(title='API管理页面')),] 创建成功之后的画面如下图: 在这里面,还有一些别的设置,如,在模型类中设置字段的提示属性 : help_text = ‘ ’","link":"/2022/11/30/Study-notes-Python-Django-API%E6%8E%A5%E5%8F%A3%E7%94%9F%E6%88%90/"},{"title":"Django 表单提交与request接收","text":"Django表单提交 Django表单数据提交涉及到视图函数中的request参数 在django中, request对象封装了表单提交的数据 ***request.GET***中存储的是get方法提交的数据 ***request.POST***中存储的是post方法提交的数据 这些方法使用get方法可以获取到对应的值,获取的名称为表单中提交的数据的名字 这些数据将会在提交之后自动储存到request中去 request其他方法: get() 获取request对象中储存的数据 path 获取当前访问的url地址 method 获取当前的访问方法 POST 存储所有使用post方法提交过来的数据 GET 存储所有使用get方法提交过来的数据 FILES 存储提交过来的文件对象 COOKIES 存储cookie信息 session 存储session信息 session使用方法 直接使用字典的键值对获取方法 1request.session['cookie名'] 使用get方法获取,可以设置默认值 1request.session.get('cookie名,default=&quot;默认值&quot;) clear,删除session中保存的数据,但是删除掉的只是值 1request.session.clear() flash,删除整条数据,完全删除所有数据,数据表完全清除 1request.session.flash() del 删除目标数据的所有内容 1del request.session['cookie'] 设置session过期时间 1request.session.set_expiry(value) 当没有设置过期时间的时候,默认将会在两周之后过期 当value值为0的时候,将会在关闭浏览器的时候过期 当value大于0的时候,将会在value秒之后过期 has_key 判断session中有没有对应的cookie 1request.session.has_key('username') ajax方法提交数据ajax请求为异步请求async : true 该选项默认为true,即异步请求 ​ 异步请求时优先执行其他的javascript代码,然后才会执行ajax请求的代码 设定为false即为同步请求 使用ajax可以实现局部刷新 使用ajax方法提交数据 ```javascript$.ajax({ 'url':'/ajax_handle', 'dataType':'json', }).success(function(data){ alert(data.res) }) 123456789- 上述为ajax在页面中的代码部分- ```python from django.http import JsonResponse #处理ajax请求的是视图函数 def ajax_handle(request): return JsonResponse({'res':1}) 上述为ajax信息处理函数 ```pythonurlpatterns = [ #构建用于处理请求的网页url path('ajax_handle/',views.ajax_handle), ] 12345678910111213 - 以上就是基础的ajax请求构建 - ajax请求禁止返回一个页面,返回的页面将会无法正常显示------1. # 踩坑详情 - ``` APPEND_SLASH=False 踩坑第一条 使用ajax进行请求时无法绕过的一个坎,必须在settings配置文件之中配置这条数据才能继续进行下去,让人惊讶的是,教学视频之中居然没有提到这个问题,暂定为django版本过高导致的配置文件差异,从而出现的问题 $.ajax({ 'url':'/ajax_handle/', 'type':'POST', 'dataType':'json', 'data':{ username:username, password:password } }).success(function (data) { if (data.res == 1){ location.href = '/admin/' }else{ $(&quot;#err&quot;).show().html('账号名或密码错误') } }) 踩坑第二条 在路由函数方面,django的路由设定比起flask更加严谨 书写路由重定向时,如果你写的路由为”admin/“这样的格式 ​ 那么最终请求的网址实际上为; ​ 当前网页网址的路由后面加上你要跳转的路由 ​ 即: 127.0.0.1:8000/login/admin 在不添加前置/的情况下,默认为在当前网址的基础上再次进行路由访问 无论是ajax里面的url请求地址,还是JavaScript基础上进行的重定向,都需要注意这一点 反之,如果在路由前面加上/,访问的界面实际上就是在最基础的127.0.0.1:8000/的基础上进行的路由访问 这些坑需要谨记","link":"/2022/11/30/Study-notes-Python-Django-DJango%E8%A1%A8%E5%8D%95%E6%96%87%E4%BB%B6%E6%8F%90%E4%BA%A4/"},{"title":"Django DRF generics","text":"Django DRF generics 这模块中封装着GenericsAPIViews类,对APIViews进行了进一步的封装,比起APIViews,它的功能进一步扩展 ```from rest_framework.generics import GenericsAPIViews 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611623. ```pythonfrom django.http import HttpResponsefrom django.views import Viewfrom rest_framework.viewsets import ModelViewSetfrom .serializer import StudentsSerializer,Students_Serializerfrom . models import CuressInfo,Studentsclass StudentsAPIViews(View): # queryset = Students.objects.all() # serializer_class = StudentsSerializer def get(self,request): students = Students.objects.all() serializer = StudentsSerializer(instance=students,many=True) return HttpResponse(serializer.data) def post(self,request): import json data_string = request.body.decode() data_dict = json.loads(data_string) serializer = Students_Serializer(data=data_dict) serializer.is_valid(raise_exception=True) serializer.save() return HttpResponse('ok')class stdSl(View): def get(self,request,id): from .serializer import SerializerModel serializer = SerializerModel(data=Students.objects.all(),many=True) serializer.is_valid() print(serializer.data) return HttpResponse(serializer.data) def post(self,request,id): from .serializer import SerializerModel import json data_string = request.body.decode() data_dict = json.loads(data_string) serializer = SerializerModel(data=data_dict) serializer.is_valid(raise_exception=True) serializer.save() return HttpResponse(serializer.data) def put(self,request,id): instance = Students.objects.get(id=id) import json data_dict = json.loads(request.body.decode()) serializer = Students_Serializer(instance=instance,data=data_dict) serializer.is_valid(raise_exception=True) serializer.save() return HttpResponse('ok')from rest_framework.views import APIViewfrom rest_framework.response import Responsefrom .serializer import ModelSerializerAPIViewsclass StudentsModelAPIViews(APIView): def get(self,request): instance = Students.objects.all() serializer = ModelSerializerAPIViews(instance=instance,many=True) return Response(serializer.data) def post(self,request): # instance = Students.objects.get(id=id) data_dict = request.data serializer = ModelSerializerAPIViews(data=data_dict) serializer.is_valid(raise_exception=True) serializer.save() print(serializer.error_messages) return Response(serializer.data)from rest_framework.generics import GenericAPIViewclass StudentsGenericAPIViews(GenericAPIView): queryset = Students.objects.all() def get(self,request): all_dict = self.get_queryset() serializer = ModelSerializerAPIViews(instance=all_dict,many=True) return Response(serializer.data) def post(self,request): serializer = ModelSerializerAPIViews(data=request.data) serializer.is_valid(raise_exception=True) serializer.save() return Response(serializer.data)from rest_framework.generics import GenericAPIViewclass StudentsGenericAPIViews(GenericAPIView): queryset = Students.objects.all() serializer_class = ModelSerializerAPIViews def get(self,request): all_dict = self.get_queryset() serializer = self.get_serializer(instance=all_dict,many=True) return Response(serializer.data) def post(self,request): serializer = self.get_serializer(data=request.data) serializer.is_valid(raise_exception=True) serializer.save() return Response(serializer.data) # ListModelMixin 展示全部数据,里面封装的是全部数据的获取,方法名为list# CreateModelMixin 添加新的数据,这里面封存的方法名为create# DestroyAPIView 删除对应的数据,对应的方法为destroy# UpdateAPIView 修改对应的数据,对应的方法名为update# RetrieveAPIView 获取单条数据,需要有对应的pk数值传入,方法名为retrieve# RetrieveUpdateDestroyAPIView 这个里面封装了多个视图分支类,包括DestroyAPIView-------UpdateAPIView------RetrieveAPIView------GenericsAPIView四个,所以它里面包含了对应的方法from rest_framework.mixins import ListModelMixin,CreateModelMixinclass StudentsListModelMixin(GenericAPIView,ListModelMixin,CreateModelMixin): queryset = Students.objects.all() serializer_class = ModelSerializerAPIViews def get(self,request): return self.list(request) def post(self,request): return self.create(request) # from rest_framework.generics import ListAPIView,CreateAPIView,DestroyAPIView,UpdateAPIView,RetrieveAPIView# from rest_framework.generics import RetrieveUpdateDestroyAPIView# # 终极形态# # ModelAPIViewSet里面,相当于继承了以上的多种方法# 同时,在使用ModelAPIViewSet创建的类视图进行注册时,需要在as_view中传入参数具体操作方法请往下看==============&gt;from rest_framework.viewsets import ModelViewSetclass Students_ListModelMixin(ModelViewSet): queryset = Students.objects.all() serializer_class = ModelSerializerAPIViews from rest_framework.viewsets import ReadOnlyModelViewSetclass Students_ListModelMixin(ReadOnlyModelViewSet): # 当使用ReadOnlyModelViewSet时,所获得的方法是只读的,也即是说,里面并没有封装修改数据的方法 queryset = Students.objects.all() serializer_class = ModelSerializerAPIViews# ==================&gt; # 路由属性为显示全部数据时的注册方式 path('students5/',Students_ListModelMixin.as_view({ 'get':'list', 'post':'create' })), #当需要使用路由获取参数时的方式 re_path(r'students5/(?P&lt;pk&gt;\\d+)/',Students_ListModelMixin.as_view({ 'get':'retrieve', 'put':'update', 'delete':'destroy' })), # 确切的书,在as_view中,你需要将访问方式同函数绑定起来,确保你在访问时将会执行对应的方法 # 访问方式与方法的绑定一般整合为一个字典形式传入视图类 在视图函数函数类中调用多个序列化器1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from rest_framework.viewsets import ModelViewSetfrom .serializer import CuressInfo,ModelSerializerAPIViewsclass StudentsModelViewSet(APIView): queryset = Students.objects.all() serializer_class = ModelSerializerAPIViews # 重写get_serializer_class方法,在其中添加判断,如果对应的访问方式,就返回需要用到的序列化器 def get_serializer_class(self): if self.request.method == 'GET': return ModelSerializerAPIViews else: return CuressInfo def get(self,request): instance = self.get_queryset() serializer = self.get_serializer(instance=instance,many=True) return Response(serializer.data) def post(self,request): data = request.data serializer = self.get_serializer(data=data) serializer.is_valid(raise_excaption=True) return Response(serializer.data) class StudentsModelViewSet(ModelViewSet): queryset = Students.objects.all() serializer_class = ModelSerializerAPIViews # 重写get_serializer_class方法,在其中添加判断,如果对应的访问方式,就返回需要用到的序列化器 def get_serializer_class(self): #在视图集函数中,使用的并不是self.request.method进行判断,而是self.action if self.action == 'list': return ModelSerializerAPIViews else: return CuressInfo def get(self,request): instance = self.get_queryset() serializer = self.get_serializer(instance=instance,many=True) return Response(serializer.data) def post(self,request): data = request.data serializer = self.get_serializer(data=data) serializer.is_valid(raise_excaption=True) return Response(serializer.data)","link":"/2022/11/30/Study-notes-Python-Django-DRF%E4%B8%ADgenerics/"},{"title":"Django DRF 基础操作","text":"DRF初试 创建一个新的序列化器 Serializer序列化器创建,通常这个序列化器我们使用一个新的文件承载 ```pythonfrom rest_framework import serializerfrom .models import *class StudentsSerializer(serializer.ModelSerializer):class Meta: model = Students fields = '__all__' 1234567891011121314151617181920 - 在创建序列化器的时候,需要直接或者间接的继承serializer.Serializer这个类 - 当序列化器继承自serializer.ModelSerializer的时候,需要在序列化器的模型类中声明要使用的模型类model,以及要使用的字段fields - 当fields的值为'__all__'的时候,就相当于对应着这个模型类中的全部字段 - 字段声明 - 2. 在view中使用这个序列化器 - ```python from rest_framework.viewsets import ModelViewSet from .serializer import StudentsSerializer from .models import * class StudentsAPIViews(ModelViewSet): queryset = Students.objects.all() serializer = StudentsSerializer 在settings文件中注册test_framework应用 ```pythonINSTALLED_APPS = ['test_framework' ]123456789101112134. 设置test_framework模块路由 - ```python from test_framework.routers import DefaultRouter from .views import StudentsAPIViews urlpatterns = [] router = DefaultRouter() router.register('student',StudentsAPIViews,basename='StudentsAPIViews') urlpatterns += router.urls 在主路由中配置好应用的路由 from fjango.contrib import admin from django.urls import path,include urlpatterns = [ path('admin/',admin.site.urls), path('api/',include('drf_app.urls')), ]","link":"/2022/11/30/Study-notes-Python-Django-DRF%E4%BD%BF%E7%94%A8/"},{"title":"Django DRF 数据分页器","text":"分页器类书写 相关代码 123456789101112131415# 废话不多说,直接上代码from rest_framework import paginationclass PaginationStudents(pagination.PageNumberPagination): page_size = 5 # 在不传入page_size参数时的默认长度 page_size_query_param = 'size' # 设定在使用url传递参数时每页展示数量的参数名 page_query_param = 'page' # 设定在使用url传递参数时展示的页数的参数名 max_page_size = 5 # 设定每页的最大展示数量 #注意,在使用分页的时候,可以不传入每页展示数量,默认将会使用设定好的page_size 创建好分页器类之后,进行使用 123456from rest_framework.views import ModelAPIViewclass MyPaginations(ModelAPIView): queryset = Students.objects.all() serializer_class = MySerializers pagination_class = PaginationStudents 使用时的传参方式 127.0.0.1/api/students5/?page=1&amp;size=5 page为页码 size为单页显示的最大数量 ​","link":"/2022/11/30/Study-notes-Python-Django-DRF%E6%95%B0%E6%8D%AE%E5%88%86%E9%A1%B5%E5%99%A8/"},{"title":"Django DRF 封装的数据模块","text":"Django Rest Framework APIViews from rest_framework.views import APIViews 封装在DRF中的视图模块,相比直接使用django中的views模块,这个模块的使用更加的严谨,功能更多,更好用 从页面中使用request获取到的json数据将会自动转换成字典类型的数据,方便使用,不需要额外导入一次json模块 其他方面的使用views使用方式相同 从根本上来看,APIViews继承自Views","link":"/2022/11/30/Study-notes-Python-Django-DRF%E4%B8%AD%E7%9A%84APIViews%E8%A7%86%E5%9B%BE%E7%B1%BB/"},{"title":"Django DRF 文件上传及访问","text":"DRF文件上传及访问 关于DRF文件上传:​ 其实我的掌握并不强,目前也只能完成非前后端分离的状态提交的数据,在前后端分离的状态,我提交的数据不知道为什么始终无法传上来,查验代码之后提示是我的上传数据格式不一致,但是查了好久都没找到具体的解决办法 ​ 那么,先演示一下使用drf进行的django工程内部的提交: ​ 123456789101112131415161718192021222324252627282930313233343536373839404142class imgCal(ModelViewSet): queryset = imageInfo.objects.all() serializer_class = StudentsSerializer #重写get方法,确保在进行get访问时获取进行文件上传的模板 def get(self,request): return render(request,'./imgfrom.html') #在这里,我选择了重写ModelViewSet中自带的create方法,并将其更名为post #模板自带的create方法所获得的request.data并不是表单直接提交的数据,因为个人原因(其实就是懒)我将request.data方法改掉,改为使用表单的方式获取数据,然后存储到一个字典之中, def post(self, request, *args, **kwargs): # 创建一个新的字典 a_dict = {} # 获取上传的文件,并存储到创建的字典中去 a_dict['name'] = request.POST.get('name') a_dict['img'] = request.FILES.get('img') # 最后得到的,是这样一个集合 # &lt; QueryDict: {'name': ['小小'], 'img': ['C:\\\\fakepath\\\\003.png']} &gt; print(request.GET) # 创建序列化器对象,将床架你的字典作为data放入序列化器 serializer = self.get_serializer(data=a_dict) print(serializer.is_valid()) # create方法原生的使用方法,转到下一个类方法中 self.perform_create(serializer) headers = self.get_success_headers(serializer.data) # 将最终结果返回回去,不需要修改 return Response(serializer.data, status=status.HTTP_201_CREATED, headers=headers) def perform_create(self, serializer): # 对序列化器结果进行判断,这里加不加都可以,之所以加上这层判断,只是我在书写过程中,为了确保结构完整,并且能够在每一个位置进行判断,确定bug位置而书写的 #当序列化通过时,执行save if serializer.is_valid(raise_exception=True): serializer.save() # 当序列化未通过时,将会执行以下数据 else: print('数据无效',serializer.data) return Response(serializer.data) def get_success_headers(self, data): try: return {'Location': str(data[api_settings.URL_FIELD_NAME])} except (TypeError, KeyError): return {} 以下是序列化器部分: 12345class StudentsSerializer(serializers.ModelSerializer): class Meta: model = imageInfo fields = '__all__' 平平无奇的序列化器,没有添加什么额外的配置要求 值得一提的是,在数据库中我并不是使用的imageFields,而是使用了兼容性感觉更高的filefield 下面是数据库配置: 12345678910111213141516def upload_to(instance, fielname): return '/'.join([settings.MEDIA_ROOT, instance.user_name, fielname])#以上代码是自己重写的文件下载模块,已弃之不用class imageInfo(models.Model): name = models.CharField(max_length=50) img = models.FileField(upload_to='') # 必备参数,upload # 如果你不需要再设定好的下载路径中再次准备一个文件夹进行文件存储,可以直接为空,即: upload='' # 当然,在使用之前,需要准备好MEDIA_URL以及MEDIA_ROOT,如果想要在网页中能直接通过网址进行蹄片查看,需要配置好meida路由 def __str__(self): return self.name class Meta: db_table = 'imageinfo' 以下是路由配置: 123456789re_path('media/(?P&lt;path&gt;.*)',serve,{'document_root':settings.MEDIA_ROOT}),#setting文件配置MEDIA_URL = 'static/media/'MEDIA_ROOT = os.path.join(BASE_DIR, 'static\\media') 在完成上述的代码之后,只需要配置好模板文件即可,正常的form表单的提交方法,后台的代码将会自行对获得的数据进行解析存储","link":"/2022/11/30/Study-notes-Python-Django-DRF%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E4%B8%8E%E9%9D%99%E6%80%81%E6%96%87%E4%BB%B6%E8%AE%BF%E9%97%AE/"},{"title":"Django DRF 框架 权限验证其相关","text":"DRF框架权限验证器: 基础验证器 drf中集成了关于权限验证的一些基本模块,具体可以分为: ​ 所有用户均可访问———AllowAny ​ 登录用户可访问–-——IsAuthenticated ​ 仅限管理员权限可以访问——–IsAdminUser 关于基本权限验证模块的使用方法 话不多说,上代码 1234567891011121314151617181920212223242526272829from rest_framework.permissions import AllowAnyfrom rest_framework.views import APIViewclass MyPermissions(APIView): permission_classes = [AllowAny] #当视图函数中设定permission_classes的值为AllowAny时,这个页面允许任何人访问,也就是说,相当于允许游客进行访问 def get(selfg,request): return HttpResponse('权限验证通过,欢迎') from rest_framework.permissions import IsAuthenticated,IsAdminUserfrom rest_framework.views import APIViewclass MyPermissions(APIView): permission_classes = [IsAuthenticated] #当视图函数中设定permission_classes的值为IsAuthenticated时,这个页面允许任何用户访问,也就是说,只要你进行登录,就可以进行访问 def get(selfg,request): return HttpResponse('权限验证通过,欢迎') from rest_framework.permissions import IsAdminUserfrom rest_framework.views import APIViewclass MyPermissions(APIView): permission_classes = [IsAdminUser] #当视图函数中设定permission_classes的值为IsAdminUser时,这个页面只允许拥有管理员权限的用户进行访问 def get(selfg,request): return HttpResponse('权限验证通过,欢迎') 自定义权限验证器: 在drf中,允许我们自定义一些权限验证器,来实现我们想要的一些功能,比如说.……留后门、留后门,还有…留后门之类的操作,当然,如果是大公司,同时自己又没有高级权限的话,不建议留后门,毕竟如果留后门被发现了的话不太好,哈哈哈哈 上代码 首先创建一个自定义的权限验证,这个验证就不需要使用自带的模块了,直接自定义 12345from rest_framework.permissions import BasePermissionclass My_permissions(BasePermission): def has_permission(self,request,view): if request.query_paramy.get('user') == 'root': return True 在调用权限验证的时候选择使用自定义的权限验证器 1234class MyPermissions(APIView): permission_classes = [My_permissions] def get(self,request): return HttpResponse('权限验证通过,欢迎') 限流全局限流设置: 在setting文件中设定限流要求 1234567891011121314REST_FRAMEWORK = { 'DEFAULT_THROTTLE_CLASSES':( 'rest_framework.throttling.AnonRateThrottle', 'rest_framework.throttling.UserRateThrottle' ), 'DEFAULT_THROTTLE_RATES':{ #单位时间内访问次数限制 #anon匿名用户限流 'anon':'1/second', #user注册用户限流 'user':'1/second' }} 注册限流信息时,所需要使用的rest_framework模块需要现在INSTALLED_APPS中进行配置才能进行使用,同时,这里使用的时候需要与注册的app名相同 局部限流设置: 无论是局部限流还是全局限流,都需要现在setting文件中进行配置,但是这个配置并不相同 123456REST_FRAMEWORK = { 'DEFAULT_THROTTLE_RATES':{ 'anon':'1/second', 'user':'1/second' }} 在进行局部限流时,不需要导入自带的限流模块 123456from rest_framework.views import APIViewfrom rest_framework.throttling import UserRateThrottleclass Mythrottling(APIView): throttling_class = [UserRateThrottle] def get(self,request): return Response('这是投票页面')","link":"/2022/11/30/Study-notes-Python-Django-DRF%E6%A1%86%E6%9E%B6%E7%9A%84%E6%9D%83%E9%99%90%E9%AA%8C%E8%AF%81%E5%99%A8%E4%B8%8E%E9%99%90%E6%B5%81/"},{"title":"Django DRF 验证器机制","text":"DRF验证器机制 创建基于drf框架的验证器 ```pythonfrom rest_framework import serializerclass SerializerStudents(serializer.Serializer):name = serializer.CharField(max_length=10,min_length=2) 1234567891011121314151617181920212223242. 在创建好的视图类中使用这个验证器类 ```python class Students(View): def post(self,request): #获取客户端传输过来的json数据并进行解码操作 data_string = request.body.decode() #导入json模块 import json #使用json模块的方法对获取到的字符串进行转换,变成一个字典 data_dict = json.loads(data_string) #使用创建好的验证器类 from .serializer import SerializerStudents serializer = SerializerStudents(data=data_dict) #验证器类的is_valid方法返回的数据为bool数据,当通过验证时,返回结果为true,否则返回false print(serializer.is_valid()) #is_valid方法的属性为raise_excaption #这个属性默认时关闭的,当他开启时,意味着当验证出错时,将会直接报错,并直接停止代码的继续运行, print(serializer.is_valid(raise_exception=True)) #返回报错信息,这个并不会阻止程序的继续运行 print(serializer.erreo_messages) # 创建一个验证器方法进行验证 单字段验证 ```pythondef validate_name(self,data):#传入的参数可以直接设定为data if data == 'root': raise serializer.ValidationError(message='用户名不能为root') #当满足条件时,返回错误原因和信息 return data #当通过验证时,返回原数据data 123456789102. 全局字段验证 - ```python def validate(self,data_dict): #验证方法和单字段验证基本相同,区别是传入的数据类型是字典,进行判断是只需要从字典中取出对应的值即可进行判断,要注意的是,最终依然要将这个数据返回回去 name = data_dict.get('name') password = data_dict.get('password') return data_dict 创建一个验证器类外部的方法进行使用 在验证器类外部进行函数定义 ```pythondef check_validate(data):if data=='user': raise serializer.ValidationError(message='不能使用user作为用户名') return data 123452. 在验证器类中定义字段属性时使用 - ```python name = serializer.CharFIeld(max_length=20,min_length=2,validates=[check_validate]) 在验证器类中重写create(添加一条数据)方法和update(修改一条数据)方法 重写create方法 ```pythondef create(self,validate_data):name = validate_data.get('name') instance = Students.objects.create( name=name ) return instance 1234567892. 重写update方法 - ```python def update(self,instance,validate_data): name = validate_date,get('name') instance.name = name instance.save() return instance 使用这两种方法 通用的激活方法 ```pythonserializer.save()123456789101112131415 - 通过验证之后的数据,可以使用方法save()2. create可以直接使用,当验证时传入的参数仅有一个data时,将会调用create方法,而不会激活update方法3. 使用update方法 1. ```python def update(self, instance, validated_data): name = validated_data.get('name') curess = 1 instance.name = name instance.curess_id = curess instance.save() return instance class stdSl(View): def put(self,request,id): instance = Students.objects.get(id=id) import json data_dict = json.loads(request.body.decode()) serializer = Students_Serializer(instance=instance,data=data_dict) serializer.is_valid(raise_exception=True) serializer.save() return HttpResponse('ok') 以上内容为serializer.Serializer组件的使用,相比较起来,drf框架中其实已经封装好了专用的模型类序列化器 serializer.ModelSerializer: 验证器使用时的属性和字段:","link":"/2022/11/30/Study-notes-Python-Django-DRF%E9%AA%8C%E8%AF%81%E5%99%A8%E6%9C%BA%E5%88%B6%E4%BD%BF%E7%94%A8/"},{"title":"Django 自动逸错误捕获机制","text":"自定义错误捕获: 在drf和django框架中,仅仅内置了一些常见的错误捕获,如: ​ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859HTTP_100_CONTINUE = 100HTTP_101_SWITCHING_PROTOCOLS = 101HTTP_200_OK = 200HTTP_201_CREATED = 201HTTP_202_ACCEPTED = 202HTTP_203_NON_AUTHORITATIVE_INFORMATION = 203HTTP_204_NO_CONTENT = 204HTTP_205_RESET_CONTENT = 205HTTP_206_PARTIAL_CONTENT = 206HTTP_207_MULTI_STATUS = 207HTTP_208_ALREADY_REPORTED = 208HTTP_226_IM_USED = 226HTTP_300_MULTIPLE_CHOICES = 300HTTP_301_MOVED_PERMANENTLY = 301HTTP_302_FOUND = 302HTTP_303_SEE_OTHER = 303HTTP_304_NOT_MODIFIED = 304HTTP_305_USE_PROXY = 305HTTP_306_RESERVED = 306HTTP_307_TEMPORARY_REDIRECT = 307HTTP_308_PERMANENT_REDIRECT = 308HTTP_400_BAD_REQUEST = 400HTTP_401_UNAUTHORIZED = 401HTTP_402_PAYMENT_REQUIRED = 402HTTP_403_FORBIDDEN = 403HTTP_404_NOT_FOUND = 404HTTP_405_METHOD_NOT_ALLOWED = 405HTTP_406_NOT_ACCEPTABLE = 406HTTP_407_PROXY_AUTHENTICATION_REQUIRED = 407HTTP_408_REQUEST_TIMEOUT = 408HTTP_409_CONFLICT = 409HTTP_410_GONE = 410HTTP_411_LENGTH_REQUIRED = 411HTTP_412_PRECONDITION_FAILED = 412HTTP_413_REQUEST_ENTITY_TOO_LARGE = 413HTTP_414_REQUEST_URI_TOO_LONG = 414HTTP_415_UNSUPPORTED_MEDIA_TYPE = 415HTTP_416_REQUESTED_RANGE_NOT_SATISFIABLE = 416HTTP_417_EXPECTATION_FAILED = 417HTTP_422_UNPROCESSABLE_ENTITY = 422HTTP_423_LOCKED = 423HTTP_424_FAILED_DEPENDENCY = 424HTTP_426_UPGRADE_REQUIRED = 426HTTP_428_PRECONDITION_REQUIRED = 428HTTP_429_TOO_MANY_REQUESTS = 429HTTP_431_REQUEST_HEADER_FIELDS_TOO_LARGE = 431HTTP_451_UNAVAILABLE_FOR_LEGAL_REASONS = 451HTTP_500_INTERNAL_SERVER_ERROR = 500HTTP_501_NOT_IMPLEMENTED = 501HTTP_502_BAD_GATEWAY = 502HTTP_503_SERVICE_UNAVAILABLE = 503HTTP_504_GATEWAY_TIMEOUT = 504HTTP_505_HTTP_VERSION_NOT_SUPPORTED = 505HTTP_506_VARIANT_ALSO_NEGOTIATES = 506HTTP_507_INSUFFICIENT_STORAGE = 507HTTP_508_LOOP_DETECTED = 508HTTP_509_BANDWIDTH_LIMIT_EXCEEDED = 509HTTP_510_NOT_EXTENDED = 510HTTP_511_NETWORK_AUTHENTICATION_REQUIRED = 511 但是一些偏门的,或者说,更加细微之处的错误,就需要我们自定义一些错误捕获函数来进行捕获 那么,废话不多说,上代码 第一步,创建一个错误捕获函数的文件 12345678910111213141516from rest_framework import statusfrom rest_framework.response import Responsefrom rest_framework.views import exception_handlerdef func(exc,context): #实例化框架自带的错误捕获,先进行筛选 response = exception_handler(exc,context) #当没有捕获到错误之后,执行我们自定义的错误验证机制 if response == None: '''开始进行判断''' if isinstance(exc,ZeroDivisionError): print('服务器错误') response = Response('服务器错误',status=status.HTTP_500_INTERNAL_SERVER_ERROR) #上面的验证返回的结果存储到response,这是一个已经完成序列化的数据,可以直接返回 return response 在配置文件setting中进行配置(通常情况下,我们创建一个新的文件夹来存放这个文件,然后再setting中以app的形式注册这个文件夹) 123REST_FRAMEWORK = { 'EXCEPTION_HANDLER':'utiles.excaptions.func'} 完成了上述的工作之后,当我们的码出现错误之后,将会自动进行错误捕获","link":"/2022/11/30/Study-notes-Python-Django-DRF%E8%87%AA%E5%AE%9A%E4%B9%89%E9%94%99%E8%AF%AF%E6%8D%95%E8%8E%B7%E6%9C%BA%E5%88%B6/"},{"title":"Django filter过滤器","text":"DJango-filter模块使用: 目前,过滤器django_filter只能在ListAPIView的子类中使用 过滤 ```pythonfrom rest_framework.generics import ListAPIViewfrom django_filters.rest_framework import DjangoFilterBackendclass MYfilter(ListAPIView): queryset = Students.objects.all() serializer_class = StudentsSerializer filter_backends = [DjangoFilterBackend] filter_fields = (&quot;id&quot;,) #注意,filter_fields的值必须是一个元组!!!! 123- 使用方法 在网址url之后追加——-/?id=1 12345678910111213 2. ### 排序 - ```python from rest_framework.filters import OrderingFilter from rest_framework.generics import ListAPIView class MYfilter(ListAPIView): queryset = Students.objects.all() serializer_class = StudentsSerializer filter_backends = [OrderingFilter] ordering_fields = ['id'] 使用方法 123在网址url之后追加------/?ordering=xxxx为ordering_fields中准备的用来作为排序基准的字段倒序为在xx前加上-号","link":"/2022/11/30/Study-notes-Python-Django-Django-filter%E8%BF%87%E6%BB%A4%E5%99%A8%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"title":"Django cookie 与 session","text":"COOKIES 设置cookie 创建一个全新的HttpResponse对象 12response = HttpResponse('设置cookie')response.set_cookie('num',1,max_age=10*24*3600) 获取cookie 12num = request.COOKIES['num']return HttpResponse(num) 设置过期时间 max_age,设置单位为秒 1response.set_cookie('num',1,max_age=10*24*3600) exprice,设置较为复杂,但是适合长期设置 123from django.http import JsonResponse,HttpResponsefrom datetime import datetime,timedeltaresponse.set_cookie('num',1,expires=datetime.now() + timedelta(days=10)) 相较于cookie,session安全性较高,在保存一些重要数据时,更适合使用session来进行保存,如用户的帐号,密码,私人信息等sessions 设置session ```python#设置sessiondef set_session(request):request.session['username'] = 'root' request.session['age'] = '000000' return HttpResponse('设置成功') 1234567892. 获取session 1. ```python #获取session def get_session(request): username = request.session['username'] age = request.session['age'] return HttpResponse(username + ':' + str(age)) session使用方法 直接使用字典的键值对获取方法 1request.session['cookie名'] 使用get方法获取,可以设置默认值 1request.session.get('cookie名,default=&quot;默认值&quot;) clear,删除session中保存的数据,但是删除掉的只是值 1request.session.clear() flash,删除整条数据,完全删除所有数据,数据表完全清除 1request.session.flash() del 删除目标数据的所有内容 1del request.session['cookie'] 设置session过期时间 1request.session.set_expiry(value) 当没有设置过期时间的时候,默认将会在两周之后过期 当value值为0的时候,将会在关闭浏览器的时候过期 当value大于0的时候,将会在value秒之后过期 has_key 判断session中有没有对应的cookie 1request.session.has_key('username')","link":"/2022/11/30/Study-notes-Python-Django-Django%E4%B8%ADcookie%E5%92%8Csession/"},{"title":"Django 中间件","text":"django中间件 中间件需要使用request参数的META属性 获取用户ip地址 ​ 如下所示 1request.META['REMOTE_ADDR'] 自定义中间件类对访问的ip进行限制,从而对特定ip进行封号处理 创建一个全新的中间件文件 创建一个新的中间件问价 middleware.py ```pythonfrom django.http import HttpResponse class BlockedIPSMiddleware(object): EXCLUDE = ['192.168.1.238','127.0.0.1'] def process_view(self,request,view_func,*view_args,**view_keargs): user_ip = request.META['REMOTE_ADDR'] if user_ip in BlockedIPSMiddleware.EXCLUDE: return HttpResponse(&quot;拒绝访问&quot;) 1234567891011121314- 注意,2.0以后无法使用以上的方法进行中间价创建- ```python from django.http import HttpResponse from django.utils.deprecation import MiddlewareMixin class BlockedIPSMiddleware(MiddlewareMixin): EXCLUDE = ['192.168.1.238','127.0.0.1'] def process_view(self,request,view_func,*view_args,**view_keargs): user_ip = request.META['REMOTE_ADDR'] if user_ip in BlockedIPSMiddleware.EXCLUDE: return HttpResponse(&quot;拒绝访问&quot;) 创建好之后,再setting文件中进行中间件注册 setting.py文件中 MIDDLEWARE = [要注册的中间件函数路径] 该路径需要精确到中间件函数类 运行django项目,可以看到,被记录的函数已经无法正常访问这个项目了 中间件覆盖了项目所属所有的页面 process_exception 这个中间件函数将会在视图函数出错的时候进行调用 注意,这个函数在进行调用的时候,如果多个中间件类都定义了这个函数并且都进行了注册,将会以从下往上的顺序进行调用","link":"/2022/11/30/Study-notes-Python-Django-Django%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"title":"Django 反转url","text":"Django反转url 反转url: 使用模板语言url 123- {% url ‘index’ %}- {% url ‘url路由函数名’ %} 在url定义中设定路由函数名 path(‘index’,views.index,name=‘index’) 关于应用注册的路由的反转使用 注册应用路由时添加namespace path(‘’,include(‘besate’,namespace=‘basete’)) 在应用的urls文件中添加 app_name = ‘besate’ 使用反转12 - {% url ‘besate:index’ %} 关于带参数的路由的反转编译 1. 1{% url ‘index’ 参数数值 %} 位置参数 1{% url ‘index’ 参数名=参数数值 %} 命名参数 在视图中进行反转url操作 导入模块 原始的方法是从另一个模块中导入,但是3.0中包含了这个模块 所以这个模块现在集成到了django.shortcuts中,只需要再导入render的时候顺便就可以一并导入进去了 from django.shortcuts import render,reverse,redirect 传入参数 传入位置参数时 reverse(‘three:index’,args=(1,2,3)) 传入命名参数时 reverse(‘three:index’,kwargs={‘a’:1,‘b’:2})","link":"/2022/11/30/Study-notes-Python-Django-Django%E5%8F%8D%E8%BD%ACurl/"},{"title":"Django 静态文件访问","text":"Django 关闭debug模式后的静态文件访问 ```STATIC_URL = ‘/static/‘#配置默认的static静态文件夹位置STATIC_ROOT = ‘static’STATICFILES_DIRS = [ os.path.join(BASE_DIR,'/static/') ] 1234567891011121314 - ### 设置setting2. 设置url 1. ```. from django.views import static ##新增 from django.conf import settings ##新增 from django.conf.urls import url ##新增 ............ url(r'^static/(?P&lt;path&gt;.*)$', static.serve, {'document_root': settings.STATIC_ROOT}, name='static'), 固定公式可以直接使用 模板中渲染 &lt;div id=&quot;box1&quot;&gt;&lt;img src=&quot;{% url 'static' path='logo/logo0.png' %}&quot; alt=&quot;&quot;&gt;&lt;/div&gt; 使用url反转路由","link":"/2022/11/30/Study-notes-Python-Django-Django%E5%85%B3%E9%97%ADdebug%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%90%8E%E7%9A%84%E9%9D%99%E6%80%81%E6%96%87%E4%BB%B6%E8%AE%BF%E9%97%AE/"},{"title":"Django 元选项","text":"元类 通过在模型类中定义元类来对创建好的数据库表进行约束,以确保生成的数据表不依赖于应用文件的名字123456from django.db import modelsclass BookInfo(models.Manager): btitle = models.CharField(max_length=20) class Meta: db_table = 'bookinfo' 定义元类Meta,设定好创建的数据库的名字之后,执行数据库迁移产生的数据库名字将不会安装应用名_模型类名的规则进行创建模板错误试图:调试模式: ​ 在网站上线时,应关闭DE_BUG模式,该选项在setting文件中进行配置,默认为true ​ 默认选项 1234DEBUG = TrueALLOWED_HOSTS = [] ​ 修改选项 123DEBUG = FalseALLOWED_HOSTS = ['*'] ​ ALLOWED_HOSTS选项设定的是允许访问的IP地址,一般默认设置为*,即允许所有 ​ 错误访问视图 当出现错误请求或者服务器异常时,我们可以对从异常操作的响应进行,如自定义错误页面 常见的错误有404(页面不存在)、500(服务器错误) 自定义错误访问视图需要在templates模板文件夹下创建错误访问模板,文件名为对应的http状态码, 如: 404.html 错误试图的定义需要关闭debug模式","link":"/2022/11/30/Study-notes-Python-Django-Django%E5%85%83%E9%80%89%E9%A1%B9/"},{"title":"Django 自带后台管理","text":"Django后台管理操作 #后台管理系统为admin.py文件 1).本地化 ​ 修改语言格式 修改为中文:zh-hans ```pythonLANGUAGE_CODE = ‘zh-hans’ 1234567891011- ​ 修改时区 - 修改为中国时区 - 没有北京的时间,所以我们选择上海 - Asia/shanghai - ```python TIME_ZONE = 'Asia/shanghai' 2).创建管理员 创建超级管理员 ```python manage.py createsuperuser 1234567891011121314- 使用管理员帐号可以登入后台管理界面进行操作## 3).注册设定好的模型类- 在对应文件的amdin文件中注册模型类 - ```python class HeroInfoAdmin(admin.ModelAdmin): list_display = ['hname', 'hcomment', 'hbook_id', 'hbook_id'] #在这里可以设定展示出来的字段总览 #使用admin模块中的site.register进行注册 admin.site.register(BookCateInfo,BookCateInfoAdmin) 在后台管理界面中想要将每一项数据库中的数据渲染出来需要在模型类中重写__str__方法 ```pythondef str(self): return self.btitle #重写str方法,会使这个表渲染在管理界面时的样式改变 1234567891011- 在数据库模型中设定方法 - ```python def user(self): return self.username user.admin_order_field = 'username' #为创建的方法字段添加排序属性 user.short_description = '用户名' #为创建的方法附加一个别名用于展示 这个方法适用于自定义模型类的方法时使用 在注册模型类时可以添加的属性: list_display 设定管理页面中娴熟的模型类字段 这个字段可以可以使用模型类中自定义好的方法 list_per_page 设定管理页面中每页显示的数据量 list_per_page = 10(每页展示十条数据) action_on_bottom 设定下方的操作框 在action_on_bottom = True时,将会在展示页的下方再次增加一个操作框 底部操作框在不设定的时候默认为False action_on_top 设定上方的操作框 在action_on_top = False时,将会取消展示页上方的操作框, 顶部操作框默认为True list_filter 在页面右侧添加快速过滤选项,该选项遵循注册模型类时的约束进行 ```pythonlist_filter = [‘username’]#这个属性的参数是一个列表,可以在其中添加要过滤的字段名#过滤器将会列出所有的字段名方便选择 123456789- search_fields - 在页面顶部添加搜索框 - ```python search_fields = ['username'] #这个属性的参数是一个列表,可以在其中添加字段 #在执行搜索时,将会在指定字段的数据中进行匹配","link":"/2022/11/30/Study-notes-Python-Django-Django%E5%90%8E%E5%8F%B0%E7%AE%A1%E7%90%86/"},{"title":"Django 数据库关系属性和查询语句","text":"数据库关系属性 一对多关系 ForeignKey 外键专属,一对多关系设定,通常情况下,这个属性设置在多类属性中,关联一类属性 ```pythonnews_type = models.ForeignKey(‘NewsTypeInfo’,on_delete=models.CASCADE) 1234567891011 - ## 多对多关系 - ManyToManyField - 多对多关系建立字段,这个字段可以写在任意一个里面,并不影响使用 - ```python news_type = models.ManyToManyField('NewsInfo') 一对一关系 OneToOneField 通过模型类实现关联查询语句:​ 1234567from Two.models import *b = HeroInfo.objects.filter(hbook__id=1)#通过一对多关系获取对象的方法#关系字段加上双下划线加上对应表中的字段即可进行查询,但需要注意b = HeroInfo.object.get(id=1)b.hbook#以上两种方法都能进行查询,但是第一种方法查询出来的结果是一个查询集,第二种拿到的是一个对象 在没有关系的情况下查询的话,就不能使用关系属性字段进行查询,这个这时候,就需要用模型类的名字进行查询,用模型类的名字代替关系属性字段的名字 自关联属性字段查询:设定自关联属性字段:​ 12345class AreaInfo(models.Model): a_title = models.CharField(max_length=20) #设置关联属性字段,关联自身,即self #这个字段即意味着与自身关联 a_parent = models.ForeignKey('self',on_delete=models.CASCADE,null=True,blank=True) 自关联属于特殊的一对多在查询时遵循一对多的查询方式由一查多时使用字段名__set.all()进行查询 由多查一时使用字段名查询","link":"/2022/11/30/Study-notes-Python-Django-Django%E6%95%B0%E6%8D%AE%E5%BA%93%E5%85%B3%E7%B3%BB%E5%B1%9E%E6%80%A7/"},{"title":"Django 数据库操作","text":"Django数据库操作 #Django中不需要定义id字段数据库创建–基础字段: CharField(定义一个字符串类型的字段) max_length=xx, 定义字符串最大长度 DateField(定义一个时间类型的字段) 时间类型字段一共有三个,TimeField BooleanField FileField 文件字段 TextField 大文本字段,超过4000字符的文本使用这个字段 ImageField 上传图片 ForeignKey 外键 第一个参数为要关联的模型类 第二个参数必须加上下面这个参数,才能保证正确1on_delete=models.CASCADE 在django2.0之后的版本中不在有默认的on_delete,所以这里需要手动传入这个参数 ```on_delete=None, # 删除关联表中的数据时,当前表与其关联的field的行为on_delete=models.CASCADE, # 删除关联数据,与之关联也删除 on_delete=models.DO_NOTHING, # 删除关联数据,什么也不做 on_delete=models.PROTECT, # 删除关联数据,引发错误ProtectedError # models.ForeignKey('关联表', on_delete=models.SET_NULL, blank=True, null=True) on_delete=models.SET_NULL, # 删除关联数据,与之关联的值设置为null（前提FK字段需要设置为可空,一对一同理） # models.ForeignKey('关联表', on_delete=models.SET_DEFAULT, default='默认值') on_delete=models.SET_DEFAULT, # 删除关联数据,与之关联的值设置为默认值（前提FK字段需要设置默认值,一对一同理） on_delete=models.SET, # 删除关联数据, a. 与之关联的值设置为指定值,设置：models.SET(值) b. 与之关联的值设置为可执行对象的返回值,设置：models.SET(可执行对象) 1234567891011121314151617181920212223242526272829303132333435363738## 字段属性- default- null - 判断字段能不能为空 - 默认为false- primary_key - 主键约束- Foregin_key - 外键约束\\- autoincerment - 自增长- blank - 后台出入数据时是否允许为空- verbose_name - 设定字段在管理页面显示的名称## 插入、更新和删除#### 插入、更新​ save()#### 删除​ delete------## 生成迁移文件- ```python python manage.py makemigrations #生成迁移脚本 ```pythonpython manage.py migrate#将迁移脚本映射到数据库之中 123456789101112131415161718192021------## 修改Django数据库默认配置:```python#修改setting文件中的database中的参数DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', # 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), 'NAME': 'django_demo',#数据库名称,需要手动创建好数据库 'USER': &quot;root&quot;, #用户名,默认为root 'PASSWORD': '000000',#mysql密码 'HOST': 'localhost',#数据库地址,一般为本机127.0.0.1,通localhost 'PORT': '3306'#MySQL数据库端口号.默认3306 }} 运行整个项目:1python manage.py runserver 通过models文件在数据库中添加数据12345678910python manage.py shell#进入交互模式from 模块名.models import 模型类名from first.models import BookInfob = BookInfo()b.btitle = '天龙八部'from datetime import dateb.bpub_data = date(1998,3,22)b.save() 通过模型类从数据库中获取对应的数据1b1 = BookInfo.objects.get(id=1) #通过模型对象BookInfo的objects方法的get方法获取满足对应条件的字段 type(b1) #返回对象是First.models.BookInfo 修改表中数据:12b1.btitle = '新的数据'b1.save() 从根本上来说,修改数据仅仅只是拿到数据之后重新赋值,然后进行提交而已 1b1.delete() 从数据库中删除这条数据 外键查询: ForeignKey设置外键 1234567891011121314151617181920from django.db import models# Create your models here.class BookInfo(models.Model): #书名 btitle = models.CharField(max_length=10) #上架时间 bpub_data = models.DateField()class HeroInfo(models.Model): #人物名 hname = models.CharField(max_length=10) #人物性别,true为男,false为女 hgender = models.BooleanField(default=False) #简介 hcomment = models.CharField(max_length=300) #人物所属书籍,一对多关系 #设置外键 hbook = models.ForeignKey('BookInfo',on_delete=models.CASCADE) 外键创建好之后的字段名为设定好的models模型里面的外键字段加上_id,这个id和另一个表的id对照 b = BookInfo() b.btitle = '天龙八部' from datetime import date b.bpub_data = date(1998,2,33) b.save() #之后添加HeroInfo表的数据 h = HeroInfo() h.hname = '段誉' h.hgender = True #外键字段赋值为已经创建好的与这条数据关联的bookinfo表对象 #通过这个字段可以查询到对应表的信息 h.hbook = b ################################################################### #一对多的情况下查询一的信息 &gt;&gt;&gt;h.hbook.btitle &gt;&gt;&gt;'天龙八部' &gt;&gt;&gt;h.hbook.bpub_data &gt;&gt;&gt;datetime.date(1998,2,33) #一对多查询所属信息 &gt;&gt;&gt;b.heroinfo_set.all() #返回的结果是一个集合,里面以列表形式放置了所有对应的数据 &gt;&gt;&gt; &lt;QuerySet [&lt;HeroInfo: HeroInfo object (1)&gt;]&gt; &gt;&gt;&gt;b.heroinfo_set.first() #返回的结果是对应数据的第一条 &gt;&gt;&gt; &lt;HeroInfo: HeroInfo object (1)&gt; #通过将这一条数据赋值之后可以继续进行一些查询 &gt;&gt;&gt;b1 = b.heroinfo_set.first() &gt;&gt;&gt;b1.hname &gt;&gt;&gt;&quot;段誉&quot; ###################################################### #查询全部数据 &gt;&gt;&gt; BookInfo.objects.all() &gt;&gt;&gt; HeroInfo.objects.all() #模型类名.objects.all() 数据库查询语句条件查询 all all返回的是一个数据集合 此返回数据可以进行遍历 filter 返回的结果也是一个集合,可以遍历 属性名__exact = 1 判断是否相等 此项属性等同于id = 1 属性名__contains 查询包含某个字符的数据 属性名__endswith 查询以某字符结尾的数据 以某字符串结尾 属性名__startswith 以某字符串开头 属性名__isnull 查询某字段为空或者不为空的数据 属性名_isnull=false 不为空 属性名__isnull=true 为空 属性名__in = [1,2,3] 查询目标字段再此列表元素的范围内的数据 属性名__gt = 3(gt为大于) 等同于 属性名 &gt; 3 属性名__lt = 3(lt为小于) 等同于 属性名 &lt; 3 属性名__gte = 3(gte为大于等于) 等同于 属性名 &gt;= 3 属性名__lte = 3(lte为小于等于) 等同于 属性名 &lt;= 3 属性名__year bpub_date__year = 1980 查询1980年之后的数据 查询时间信息还可以使用上面的大于等于的方法 bpub_date_gt = date(1998,1,1) 查询在1998年1月1日之后的数据 exclude 返回的结果也是一个集合,可以遍历 查询不满足某个条件的数据 order_by排序 返回的结果也是一个集合,可以遍历 排序字段一般是放在条件查询之后进行的 BackInfo.objects.all().order_by(‘id’) 默认排序方式为升序排列 BackInfo.objects.all().order_by(‘-id’) 在前面加个-将会进行降序排列 多条件查询 两个条件并存 与&amp; filter(id_gt=1,age_gt=10) id大于1并且age大于10 或 | 这个条件查询需要导入一个新的模块 from django.db.models import Q filter(Q(id=1) | Q(age=15)) 创建新的Q对象,作为或条件的单位, 查询条件为一个字段的数值大于另一个字段的数据 如,查询一本书的阅读量大于评论量的的数据 这个时候需要导入一个新的模块F from django.db.models import F 使用f模块构造对应字段的对象,并将这个对象作为条件进行查询 BookInfo.objects.filter(bread_gt=F(bcomment)) 聚合函数.—-—-聚合操作的关键字:​ aggregate ​ 查询时所使用的模块都在django.db.models这个包里面 聚合函数总览 count该字段的总个数 max最大数值 min最小数值 sum该字段数值的和 avg平均数值 聚合函数使用时需要导入模块 ​ from django.db.models import Count,Max,Min,Avg,Sum ​ 这五个聚合函数需要从django.db.models里面引入进来 ​ BookInfo.objects.aggregate(Sum(‘bread’)) ​ 模型类名.bojects.aggregate(聚合函数(字段名)) ​ 当不加查询条件时默认为all,也就是说,all时可以省略的 当进行聚合之后,返回的值是一个字典,书写形式为,key = 聚合的属性名__聚合方式 ,value = 聚合的结果聚合函数另一种用法:在已经进行过条件查询的情况下,直接使用小写的.count()得到的结果时一个数值,这个数值是已经查询到的数据的总数 但是因为条件查询完毕之后获取的是数据集合,所以这个数据集合是没有其他的属性的,他唯一适用的聚合函数只有一个,那就是计算总数的**count** 注意,在所有返回的为数据集合的查询语句后面,以上的所有函数都是可以用的,你可以继续在后面使用分类, 所有结果为查询集的结果都是可以进行切片的,[:] 注意,这个切片使用的下标是不允许使用负数的 切片之后的结果依然是一个查询集,可以使用get()方法进行查询 exists() 判断这个查询集中是否有数据,有返回True,没有返回False existe 相当于查看这个判断并展示这个查询及的全部数据","link":"/2022/11/30/Study-notes-Python-Django-Django%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C/"},{"title":"Django 基础内容","text":"Django首次使用 创建django项目: django-admin startproect 项目名创建一个完整的django项目 创建完整的django文件之后会生成一整套的文件,里面包含了django的各种配置: ‘init.py’ 声明这是一个python文件 settings.py 整个项目的配置文件 urls.py url路由的配置 wsgi.py web服务器与django交互入口 manage.py 项目管理文件 此处错误问题:12if version &lt; (1, 3, 13): raise ImproperlyConfigured('mysqlclient 1.3.13 or newer is required; you have %s.' % Database.__version__) 提示版本错误,这一点需要将base.py文件中的报错代码注释掉,使其不会因为自身的mysqlclient版本过高导致报错 第二处错误为MySQLdb不存在 解决办法 12import pymysqlpymysql.install_as_MySQLdb() 打开项目中的init配置文件,在其中导入以上代码 django开发时每个模块都需要创建一个应用来实现: 创建应用的方法: 实例图片 ```python#进入项目文件#打开cmd命令窗口#输入指令python manage.py startapp 要创建的应用名称#创建完成之后,将会自动生成一个应用文件夹,文件夹名字为创建时输入的名字,文件夹中包含各种初始化的配置文件 123456789101112131415161718192021222324252627 2. 配置文件详解: 1. '__init__.py' 声明这是一个python文件 2. models.py 与数据库相关的内容 3. views 接受请求,进行处理,与models/template进行交互,返回应答 1. 定义处理函数 2. 每一个请求都对应着一个处理函数 4. tests.py 写测试代码的文件 5. admin.py 网站后台管理相关文件3. 启动django服务器服务 1. 在setting.py文件中注册模块信息 2. ```python INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'First', ] python manage.py runserver","link":"/2022/11/30/Study-notes-Python-Django-Django%E6%96%87%E4%BB%B6%E5%88%9D%E5%88%9B/"},{"title":"Django 模型管理器","text":"模型管理器详解 objects模型管理器 本质上来说,objects其实是django为我们封装的一个特殊的模型管理器类,它本质上其实是models.Manager对象 也就是说,我们可以通过重写models.Manager的方法来实现给这个管理器进行重命名 如: ```python#在模型类属性之中直接重写models.manger方法book = models.manger()#自定义一个管理器对象,这之后我们就可以不适用objects来进行查询#而是使用book来进行查询BookInfo.book.all()#在进行定义之后,将无法使用原本的objects 123456782. ## 常用的重写模型管理器方法 1. 创建一个模型类来进行重写 1. ```python class BookInfoManger(models.Manager): pass 这是最简单得到重写Manager的方法 通过这个类可以创建全新的Manager对象来进行查询 注意,一旦进行重写之后,django自带的objects将会无法使用,只能使用自己创建的对象进行查询 在模型类之中进行其他的编写 重写all方法(改变结果集) ```pythondef all(self): #通过super方法调用父类的all方法对模型类对象进行查询 books = super().all() #对处理过的结果进行处理 books = books.filter(id__gt=1) #将结果返回 return books 123456789- 这样处理过的all方法在调用时将不会返回全部的数据,而是会根据这个重新定义的all方法对数据进行处理之后才会返回,- 最终返回结果为 ```python &gt;&gt;&gt; BookInfo.a.all() &gt;&gt;&gt; &lt;QuerySet [&lt;BookInfo: 神墓&gt;, &lt;BookInfo: 龙蛇演义&gt;, &lt;BookInfo: 那年那兔那些事儿&gt;, &lt;BookInfo: 亮剑&gt;, &lt;BookInfo: 秒速五厘米&gt;, &lt;BookInfo: 你的名字&gt;]&gt; 返回结果为一个返回集 自定义类方法(定义额外的方法) 在这里我们可以自定义一些方法,比如自定义一些添加用的方法,也即是说,把原本复杂的添加方法封装成为类方法进行使用,这样的话,可以减少代码的数量 重写字段添加 ```python@classmethoddef create_book(cls,btitle,bpub_date,cate_id): obj = BookInfo() obj.btitle = btitle obj.bpub_date = bpub_date obj.cate_id = cate_id obj.save() return obj 12345678910111213- 重写添加信息方法- 在模型类中的写法,此方法不适合写在模型类中,因为过多的类方法会使模型类变得臃肿,不方便使用,所以我们一般将这些方法定义在我们重写的manager模型管理器中- ```python def create_book(self,btitle,bpub_date,cate_id): obj = BookInfo() obj.btitle = btitle obj.bpub_date = bpub_date obj.cate_id = cate_id obj.save() return objpython 这两种方法都可以使用, 重写封装的方法适用于增删改查的所有方法 在manager函数中,封装好了一个model方法,这个方法返回的是调用这个方法的模型类 1self.model() 使用这个方法之后,就不会出现换一个模型类就无法调用的情况了","link":"/2022/11/30/Study-notes-Python-Django-Django%E6%A8%A1%E5%9E%8B%E7%AE%A1%E7%90%86%E5%99%A8/"},{"title":"Django 模板相关内容","text":"Django模板使用 ``` 123456789 - for循环中可以使用forloop.counter获取当前循环的次数2. ``` {% if books %} {% else %} {% endif %} if循环,包括elseif与else 模板注释单行注释: 1{# 注释的内容 #} 多行注释: 123{% comment %}注释的内容{% endcomment %} 过滤器: default - length date 默认时间显示格式为美式 ```dete:’Y年-m月-d日’ 1234567891011- 通过过滤器可以将时间格式转化为想要的状态- safe,关闭自动转义- autoescape标签关闭转义 - ``` {% autoescape %} 模板内容 {% endautoescape %} 模板硬编码不会进行转义 自定义过滤器: ​ 在应用下新建templatetags文件夹,此文件夹名为固定名称,不可自定义 ​ 在文件夹中创建一个py文件用来存放自定义过滤器 ```pythonfrom django.tempalte import Library#创建一个实例化Library对象register = Library()#创建一个过滤器函数@register.filterdef mod(num):return num %2 == 0 1234563. 在模板中导入这个过滤器文件 - ``` {% load filters %} #在这里进行的模块导入只需要将过滤器文件的名字导入进去就可以直接进行使用了 一个坑:​ 自定义过滤器时,创建出来的library对象的名字必须是register ​ 目前还不明白原因,但是使用了library之后会出现无法查询到filters模板的问题, 自定义过滤器时最少需要有一个参数传入,最多两个","link":"/2022/11/30/Study-notes-Python-Django-Django%E6%A8%A1%E6%9D%BF%E6%A0%87%E7%AD%BE/"},{"title":"Django 路由参数捕获","text":"URL参数捕获 参数捕获的方法有两种 位置参数传参 使用一个正则表达式获取参数并传入视图函数中 ```pythonfrom django.urls import re_path,path urlpatterns = [ re_path(r’index/(\\d+)‘,veiws.index), ] 12345678910111213 - 关键字参数传参 - 使用关键字参数传参和位置参数是一样的,区别就是再正则表达式中给这个字段起一个别名 - ```python from django.urls import re_path,path urlpatterns = [ re_path(r’index/(?P&lt;num&gt;\\d+)‘,veiws.index), ]","link":"/2022/11/30/Study-notes-Python-Django-Django%E8%B7%AF%E7%94%B1%E5%8F%82%E6%95%B0%E6%8D%95%E8%8E%B7/"},{"title":"Django view视图文件管理","text":"Django视图管理 #视图函数在views.py文件中进行定义1).定义视图函数​ 123456from django.shortcuts import renderfrom django.http import HttpResponsedef index(request): return HttpResponse('视图函数成功') 2).配置路由 导入路由模块 ```pythonfrom django.urls import pathfrom django.conf.urls import includeurlatterns = [ path('',include('路径')) ] 123456789101112131415- 设定好总的路由之后开始设定从属的路由- ```python from django.urls import re_path from First import views #导入views视图函数文件 urlpatterns = [ #给视图函数绑定url, re_path(r'index/',views.index) ################################### #两种方法都可以使用 path('index/',views.index) ] path配置方法不需要考虑路由名字匹配的问题 3).构建模板文件 构造模板 在django2.2.7中,创建新的项目时会自动配置好tempalte文件,只需要在里面创建应用文件的模板文件夹即可, 旧版本中并不主动生成template文件夹,所以需要自己创建并进行配置 ```‘DIRS’: [os.path.join(BASE_DIR, ‘templates’)], 123456789101112131415- 模板构造与导入 - 加载模板文件 - 从模板文件目录获取html文件的内容,得到一个模板对象 - 定义模板上下文 - 向模板文件传递数据 - 模板渲染 - 得到一个标准的html页面 - ```python re_path(r'lists/(\\d+)',views.lists) 重定向,路由传参方法,使用正则匹配路由 def lists(request,bid): books = BookInfo.objects.get(id=bid) heros = books.heroinfo_set.all() return render(request,'FirstTemplates/lists.html',{ 'books' : books, 'heros' : heros })","link":"/2022/11/30/Study-notes-Python-Django-Django%E8%A7%86%E5%9B%BE%E7%AE%A1%E7%90%86/"},{"title":"Django 编辑页选项","text":"Django编辑页选项 设定编辑页数据显示 设定显示顺序 fields 这个属性的值是一个字典,字典中将编辑页面的字段按照想要的顺序排列好 ```pythonfields = [‘username’,’password’] 123456789101112- 设定显示顺序,这个设定是分块设定,即将编辑页分为几个块 fieldsets - 这个属性的值是一个元组,元组中包含着元组,并按照里面的元组分成块 - ```python fieldsets = ( ('名字',{'fields':['username']}), #设定块时,元组的第一个值是这个块的名称,第二个值是一个字典,键为 fields ,值为一个列表,其中保存的是要展示在这个块中的字段 ('密码',{'fields':['password']}) #无论如何设定,遵循基本的规则,字典的键都是fields,值都是一个列表 ) 设定一类模型编辑时显示字类模型数据 第一种: ```pythonclass Power_list_StackedInline(admin.StackedInline): model = Basic_information_of_characters #model属性的值为这个表对应的多类表的对象 extra = 3 #在显示子类的时候留三个空白的字类方便添加和编辑 #将这个类在模型类约束中引入Inlines = [Power_list_StackedInline]#inlines属性的值是一个列表 123456789 - 这个类设定的是显示结果是将对应的子类数据划分为一个个的块- 第二种 - ```python class Power_list_TabularInline(admin.TabularInline): model = Basic_information_of_characters extra = 3 从设定方法和使用方法来说,和第一种方法没什么两样,唯一的区别就是继承的对象不一样 这个方法展示出来的子类是以表格形式展示出来的","link":"/2022/11/30/Study-notes-Python-Django-Django%E7%BC%96%E8%BE%91%E9%A1%B5%E9%80%89%E9%A1%B9/"},{"title":"Django 与 Vue跨域问题","text":"创建一个django工程并运行 运行vue工程 vue配置跨域 cd vueproject/src/main.js 导入axios模块 import axios from ‘axios’ 注册axios模块 Vue.prototype.$axios = axios cd vueproject/config/index.js 配置跨域请求—-—-proxytable ```javascriptproxyTable: { '/api': { target: 'http://127.0.0.1:8000/api/test/', #要连接的接口 changeOrigin: true, #是否跨域 pathRewrite: { '^api/':'' #重写接口的域名 } } } 1234567891011121314151617- cd 到要发送跨域请求的vue文件 - 设置跨域 - ``` export default { name:'User', data(){ return { page_info: 'this is user route' } }, created(){ this.$axios.get('http://127.0.0.1:8000/api/test').then(response =&gt; {console.log(response.data)}) } } 在将以上数据配置完之后,还会出现请求被拒绝的情况,这个时候需要对django项目进行设定 django项目内部设定方法 cd django/setting.py ```pythonCORS_ALLOW_CREDENTIALS = TrueCORS_ORIGIN_ALLOW_ALL = True 1234567891011- 创建django跨域中间件 - 在应用中创建middlewares.py文件 - ```python from django.utils.deprecation import MiddlewareMixin class MyTest(MiddlewareMixin): def precess_response(self,request,response): response['Access-Control-Allow-Origin'] = ['*'] return response 注册中间件 ```‘app.middlewares.MyTest’ 1234567895. 在完成了上述步骤之后,跨域请求的基本要求就完成了6. 将获得数据绑定到vue模板中去 - 在模板版中准备一个接受用的变量 - ``` django_message : '' 在请求到数据之后将数据赋予这个变量 this.django_message = response.data.message","link":"/2022/11/30/Study-notes-Python-Django-axios%E8%AF%B7%E6%B1%82%E4%B8%8E%E8%B7%A8%E5%9F%9F/"},{"title":"Django ORM 概念","text":"ORM框架 orm的全称: ​ O object 对象 在models.py中涉及模型类, ​ R Relations 关系,用于关系数据库中的表 ​ M Mapping (数据库)映射 将对数据库的操作映射到数据库中去","link":"/2022/11/30/Study-notes-Python-Django-%E5%85%B3%E4%BA%8EORM%E6%A1%86%E6%9E%B6%E8%AF%A6%E8%A7%A3/"},{"title":"MVC 与 MVT","text":"==web设计模式== —web mvc—mvc基本概念: ​ mvc的产生理念是: ==分工== ​ mvc的核心理念是: 解耦 解耦的概念并不难理解,将mvc当成一台电脑,如果你的硬盘出了问题,那么你并不需要将整台电脑换掉,只需要找到损坏的硬盘并将其替换更新即可 同理,在mvc类型的框架中,如果代码出现了问题,只需要修正对应部分的代码即可,并不需要将所有的代码推翻重来 在这种情况下,只要所属模块实现了需要的功能,整个框架就是可以运行的 ​ mvc的组成概念相当于正常的公司,旗下有多个下属的职能部门, ​ 同理,mvc的概念是就是将多个模块集成起来分工合作,将输入,处理输出三个部分的功能交给不同的模块进行. mvc为简称 model 模型, 用于后端与数据库进行交互 view 视图 用于前端,负责产生html页面 controller 控制器 接受请求,进行处理,负责model与view进行交互,并返回应答 —python MVT— MVT基本概念: MVT其实是基于mvc的生产模式,只是在==Django==中因为模块的少许差异简写为MVT MVT是简写,全程为: model(模型) —–&gt; view(视图) —–&gt; template(模板) MVT为简称 model 模型, 用于后端与数据库进行交互 views 相当于mvc中的c, 用于m,t交互,处理信息,并返回应答 控制器 接受请求,进行处理,负责model与template进行交互,并返回应答 template 相当于mvc中的v, 接受信息,负责产生html页面 快速开发和DRY原则Do not repeat yourself不要自己去重复一些工作","link":"/2022/11/30/Study-notes-Python-Django-%E5%85%B3%E4%BA%8Emvc/"},{"title":"Vue 遇到的一些问题","text":"第一点 ​ 最初老师教的其实是2.0版本,相比起来,更加高端的vue/cli3的代码更加简洁高效,我也不知道为啥学校不叫最新的,反正当初这个坑是被我踩到了 ​ 这其中有着很多的问题,如缺少依赖包,缺少必备的代码库,林林总总的红色报错在当时直接把我惊呆了 ​ 因为在出问题之前,我一直是在使用webstorm进行代码的编写,webstorm是个很好的软件,基本上,只要你装好了环境,他就能帮你进行一系列的操作,但也正是这些,导致我压根没发现这些潜在的问题 ​ 事情的起因,是因为我的webstorm到期了…… ​ 这是一个悲伤的故事,我的电脑中安装了很多编辑器 ​ 包括webstorm,pycharm,codeblock,vscode,golang等等,也正是因为到期了,我便决定收拾一下vscode,将vue文件放到vscode中去写 ​ 然后问题就出现了. 先是新建的vue文件无法显示,然后就是疯狂报错,到最后,我三个dos窗口全是红色,惨不忍睹 于是忍痛删掉了电脑中的vue插件,然后开始重新安装 以下是这两天安装的全部插件,不排除被我忘记的可能 npm install @vue/cli -g –save npm install axios npm install stylus npm install stylus-loader npm install style-loader npm install css-loader npm install pug npm install webpack npm install cors-js ps:更多的插件一时想不起来了.反正,凡是报错了的插件,我几乎全都下载了一遍 到现在,之前所有的错误提示全部消失,不仅如此,我发现最新版本的vue居然是4.0+??? WTF? 我之前用的可是2.6啊混蛋 难怪nanarino给我的项目那么简单,感情人家的已经是领先好几代了 临近考试了,本来以为这次考试稳了,现在看来,要是不赶紧摸清楚新版本的变化,恐怕这次要跪啊! . 使用vue cli 4 的注意事项 创建语句 vue create projects_name 下载模块 npm install models_name -g –save 构建vue工程之后第一点就是重新下载axios与pug相关依赖,具体可以根据报错提示进行操作","link":"/2022/11/30/Study-notes-Python-Django-%E5%85%B3%E4%BA%8Evue%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/"},{"title":"Python Matplotlib 使用","text":"Matplotlib 使用了一些numpy的功能,但是其主要功能在于对图片的操作 一、Matplotlib使用Matplotlib对numpy数组进行操作,该数组来源于一张图片二、实例(一)对猫咪的图片进行相关操作1.具体实现的代码块1234567891011import matplotlib.pylab as plt# 导入对图片处理的模块img_arr_1 = plt.imread('../photo/困.jpeg')# 使用imread对图片以数组的形式进行读取img_arr_2 = plt.imread('../photo/困.jpeg')# 多个数组之间可以进行拼接img_arr_3 = (img_arr_1 + img_arr_2) * 2# 在对图片数组进行直接性加值的时候,需要注意,运算之后的数据不能大于255,否则将会报错# ValueError: Image RGB array must be uint8 or floating point; found uint16# 如上述代码出现,则意味着加值过高,长处rgb颜色的范围,需要将数值修改plt.imsave('../photo/new.jpeg', img_arr_3 + 100) 2.详细的图片处理方法(1)imread 以数组的形式读取图片信息 imread(filePath) (2)imsave 将数组形式的图片信息保存起来 imsave(filename, fileArray) (3)imshow 将图片数组以图片的形式展示 imshow(fileArray)","link":"/2022/11/30/Study-notes-Python-data-Matplotlib%E4%BD%BF%E7%94%A8/"},{"title":"Django 自编写哈希加密函数","text":"哈希加密 该加密算法对于相同的字符串将会是一致的,同时,这种加密是单向的,无法反向编译出来,也就是说,想要对 加密后的数据进行比较,你需要将要比较的数据先进行加密,然后才能进行比较,当两者内容一致时,才会通过验证 当使用在登陆页面时,其流程时这样的:输入密码===&gt;进行加密===&gt;调用数据库===&gt;进行比较===&gt;验证通过===&gt;返回session信息,进入登陆状态 ```python#导入哈希加密包#在py3之后,md5包被移除,其功能被整合到hashlib包中import hashlib#创建一个新的哈希加密对象h1 = hashlib.md5()#在对象中添加要加密的字符串,该字符串需要先转化成二进制格式,即utf8h1.update(str.encode(encoding=’utf8’))#查看转码后的数据,hexdigest为十六进制模式print(h1.hexdigest())#查看转码后的数据,digest为二进制模式print(h1.digest()) 1234567891011121314151617181920- 这里要注意的是,每次进行加密,都要重新创建一个哈希对象,也即是重新书写一遍==&gt;h1 = hashlib.md5()- 如果不进行重新定义的话,当加密时遇到相同的字符串,如&quot;hello world&quot;这样的字符串,将会出现&quot;hello worldhello world&quot;这样的叠加形式,所以需要重新定义```python# 哈希加密在view中的实现, 对创建时的密码以及登陆时的密码进行对应的加密def make_password(mypass): #生成md5对象 md5 = hashlib.md5() # 定义加密对象 sign_str = mypass # 转码 sign_utf8 = str(sign_str).encode(encoding='utf-8') # 加密操作 md5.update(sign_utf8) # 生成密文 md5_server = md5.hexdigest() # 返回最终数据 return md5_server","link":"/2022/11/30/Study-notes-Python-Django-%E5%85%B3%E4%BA%8E%E5%93%88%E5%B8%8C%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95/"},{"title":"Python Pandas","text":"Pandas的数据结构 12import pandas as pdfrom pandas import Series,DataFrame 一、Series1.Series时一种类似于一维数组的对象,主要由以下两个部分 组成: values:一组数据 index: 相关的数据索引标签 2.Series创建方式 由列表使用列表创建 ```python创建一个对象,该对象不指定索引,将会默认使用0—(n-1)的形式import pandas as pdfrom pandas import Series,DataFramea_ser = Series(data=[1,2,3,4])print(a_ser)print(a_ser[‘a’])12345678910- ```python # 创建指定索引的对象,该对象的索引可以自行指定 # 显示索引的存在并不影响正常索引的使用 # 此种索引的形式被称之为 显示索引 import pandas as pd from pandas import Series,DataFrame a_ser = Series(data=[1,2,3,4], index=['a', 'b', 'c', 'd']) print(a_ser) print(a_ser['a']) 使用numpy数组创建使用数组创建 ```python隐式索引import pandas as pdfrom pandas import Series,DataFrameimport numpy as npb_ser = Series(data=np.random.randint(0,100,size=(5,)))print(b_ser)123456789- ```python # 显示索引 import pandas as pd from pandas import Series,DataFrame a_ser = Series(data=[1,2,3,4]) import numpy as np b_ser = Series(data=np.random.randint(0,100,size=(5,)), index=['a', 'b', 'c', 'd', 'e']) print(b_ser) 二、切片显示索引切片与隐式索引切片 ```pythonprint(a_ser[0:3])123456789101112## 三、series基本概念1. 可以将Series看作是一个定长的有序字典2. 使用size,shape,index,values获取对应的属性 - ```python a_ser.size # 元素的长度 a_ser.shape # 对象的形状 a_ser.values # 对象的内容,也就是这个对象中储存的所有内容 a_ser.index # 对象的索引,如果有显示索引的话返回显示索引,没有的话显示隐式索引 使用head和tail获取前n个数或者后n个数 ```pythonprint(b_ser.head(2))123- ```python print(b_ser.tail(2)) 对Series进行去重 ```pythonprint(b_ser.unique())123456785. 对两个Series进行相加 - ```python s = a_ser + b_ser print(s) # 在执行加法运算的时候,将会把对应索引位置的数值相加 # 如果其中一个的数值不存在,那么这个位置的数值将会变成null, 也就是显示出来的NaN 关于是否为空的判定 ```pythonprint(s.isnull())print(s.notnull())1234567891011127. 基于空值判定的结果进行取值 - ```python print(s[[1,2]]) # 正常的下标取值,可同时取多个值 print(s[[True, True, True, False, False]]) # 通过布尔值进行取值,将会根据对应位置的布尔值进行取值,当为True时将会把值取出来,否则不取 print(s[s.notnull()]) # 根据空值判断结果进行取值,取出不为空的数值 print(s[s.isnull()]) # 根据控制判断结果进行取值,取出为空的数值","link":"/2022/11/30/Study-notes-Python-data-Pandas/"},{"title":"Python 数据分析简介","text":"数据分析​ 把隐藏在一些杂乱无章的数据背后的信息提炼出来,总结出所研究对象的内在规律 数据分析三大模块 Numpy Pandas Matplotlib","link":"/2022/11/30/Study-notes-Python-data-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%AE%80%E4%BB%8B/"},{"title":"Python Numpy","text":"Numpy 是python语言的一个扩展程序库,支持大量维度数组与矩阵运算,此外也针对数组运算提供大量的数学函数库 一、创建ndarray1.使用np.array()创建一维数组创建 ```pythonimport numpy as nparray_1 = np.array([1,2,3])print(type(array_1))123456789 ### 二维数组创建- ```python import numpy as np array_2 = np.array([[1,2,3],[1,2,3],[1,2,3]]) print(type(array_2)) 使用numpy模块的array方法,可以创建一维数组 同时,也可以空来创建二维数组 2.关于numpy 的一些用法 linspace 创建一个等差数列 ```pythonnp.linspace(0, 100, num=35)linspace(start, stop, num ) num 为获取的数组的长度12345678- ### arange - 创建一个等差数列 - ```python np.arange(0, 100, 20) # 第三个参数为每个数纸剑的差值 random.randint 随机创建多维数组 ```pythonnp.random.randint(0, 100, size=(4,))np.random.randint(0, 100, size=(4, 10))np.random.randint(0, 100, size=(4, 10, 3))np.random.randint(0, 100, size=(4, 10, 3, 2)) 1234567891011121314151617 - 在randint中,第一个参数为随机数的最小值,第二个参数为随机数的最大值,size为数组的维度参数,具体表现形式为一个元组类型的数据,元组中只有一个数据为一维数组,两个为二维数组,以此类推- random.seed - 固定随机性 - 使用seed来使当前的随机状态固定下来,在seed不变的情况下,随机出来的数组将完全不会改变,除非修改了seed的参数,才会重新开始一次循环,否则seed将会记录下当前的数据状态,即使重新运行当前的python文件,也不会改变结果- random.random - 创建一个0到1 的随机数数组 - ```python print(np.random.random(size=(4))) print(np.random.random(size=(4, 10))) print(np.random.random(size=(4, 10, 3))) print(np.random.random(size=(4, 10, 3, 2))) 二、numpy数组属性 ndim 数组的维度,返回的结果为一个数字,对应数组的维度,一维数组为1 使用方法: ```pythonprint(arr.shape)123456789- ### shape - 数组的形状(即为各维度的长度),返回的结果为数组各个维度的数值组成的元组 - 使用方法: - ```python print(arr.ndim) dtype 查看当前数组中的数据的类型 ```pythonprint(arr.dtype) 1234567- ### size - 查看当前数组的长度 - ```python print(arr.size) 三、基本操作 索引 无论是一维还是多维,索引与列表是相同的 切片 无论是一维还是多维,切片与列表是相同的 但是数组可以通过切片获取多维数组的前两列 也即是说,每一个元素中只取前两个 ```pythonarr[1:4,:2,4:5] 12345- 图片裁剪并保存- ```python plt.imsave('../photo/new4.jpeg', img_arr_1[:400,120:600,::-1]) 变形 将数组进行维度转义 ```pythonadd = arr.reshape((40,)) 1234567891011121314151617 - 填充参数为一个元组类型的数据,可以将数组转译成符合参数概念的数组 - 但是要注意的是,转义需要你精确的转移,也就是说,如果你的数组中只有20个元素,而你想转译成21个元素,就是不行的- ### 级联 - 对数组进行横向或者纵向的拼接 - ```python img_arr_1 = plt.imread('../photo/困.jpeg') img_arr_2 = plt.imread('../photo/困.jpeg') new_img = np.concatenate((img_arr_1,img_arr_2), axis=0) new_img_1 = np.concatenate((new_img,new_img),axis=1) print(plt.imshow(new_img)) plt.imsave('./new_img_1.jpeg', new_img_1) 通过对图片数组的级联操作,我们可以实现图片的横向与纵向拼接 四、聚合操作 sum 求和 max 最大值 min 最小值 mean 平均值 进行运算时,参数axis可以不添加,当axis指定为0时,将会求出每一行的结果,当axis为1时,求出每一列的结果 五、排序操作 sort 通过axis来确定作用范围,当axis不指定的时候,将会默认进行全局排序,当axis=0的时候,进行行排序,当axis=1时,进行列排序 使用sort时,np.sort并不会映射到原数据,arr.sort将会直接映射到原数据","link":"/2022/11/30/Study-notes-Python-data-numpy%E4%BD%BF%E7%94%A8/"},{"title":"Vue 基础","text":"Vue配置 在电脑中安装node.js ………..省略 使用node.js安装脚手架 ```npm install -g vue-cli123453. ### 查看vue版本 - ``` vue -V vue下载换源 ```pythonnpm –registry https://registry.npm.taobao.org#临时使用1234- ```python npm config set registry https://registry.npm.taobao.org #持久使用 创建一个新的vue工程 ```cmdvue init webpack 文件名123456. ### 运行vue工程 - ``` vue run dev 删除node_modules npm install rimraf -g rimraf node_modules","link":"/2022/11/30/Study-notes-JavaScript-Vue-%E5%85%B3%E4%BA%8Evue%E7%9A%84%E9%85%8D%E7%BD%AE/"},{"title":"Vue 路由配置","text":"配置vue路由 第一步 cd ./vueproject/src/router 进入到路由文件夹中,打开index.js文件 在文件中导入设定好的Vue路由文件 import User from ‘../components/User’ 这里可以通过输入vue文件名的方式进行快捷编写 在路由数组中注册路由 ```javascriptroutes: [ { path:'/user', name: 'User', component:User } ] 12345- 在app.vue文件中建立路由链接 - ``` &lt;router-link to='/user'&gt;user&lt;router-link&gt; 这是一个超链接形式的标签 路由参数传递 传递参数时,将参数放在路由链接后面 使用:to的方法绑定路由文件 name为路由文件的名字,params为要传递的参数,以字典的形式传递 1&lt;router-link :to=&quot;{ name:'User',params:{ username : 'root' } }&quot; &gt;user&lt;/router-link&gt; 渲染: 使用模板语法,对传递过来的参数进行提取","link":"/2022/11/30/Study-notes-JavaScript-Vue-vue%E8%B7%AF%E7%94%B1%E9%85%8D%E7%BD%AE/"},{"title":"Bs4安装与使用","text":"bs4安装配置: ```pythonpip install bs4 在终端中下载bs4解析库pip install lxml 在终端中下载lxml库12345678910111. 创建解析对象2. ```python from bs4 import BeautifulSoup f = open('../day01/baidu.html','r',encoding='utf8') soup = BeautifulSoup(f,features='lxml') print(soup.div) 要注意的是,soup.TagName返回的是第一个对应的标签的内容, ```pythonfrom bs4 import BeautifulSoup f = open(‘../day01/baidu.html’,’r’,encoding=’utf8’) soup = BeautifulSoup(f,features=’lxml’) print(soup.find(‘div’)) 12345678910115. soup.find('TagName')所返回的,同样是第一个元素,其效果和soup.TagName基本相同6. ```python from bs4 import BeautifulSoup f = open('../day01/baidu.html','r',encoding='utf8') soup = BeautifulSoup(f,features='lxml') print(soup.find('div',class_ = 'op-short-video-pc-poster c-span6')) find方法的属性:class_ 设定的是目标的class名,只有符合条件的TagName才能被查询出来 ```pythonfrom bs4 import BeautifulSoup f = open(‘../day01/baidu.html’,’r’,encoding=’utf8’) soup = BeautifulSoup(f,features=’lxml’) print(soup.find_all(‘div’,class_ = ‘op-short-video-pc-poster c-span6’)) 12345678910119. find_all的用法和find基本相同,但是返回的数据格式是一个列表,并且返回的是所有符合条件的数据,而不是第一个10. ```python from bs4 import BeautifulSoup f = open('../day01/baidu.html','r',encoding='utf8') soup = BeautifulSoup(f,features='lxml') print(soup.select('.opr-recommends-merge-content')) select方法的用法,参数为选择器,支持标签选择器,层级选择器,id选择器,类选择器 ```pythonfrom bs4 import BeautifulSoup f = open(‘../day01/baidu.html’,’r’,encoding=’utf8’) soup = BeautifulSoup(f,features=’lxml’) print(soup.select(‘.opr-recommends-merge-content &gt; div &gt; a’)) 1234567891011121313. 层级选择器写法,使用 &gt; 来指向下一级元素14. 层级选择器如果需要定位子类的子类的话,可以不写 &gt;15. ```python from bs4 import BeautifulSoup f = open('../day01/baidu.html','r',encoding='utf8') soup = BeautifulSoup(f,features='lxml') print(soup.select('.opr-recommends-merge-content a')) 以上为不指向性获取包含的内容 ```pythonfrom bs4 import BeautifulSoup f = open(‘../day01/baidu.html’,’r’,encoding=’utf8’) soup = BeautifulSoup(f,features=’lxml’) print(soup.select(‘.cr-title,.c-clearfix span’)[0].span.string) from bs4 import BeautifulSoup f = open(‘../day01/baidu.html’,’r’,encoding=’utf8’) soup = BeautifulSoup(f,features=’lxml’) print(soup.select(‘.cr-title,.c-clearfix span’)[0].span.text) 1234567891011121314151617181918. text与string均可获取标签中的文字内容19. text获取的是标签中所有的文本内容,20. string获取的是标签中的直系文本内容以外的内容无法获取21. &quot;我的奴隶的奴隶,不是我的奴隶&quot;22. ```python from bs4 import BeautifulSoup f = open('../day01/baidu.html','r',encoding='utf8') soup = BeautifulSoup(f,features='lxml') print(soup.select('.cr-title,.c-clearfix span')[0].a['class']) print(soup.find('div',class_ = 'op-short-video-pc-poster c-span6').a['class']) print(soup.find_all('div',class_ = 'op-short-video-pc-poster c-span6')[0].a['class']) 通过定位获取属性值","link":"/2022/11/30/Study-notes-Python-Spider-bs4%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"title":"CrawSpider使用","text":"基于crawlSpider进行的全栈数据爬取 crawlspider是爬虫类中spider的一个子类使用流程创建一个基于crawlspider的爬虫文件 ​ scrapy genspider -t crawl spiderName www.xxx.com 使用指令创建一个基于crawlSpider的爬虫文件 构造链接提取器和构造解析器 链接提取器 可以根据制定的规则进行指定链接的提取 构造解析器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom ..items import Homework1Item, Homework1ContentItemclass SunSpider(CrawlSpider): name = 'sun' # allowed_domains = ['www.xxx.com'] start_urls = [ 'http://wz.sun0769.com/index.php/question/questionType?type=4' ] rules = ( # 进行链接提取,follow开启深度提取 , Rule(LinkExtractor(allow=r'type=4&amp;page=\\d+'), callback='parse_item', follow=True), # 进行详情页链接提取,但是关闭深度提取 Rule(LinkExtractor(allow=r'question/\\d+/\\d+\\.shtml'), callback='parse_detail', follow=False), ) # 在进行上述的网页处理的时候,这两个将会同步进行,也就是说,在第一个深度提取运行一次的时候,另一个也会一起运行一次 # 这种情况下,每次我们获取一个新的页面,第二个提取器将会对这个新的页面进行处理,然后再处理下一个页面 # 正常的spider和这个crawlspider是通用的,也就是说,在使用这个crawlspdier进行深度爬取的时候,其他的spider操作也是没问题的 def parse_item(self, response): tr_list = response.xpath( '//*[@id=&quot;morelist&quot;]/div/table[2]//tr/td/table//tr') for tr in tr_list: title = tr.xpath('./td[2]/a[2]/@title').extract_first() detail = tr.xpath('./td[3]/span/text()').extract_first() # print(title,detail) item = Homework1ContentItem() item['title'] = title item['detail'] = detail yield item def parse_detail(self, response): content = response.xpath( '/html/body/div[9]/table[2]//tr[1]//text()').extract() content = ''.join(content) item = Homework1Item() item['content'] = content yield item","link":"/2022/11/30/Study-notes-Python-Spider-crawlSpider%E4%BD%BF%E7%94%A8/"},{"title":"GIT 仓库基础操作","text":"git git bash操作实例: ​ 在文件夹中打开git(在文件页面上方的路径里面输入git bash即可快速打开git编辑界面) ```git init //初始化git文件包 123- ``` git add 文件名 //创建镜像文件 ```git commit //提交 123- ``` git commit -m '描述信息' //提交,但不会打开编辑窗口 ```git clone https://github.com/fcg22450/my_flask.git #https://github.com/fcg22450/my_flask.git这里的内容从github页面获取 1234- ``` git add . #将当前文件夹中的所有文件打包成迁移文件 ```git push -u origin master#将生成的迁移文件上传到github仓库中 1234- ``` cd test #进入对应文件夹 ```git status#查看当前准备好的迁移文件 1234- ``` git config --system --unset credential.helper # 清空密码 git config --global credential.helper store # 保存密码","link":"/2022/11/30/Study-notes-Python-Spider-git/"},{"title":"MongoDB安装流程与交互方式","text":"mongodb; 第一步,安装 官方网站网址 第二步,环境配置 第三步 构建mango.config文件 文件内部填充 ```port = 27017dbpath= D:\\MongoDB\\data\\dblogpath= D:\\MongoDB\\data\\log\\mongo.logdirectoryperdb = truefork = truelogappend=truejournal=truequiet=true 123454. 启动mongo服务 1. ``` mongod --dbpath D:\\MangoDB\\data 启动另一个窗口 输入mongo 如果进入了服务的话,说明启动成功了 python与mongodb交互:123456789101112from pymongo import MongoClientclient = MongoClient('localhost',27017)db = client.localcollection = db.spider# collection.insert({'name':'Tom','age':25,'addr':'Cleveland'})for item in collection.find({'name':'Tom'}): print(item) 将字符串数据转换成时间字段:1234from dateutil import parserdateStr = &quot;2019-12-19&quot;myDatetime = parser.parse(dateStr)print(myDatetime) mongoDB操作指令: show dbs show databases 基本上和mysql一样 use dbname 进入数据库, show tables 展示所有的表 mongodb不需要进行创建表或者是创建库,在需要使用的时候只需要直接使用就行,内部程序将会自动创建对应的数据库与数据表 在第一次向一个集合插入数据的时候,就会自动创建目标集合 mongodb自动生成_id属性作为数据得到唯一性识别码 db.tableName.finc() 查看表中的全部数据 db.tableName.insert({}) 在表中插入数据","link":"/2022/11/30/Study-notes-Python-Spider-mongodb%E5%AE%89%E8%A3%85%E6%B5%81%E7%A8%8B%E4%B8%8E%E4%BA%A4%E4%BA%92%E6%96%B9%E5%BC%8F/"},{"title":"requests请求模块详解","text":"requests模块 概念​ 一个机遇网络请求的模块,作用就是用来模拟浏览器发起请求 ​ asdsa 编码流程​ – 指定url ​ – 进行请求的发送 ​ – 获取相应数据(即,爬取到的数据) ​ – 持久化存储 ​ ** 环境的安装 ​ ** pip install requests 代码示例1234567891011121314151617181920212223242526#导入request模块import requests# 准备初始urlurl = 'https://www.baidu.com/s'# 准备伪装用的请求头Headers = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36&quot;, 'Cookie': &quot;BAIDUID=97DA851F88C92BE45D7963ADA75C3434:FG=1; BIDUPSID=97DA851F88C92BE45D7963ADA75C3434; PSTM=1567649375; BD_UPN=12314753; BDUSS=3ZzTkJOTHRNVkpqVGR4cE9BaWlOVlJUcX5JbHVXMWEwWVQ3V1pmbUJEbVQxQmRlRUFBQUFBJCQAAAAAAAAAAAEAAABbuGacAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJNH8F2TR~Bdb; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; BDSFRCVID=cvFOJeC62CrXf4cwU0e2tW9rYM4hm03TH6aIl2P5JFJeo_dcX41gEG0PDf8g0KubMVkPogKKBeOTHg_F_2uxOjjg8UtVJeC6EG0Ptf8g0M5; H_BDCLCKID_SF=JRA8oKPhtCvhDRTvhCcjh-FSMgTBKI62aKDs2ROYBhcqEIL4W6-aKPIp5nO8W4KL-6nH2IOK5DoEHxbSj4QzQU0zDPvl0RQuWI3bhqvztp5nhMJFXj7JDMP0qJ7j2RQy523ion6vQpn-KqQ3DRoWXPIqbN7P-p5Z5mAqKl0MLPbtbb0xXj_0j63-DaAJt6njJTRe0Rj-KbP_hDL9eKTjhPrMjHrJWMT-0bFH_JR2QJP5fUbJQpPVKJ3-MMc9BMvuJan7_JjYbRDVMfQhWhJChtLeyU6r3fQxtNRB-CnjtpvhKJjjWtcobUPUDMc9LUvqHmcdot5yBbc8eIna5hjkbfJBQttjQn3hfIkj2CKLtD05MDIwjTt3-4LtMxrXKK6QKCjj3toObnnqq6rkbJ83yhFzXP6-35KHf6rMKR_bbhrBj5CzKJOb5MrXeJb-5h37JD6yb-n5Lb8KDRPGehrAMJFYQ4oxJpO7BRbMopvaKquVoJQvbURvD-ug3-7qex5dtjTO2bc_5KnlfMQ_bf--QfbQ0hOhqP-jBRIEoD-2tKD5MIDr5nJbq4I85M5H54cX--o2WbCQaM7O8pcNLTDKLnLNjb72-MC8BNrJ-KTua-PMjbCxjqO1j4_PMa8OKhFeLHIebbQJyIb-hl5jDh3Ub6ksD-Rt5frp2aRy0hvc0J5cShnkDMjrDRLbXU6BK5vPbNcZ0l8K3l02V-bIe-t2XjQhDG8JJ5_eJbCsL-35HJcqfbo4-tr_KICShUFs5b5lB2Q-5KL-0-oieCbn5p_2bjD33NrHXxuJ2IDjoMbdJJjzDKoMjf6Py4LeBUO2XT0D52TxoUJg5DnJhhvG-xc4Mp8ebPRi3tQ9QgbMMhQ7tt5W8ncFbT7l5hKpbt-q0x-jLTnhVn0MBCK0HPonHjLBDTb33H; H_PS_PSSID=1460_21110_30210_30284_22160; yjs_js_security_passport=3b54287a45b25629d2f7cfdb10008d7e9d67b5bd_1576637318_js; BDRCVFR[feWj1Vr5u3D]=I67x6TjHwwYf0; delPer=0; BD_CK_SAM=1; PSINO=2; BD_HOME=1; H_PS_645EC=b428Vxu0iZX7iP6ukpImwh5YVC5tlT7Iwn5ZIzZa5ijoKNVsMoi%2FCv%2FtNME&quot;}# 准备请求时的数据params = { &quot;wd&quot; : '666'}# 开始进行数据获取# url 要爬取的网页# params 要传入的数据# headers 伪装用的请求头response = requests.get(url=url,params=params,headers=Headers)# 对获取到的页面进行转码response.encoding = 'utf8'# 将整个页面的数据进行赋值text = response.text# 打开一个新的文件,将获取到的数据进行存储with open('./baidu.html','w',encoding='utf8') as f: f.write(text)# 爬虫工作结束 易错点: 请求时,并不一定要传入数据,url是必须的,同时,要根据网站的要求进行对应的转码 当爬虫被反爬时,西药进行ua伪装 requests使用post请求: 使用post请求时,需要传入的参数基本上没有变化,唯一的区别就是params变成了data 除此以外和get请求基本一致,但是要注意,当使用axios请求时,需要将你要用到的参数放进去,如页码,每页最大数量等 由于axios返回的数据实际上是json格式的字符串,因此,在进行数据解析的时候,不能再使用text 要使用json来进行解析,然后才能进行操作 request概念请求方法 GET : 请求页面,并返回页面内容 POST : 用于提交表单数据或上传文件,数据包含在请求体重 PUT : 从客户端想服务器传送的数据取代指定文档中的内容 DELETE : 请求服务器删除指定的页面 HEAD : 类似于GET请求,只不过返回的响应中没有具体的内容,用于获取报头 CONNECT : 把服务器当成跳板,让服务器代替客户端访问其他网页 OPTIONS : 允许客户端查看服务器的性能 TRACE : 会先服务器收到的请求,用于测试或诊断 GET POST 区别: GET请求包含在URL里面,明文传输,安全性低 POST请求数据封装到了请求体中,安全性较高 关于访问请求的方法: text 查看返回的TEXT/HTML格式代码 json 将获取到的json字符串转化为dict格式 content 将获取到的文件流进行转化存储 UA伪装:关于UA伪装: ​ 模拟headers 用以模拟浏览器对网页url进行访问,以此来避开页面的反爬机制 user_agent 存储用户使用的系统版本号与浏览器信息等数据,用来模拟浏览器访问 request get/post url data/params (对请求参数进行封装) headers (UA伪装) 动态加载 ajax js 鉴定是否具备动态加载 局部搜索 全局搜索 re模块匹配:使用re模块对获取到的页面进行匹配,用到的比较少,但也是个重点 12345678import reimpore requestsres = requests.get('www.baidu.com')res.encoding = 'utf8'# ret为 正则匹配公式# re.S为 单行匹配,通常情况下,在匹配html页面的时候,都需要进行单行匹配re.findall(ret,res,re.S) requests高阶应用: 文件上传功能 ```pythonimport requestsurl = ‘http://www.httpbin.org/'Headers = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36&quot;, } f = open(‘./baidu.html’,’rb’)files = { 'file': f }response = requests.get(url=url,headers=Headers,files=files)if response: print('上传成功') 123456789101112131415 - 文件上传功能2. 模拟cookie 1. 手动添加cookie - 在headers中添加cookie属性 - ```python Headers = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36&quot;, 'Cookie': &quot;BAIDUID=97DA851F88C92BE45D7963ADA75C3434:FG=1; BIDUPSID=97DA851F88C92BE45D7963ADA75C3434; PSTM=1567649375; BD_UPN=12314753; BDUSS=3ZzTkJOTHRNVkpqVGR4cE9BaWlOVlJUcX5JbHVXMWEwWVQ3V1pmbUJEbVQxQmRlRUFBQUFBJCQAAAAAAAAAAAEAAABbuGacAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJNH8F2TR~Bdb; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; BDSFRCVID=cvFOJeC62CrXf4cwU0e2tW9rYM4hm03TH6aIl2P5JFJeo_dcX41gEG0PDf8g0KubMVkPogKKBeOTHg_F_2uxOjjg8UtVJeC6EG0Ptf8g0M5; H_BDCLCKID_SF=JRA8oKPhtCvhDRTvhCcjh-FSMgTBKI62aKDs2ROYBhcqEIL4W6-aKPIp5nO8W4KL-6nH2IOK5DoEHxbSj4QzQU0zDPvl0RQuWI3bhqvztp5nhMJFXj7JDMP0qJ7j2RQy523ion6vQpn-KqQ3DRoWXPIqbN7P-p5Z5mAqKl0MLPbtbb0xXj_0j63-DaAJt6njJTRe0Rj-KbP_hDL9eKTjhPrMjHrJWMT-0bFH_JR2QJP5fUbJQpPVKJ3-MMc9BMvuJan7_JjYbRDVMfQhWhJChtLeyU6r3fQxtNRB-CnjtpvhKJjjWtcobUPUDMc9LUvqHmcdot5yBbc8eIna5hjkbfJBQttjQn3hfIkj2CKLtD05MDIwjTt3-4LtMxrXKK6QKCjj3toObnnqq6rkbJ83yhFzXP6-35KHf6rMKR_bbhrBj5CzKJOb5MrXeJb-5h37JD6yb-n5Lb8KDRPGehrAMJFYQ4oxJpO7BRbMopvaKquVoJQvbURvD-ug3-7qex5dtjTO2bc_5KnlfMQ_bf--QfbQ0hOhqP-jBRIEoD-2tKD5MIDr5nJbq4I85M5H54cX--o2WbCQaM7O8pcNLTDKLnLNjb72-MC8BNrJ-KTua-PMjbCxjqO1j4_PMa8OKhFeLHIebbQJyIb-hl5jDh3Ub6ksD-Rt5frp2aRy0hvc0J5cShnkDMjrDRLbXU6BK5vPbNcZ0l8K3l02V-bIe-t2XjQhDG8JJ5_eJbCsL-35HJcqfbo4-tr_KICShUFs5b5lB2Q-5KL-0-oieCbn5p_2bjD33NrHXxuJ2IDjoMbdJJjzDKoMjf6Py4LeBUO2XT0D52TxoUJg5DnJhhvG-xc4Mp8ebPRi3tQ9QgbMMhQ7tt5W8ncFbT7l5hKpbt-q0x-jLTnhVn0MBCK0HPonHjLBDTb33H; H_PS_PSSID=1460_21110_30210_30284_22160; yjs_js_security_passport=3b54287a45b25629d2f7cfdb10008d7e9d67b5bd_1576637318_js; BDRCVFR[feWj1Vr5u3D]=I67x6TjHwwYf0; delPer=0; BD_CK_SAM=1; PSINO=2; BD_HOME=1; H_PS_645EC=b428Vxu0iZX7iP6ukpImwh5YVC5tlT7Iwn5ZIzZa5ijoKNVsMoi%2FCv%2FtNME&quot; } 如上图所示 ```pythonimport requests url = ‘https://www.baidu.com/s' 创建requests.cookies.RequestsCookiesJar对象jar = requests.cookies.RequestsCookieJar()Cookie = “BAIDUID=97DA851F88C92BE45D7963ADA75C3434:FG=1; “ \\ &quot;BIDUPSID=97DA851F88C92BE45D7963ADA75C3434; &quot; \\ &quot;PSTM=1567649375; BD_UPN=12314753; &quot; \\ &quot;BDUSS=3ZzTkJOTHRNVkpqVGR4cE9BaWlOVlJUcX5JbHVXMWEwWVQ3V1pmbUJEbVQxQmRlRUFBQUFBJCQAAAAAAAAAAAEAAABbuGacAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJNH8F2TR~Bdb; &quot; \\ &quot;BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; &quot; \\ &quot;BDSFRCVID=cvFOJeC62CrXf4cwU0e2tW9rYM4hm03TH6aIl2P5JFJeo_dcX41gEG0PDf8g0KubMVkPogKKBeOTHg_F_2uxOjjg8UtVJeC6EG0Ptf8g0M5; &quot; \\ &quot;H_BDCLCKID_SF=JRA8oKPhtCvhDRTvhCcjh-FSMgTBKI62aKDs2ROYBhcqEIL4W6-aKPIp5nO8W4KL-6nH2IOK5DoEHxbSj4QzQU0zDPvl0RQuWI3bhqvztp5nhMJFXj7JDMP0qJ7j2RQy523ion6vQpn-KqQ3DRoWXPIqbN7P-p5Z5mAqKl0MLPbtbb0xXj_0j63-DaAJt6njJTRe0Rj-KbP_hDL9eKTjhPrMjHrJWMT-0bFH_JR2QJP5fUbJQpPVKJ3-MMc9BMvuJan7_JjYbRDVMfQhWhJChtLeyU6r3fQxtNRB-CnjtpvhKJjjWtcobUPUDMc9LUvqHmcdot5yBbc8eIna5hjkbfJBQttjQn3hfIkj2CKLtD05MDIwjTt3-4LtMxrXKK6QKCjj3toObnnqq6rkbJ83yhFzXP6-35KHf6rMKR_bbhrBj5CzKJOb5MrXeJb-5h37JD6yb-n5Lb8KDRPGehrAMJFYQ4oxJpO7BRbMopvaKquVoJQvbURvD-ug3-7qex5dtjTO2bc_5KnlfMQ_bf--QfbQ0hOhqP-jBRIEoD-2tKD5MIDr5nJbq4I85M5H54cX--o2WbCQaM7O8pcNLTDKLnLNjb72-MC8BNrJ-KTua-PMjbCxjqO1j4_PMa8OKhFeLHIebbQJyIb-hl5jDh3Ub6ksD-Rt5frp2aRy0hvc0J5cShnkDMjrDRLbXU6BK5vPbNcZ0l8K3l02V-bIe-t2XjQhDG8JJ5_eJbCsL-35HJcqfbo4-tr_KICShUFs5b5lB2Q-5KL-0-oieCbn5p_2bjD33NrHXxuJ2IDjoMbdJJjzDKoMjf6Py4LeBUO2XT0D52TxoUJg5DnJhhvG-xc4Mp8ebPRi3tQ9QgbMMhQ7tt5W8ncFbT7l5hKpbt-q0x-jLTnhVn0MBCK0HPonHjLBDTb33H; &quot; \\ &quot;H_PS_PSSID=1460_21110_30210_30284_22160; &quot; \\ &quot;yjs_js_security_passport=3b54287a45b25629d2f7cfdb10008d7e9d67b5bd_1576637318_js; &quot; \\ &quot;BDRCVFR[feWj1Vr5u3D]=I67x6TjHwwYf0; &quot; \\ &quot;delPer=0; &quot; \\ &quot;BD_CK_SAM=1; &quot; \\ &quot;PSINO=2; &quot; \\ &quot;BD_HOME=1; &quot; \\ &quot;H_PS_645EC=b428Vxu0iZX7iP6ukpImwh5YVC5tlT7Iwn5ZIzZa5ijoKNVsMoi%2FCv%2FtNME&quot; 将准备好的cookie进行处理,并添加到jar对象中去for i in Cookie.split(‘;’): key,value = i.split('=',1) jar.set(key,value) Headers = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36&quot;, } params = { &quot;wd&quot; : '666' } 发送请求时添加cookies参数response = requests.get(url=url,params=params,headers=Headers,cookies=jar)response.encoding = ‘utf8’ text = response.textprint(text) 123456789101112131415161718192021222324252627 - 上述为第二种cookies的使用方法2. 全自动添加cookie - ```python from requests import Session # 构建全安心的Session实例 session = Session() url = 'https://www.baidu.com/s' Headers = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36&quot;, } params = { &quot;wd&quot; : '666' } response = session.get(url=url,headers=Headers,params=params) response.encoding = 'utf8' text = response.text 使用session的时候,将会全自动生成cookies 同时,相比较requests访问,session访问将会存储状态, requests访问时无状态访问,session访问将会进行状态保持,维持会话 省略了大量的cookie代码,相比较来说,更加方便 ```python requests访问的时候,每次都需要传入cookie如果需要访问的网站时需要维持登陆状态的话,使用resquests将会很不方便,因为没有状态保持session可以维持用户的登陆状态1234567891011121314151617## SSL证书验证:```pythonimport requestsimport urllib3url = 'https://www.tutumanhua.com/gaoxiao/'# 关闭网页证书报错urllib3.disable_warnings()res = requests.get(url=url, verify=False)# verify=False # 关闭SSL证书验证 Request,Session:1234567891011121314151617from requests import Session,Requesturl = 'https://www.baidu.com'session = Session()headers = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36&quot;,}res_obj = Request(method='GET',url=url,headers=headers)# 对准备好的数据进行处理,并添加请求方式res_obj2 = session.prepare_request(res_obj)# 使用session.prepare_request对数据进行处理,并打包好ret = session.send(res_obj2)# 将打包好的数据发送出去,并接受返回的数据ret.encoding = 'utf-8'with open('./baidu.html', 'w', encoding='utf-8') as f: # 持久化存储 f.write(ret.text) Urllib3: request","link":"/2022/11/30/Study-notes-Python-Spider-requests%E8%AF%B7%E6%B1%82%E6%A8%A1%E5%9D%97%E8%AF%A6%E8%A7%A3/"},{"title":"Scrayp详解","text":"scrapy: 什么是框架? 继承了各种功能且具有很强通用性(可以被应用在各种不同的需求中)的一个项目模板 我们需要做的就是学习怎么使用这些框架的功能 scrapy框架集成的功能有哪些? 高性能的数据解析操作 高性能的数据下载操作 持久化数据存储 scrapy通常只用于get请求,并不适用于post请求的模拟登陆 scrapy环境安装 pip install wheel pip install twisted twisted下载 pip install pywin32 pip install scrapy 开始创建scrapy工程 进入终端输入指令创建新的scrapy工程 ‘scrapy startproject projectname’ 按照指令创建新的爬虫文件 scrapy genspider spiderName www.xxx.com 启动爬虫程序 scrapy爬虫不能直接运行, 在命令窗口中输入指令 scrapy crawl spiderName scrapy crawl spiderName –nolog 运行时不输出日志信息 在setting文件中添加配置 LOG_LEVEL= ‘ERROR’’ 当发生错误时,将错误日志输出,方便调试 scrapy工程设置–—–-setting User_Agent 这里设置的是爬虫的请求头 ROBOTSTXT_OBEY = True 这里设置的是robots协议,当值为True时,爬虫将会在运行时优先查看robots协议,如果协议不允许将不会进行爬取 scrapy工程使用 构建解析 ```python -- coding: utf-8 --import scrapyclass NewSpiderSpider(scrapy.Spider): # 爬虫文件的名称,相当于爬虫文件的唯一标识 name = '💀' # 循序的域名, 通常情况下不会使用 # allowed_domains = ['www.baidu.com'] # 起始的url列表, scrapy将会对列表中的url自动进行请求发送 start_urls = ['http://www.budejie.com/'] def parse(self, response): # 在scrapy中,数据解析不需要手动导入etree来进行,相对的,这里面集成了etree 的功能,也就是说,我们可以直接使用scrapy的集成来达到我们进行数据解析的目标 # 这个功能的使用方式如下 res = response.xpath('//div[@class=&quot;j-r-list-c-desc&quot;]/a/text()').extract() # extract方法,提取获取到的selected对象中的data数据,当对象为单个对象的时候,获取到的对象就是单个的字符串, # 当提取到的selected对象为list对象时,获取到的数据也会自动变成一个列表,并不需要循环遍历selected列表来进行提取 for i in res: print(i) 1234567891011121314151617181920212223- scrapy持久化存储 - 基于终端窗口的持久化存储 - 特性:只可以将parse方法的返回值写入到本地的磁盘文件中 - 指令: scrapy crawl spiderName -o filePath - ```python def parse(self, response): li_list = response.xpath('//div[@class=&quot;j-r-list&quot;]/ul/li') all_data = [] for li in li_list: author = li.xpath('./div[1]/div[2]/a/text()').extract()[0] content = li.xpath('./div[2]/div[1]/a/text()').extract()[0] dic = { 'Author': author, 'Content': content } all_data.append(dic) return all_data # scrapy 将会对parse的返回值进行处理,并预置了内部的持久化存储模块.这个持久化存储基于命令终端运行,该选项不支持txt文件存储,目前仅支持('json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle')这七种文件 - 基于管道的持久化存储 管道话存储需要使用scrapy中封存的一些方法,同时需要进行一些处理,这样的存储方式支持txt文档存储 管道文件存储示例: ```pythonclass FirstblodPipeline(object): fp = None # 设定在管道运行开始之前优先运行的代码 def open_spider(self, spider): print('爬虫开始运行~~~~~~~') # 通过在管道运行开始之前打开文件的形式来确保这个文件每次只需要打开一次 self.fp = open('./firstBlod/spiders/all_data.txt', 'w', encoding='utf-8') # 当数据被提交时执行的代码,在这里进行数据持久化存储 def process_item(self, item, spider): author = item['author'] content = item['content'] # 调用已经打开的文件,并在里面进行写入操作 self.fp.write(author + ':' + content + '\\n') # 将item转交给下一个管道类 return item # 当管道关闭时执行的代码 def close_spider(self, spider): print('爬虫结束运行~~~~~~~') # 在代码整体运行结束的时候,关闭打开的文件 self.fp.close() 12345678910111213- item文件对象实例- ```python class FirstblodItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() # 构建新的item对象中的参数 # 格式为 name = scrapy.Field() # 使用field文件格式的时候,兼容几乎所有的文件类型 # 包括但不仅限于 列表,元组,字典,字符串,数字,二进制流等各种各样的形式 author = scrapy.Field() content = scrapy.Field() settings文件实例 ```python Configure item pipelinesSee https://docs.scrapy.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = { ‘firstBlod.pipelines.FirstblodPipeline’: 300, # 管道对象的名字,后面的数值为管道优先值 # 优先值高的将会优先执行 # 数值越低优先值越高 } 123456789101112131415161718192021222324252627- 找到管道相关代码进行激活- 在爬虫文件中进行使用- ```python import scrapy from firstBlod.items import FirstblodItem # 导入item设定类 class NewSpiderSpider(scrapy.Spider): name = '💀' start_urls = ['http://www.budejie.com/'] def parse(self, response): li_list = response.xpath('//div[@class=&quot;j-r-list&quot;]/ul/li') # all_data = [] for li in li_list: author = li.xpath('./div[1]/div[2]/a/text()').extract()[0] content = li.xpath('./div[2]/div[1]/a/text()').extract()[0] item = FirstblodItem() # 实例化item对象 item['author'] = author item['content'] = content # item使用方法和字典类似 # 使用yield方法返回封装完成的item对象 # 当使用yield返回的时候,将会自动将item传输到管道对应的接受类中,进行处理并持久化存储 yield item 将上述条件准备完毕之后,就可以进行数据的持久化存储了 当然,也不一定非要存储到文件中去,毕竟还有一些涉及到数据库的存储,都可以放到这个管道中来进行 也就是,管道文件中的一个管道类负责一种持久化存储的方案 item提交的时候将会把item交给优先级最高的管道类 在管道类中,return item的作用是将item转交给下一个管道类 双管道类写法以及Mysql数据库写入 ```pythonclass MysqlPip(object): conn = None cursor = None # 设定在管道运行开始之前优先运行的代码 def open_spider(self, spider): # 通过在管道运行开始之前打开文件的形式来确保这个文件每次只需要打开一次 print('爬虫2开始运行~~~~~~~') # 建立数据库游标 self.conn = pymysql.Connect(host='127.0.0.1',port=3306,user='root',password='000000',db='spider',charset='utf8') print(self.conn) # 当数据被提交时执行的代码,在这里进行数据持久化存储 def process_item(self, item, spider): author = item['author'] content = item['content'] sql = 'insert into bs values (&quot;%s&quot;,&quot;%s&quot;)'% (author,content) print(sql) self.cursor = self.conn.cursor() try: self.cursor.execute(sql) self.conn.commit() except Exception as e: print(e) self.conn.rollback() print('----------------------------------------------------------------------------') return item def close_spider(self, spider): print('爬虫2结束运行~~~~~~~') self.cursor.close() self.conn.close() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960- 手动发送请求: - 手动发送get请求 - 应用场景: 请求同一个主页面下的多个页面 - 代码 ```python # -*- coding: utf-8 -*- import scrapy from firstBlod.items import FirstblodItem class NewSpiderSpider(scrapy.Spider): # 爬虫文件的名称,相当于爬虫文件的唯一标识 name = 'new_spider' # 循序的域名, 通常情况下不会使用 # allowed_domains = ['www.baidu.com'] # 起始的url列表, scrapy将会对列表中的url自动进行请求发送 start_urls = [ 'https://www.zhipin.com/job_detail/?query=python&amp;city=101010100&amp;industry=&amp;position=', ] # 设置基本的url模板 url = 'https://www.zhipin.com/c101010100/?query=python&amp;page=%d&amp;ka=page-%d' # 记录页数 page_name = 1 def parse(self, response): li_list = response.xpath('//div[@class=&quot;job-list&quot;]') print(li_list) # all_data = [] for li in li_list: # 职位名称 Job_title = li.xpath('./div/div[1]/h3/a/div[1]/text()').extract()[0] # 公司名称 Corporate_name = li.xpath('./div/div[1]/a/text()').extract()[0] # 地址 all_address = li.xpath('./div/div[1]/p//text()').extract()[0] # 经验 experience # address,experience,Education = all_address.split('|') #学历 Education # dic = { # 'Author': author, # 'Content': content # } # all_data.append(dic) item = FirstblodItem() item['Job_title'] = Job_title item['Corporate_name'] = Corporate_name yield item # 确定当前页数, if self.page_name &lt;= 5: self.page_name += 1 # 拼接新的url new_url = format(self.url%(self.page_name,self.page_name)) # 使用yield进行请求,参数callback为处理这些页面信息所用的函数,可自行设定,也可用递归的方式使用当前的函数 yield scrapy.Request(new_url,callback=self.parse) else: return li_list 重写父类的自动请求方法 1234# 重写父类方法意味着我们可以自定义数据清洗函数,而不需要局限于parsedef start_requests(self): for url in self.start_urls: yield scrapy.Request(url,callback=self.parse) 要想使scrapy自动发送get请求,需要重写start_requests方法 scrapy图片处理: 在scrapy中,有着专门的模块对图片数据进行请求以及处理,我们只需要将获取到的图片url以item的形式传输到我们的管道之中进行处理即可,item对象的创建于寻常的创建方法没设么区别,scrapy.Fields包容性极强,不需要考虑兼容性问题, 在提交到管道中去之后,我们可以在管道之中对item中的数据进行处理, 使用scrapy中封装的图片处理专用类scrapy.pipelines.images import ImagePipeline 创建一个新的管道类,这个管道类继承自ImagePipline ```pythonclass ImgSpidersPipeline(ImagesPipeline): # 对一个图片链接进行请求发送 # item 就是scrapy提交过来的item数据 def get_media_requests(self, item, info): yield scrapy.Request(item['src']) # 准备文件名 def file_path(self, request, response=None, info=None): file_name = request.url.split('/')[-1] print('正在下载',file_name,'............') return file_name # 将item传递给下一个即将执行的管道类 def item_completed(self, results, item, info): return item 1234567891011121314156. 完成这些之后,我们还需要对图片的存储路径进行设置,在setting文件中添加新的属性 - IMAGES_STORE = “imagePath” - imagePath自行设置,最好设置为绝对路径,如果使用相对路径的话,最好参考django中的文件路径的写法,先确定工程的路径,然后再在工程路径的基础上设定相对路径 - django方法设置路径如下: - ```python import os # 获取当前文件夹的主路径 BASE_DIR = os.path.dirname(os.path.abspath(__file__)) # 将要添加的图片文件保存路径加入到主路径中去 IMAGES_STORE = os.path.join(BASE_DIR,'imgsLib') 这个方法同样设置在settings文件中 完成这些之后,只需要在settings中将写好的管道类进行注册,即可开始这个管道类 运行爬虫程序,将会开始全自动下载选中的图片,根据之前学过的方法,即设定网址的基础模板,即可实现图片的批量下载 如何提高scrapy爬虫的效率 增加并发数量 CONCURRENT_REQUESTS = 32 默认并发数量为32 可以自行设置 降低日志等级 LOG_LEVEL = “INFO” LOG_LEVEL = “ERROR” 禁止cookie COOKIES_ENABLED = False 在scrapy中会自动对cookie进行处理,不管这个页面是否需要验证cookie 将cookie处理模块进行关闭,将会提升scrapy的执行效率 禁止重试 scrapy将会自动对失败的请求进行重试,这严重影响到了爬虫的执行效率,可以将重试功能禁掉,从而提升执行效率 在settings文件中书写代码 RETRY_ENABLED = False 当请求失败的时候,scrapy将不会去处理请求失败的数据,而是会直接跳过进行下一条 减少下载超时 DOWNLOAD_TIMEOUT = 10 写入这段代码,作用是设定请求超时的时间,也就是说,当你在亲求时间超过了十秒的时候依然没有拿到数据的话,将会结束请求,执行下一项,而不是一直等待下去,这个熟知的单位是秒请求传参: 基于请求传参可以实现深度爬取 请求传参,在进行scrapy.Request请求的时候,可以使用第三个参数meta 这个参数的作用是向上一个参数callback这个解析函数中传递参数,她的数据类型是一个字典,通过键值对的形式存储需要传递的数据 当我们需要将上一个解析函数中实例化好的item对象传递到下一个解析函数中的时候,可以使用这个方法进行传递 在另一个解析函数 中,通过response参数的meta属性可以拿到参数的内容并进行提取,这个就可以拿到上一个函数中传递过来的item对象,实现不同解析函数之间的请求传递 通过这个方式可以实现深度爬取,即 当一整套数据的内容如标题和简介存在于两个关联的页面中,那么我们想要同时获得标题和简介,就需要进行深度爬取,使用这个方式传递的话,将会更简便的实现深度爬取的要求 中间件:在创建好的scrapy工程中,自带了两个基础的中间件 爬虫中间件TwoSpidersSpiderMiddleware 下载中间件TwoSpidersDownloaderMiddleware 拦截中间件 作用:批量拦截请求 拦截请求: UA伪装 目的:将所有的请求的请求头尽可能多的不同的请求载体标识 代理操作 ```pythondef process_exception(self, request, exception, spider): # 在错误捕获阶段进行代理修正, if request.url.split(':')[0] == 'http': request.meta['proxy'] = 'http://' + random.choice(PROXY_http) else: request.meta['proxy'] = 'http://' + random.choice(PROXY_https) return request 1234567891011- ```python def process_request(self, request, spider): # 实现将拦截到的request请求尽可能多的设定成不同的请求载体身份标识 request.headers['User-Agent'] = random.choice(user_agent_list) # 在每次请求之前进行代理修正 if request.url.split(':')[0] == 'http': request.meta['proxy'] = 'http://' + random.choice(PROXY_http) else: request.meta['proxy'] = 'http://' + random.choice(PROXY_https) return None 拦截响应 篡改响应对象或直接替换响应对象 下载中间件 ```python import scrapy from selenium.webdriver import Chrome,ChromeOptions from selenium.webdriver.chrome.options import Options from wangyi.items import WangyiItem 创建options实例对象,实现无头浏览器 chrome_option = Options() chrome_option.add_argument(‘–headless’) chrome_option.add_argument(‘–disable-gpu’) class WangyiSpider(scrapy.Spider): name = 'wangyi' # allowed_domains = ['www.xxx.com'] # 原始url网页 start_urls = ['https://news.163.com/'] pro = Chrome(chrome_options=chrome_option) # 用来存储后续的所有子页面的url cls_url_list = [] def parse(self, response): # 准备要爬取的目标 index_list = [3,4] li_list = response.xpath('//*[@id=&quot;index2016_wrap&quot;]/div[1]/div[2]/div[2]/div[2]/div[2]/div/ul/li') # print(li_list) for index in index_list: li = li_list[index] # 获取子页面url new_url = li.xpath('./a/@href').extract_first() # 获取独影的分类名称 news = li.xpath('./a/text()').extract_first() # 将子页面的url添加到类属性中进行存储 self.cls_url_list.append(new_url) # 开始对子页面进行过请求,并将请求数据交给下一个解析函数 yield scrapy.Request(url=new_url,callback=self.new_prase) def new_prase(self,response): div_list = response.xpath('/html/body/div/div[3]/div[4]/div[1]/div/div/ul/li/div/div') for div in div_list: # 获得新闻标题与新闻详情页面的url title = div.xpath('./div/div/h3/a/text()').extract_first() new_url = div.xpath('./div/div/h3/a/@href').extract_first() # 创建item对象,并将后面会用到的标题存储进去 item = WangyiItem() item['title'] = title # 开始对详情页面进行请求,并将获取到的页面源码与item对象一起传递给下一个解析函数 yield scrapy.Request(url=new_url,callback=self.content_parse,meta={'item': item}) def content_parse(self,response): # 抽取传递过来的item对象 item = response.meta['item'] # 解析源码中的内容数据 content = response.xpath('//*[@id=&quot;endText&quot;]/p/text()').extract() # 将内容存储进item对象 item['content'] = ''.join(content) # 将数据提交给管道 yield item def closed(self,spider): # 当爬虫结束时,关闭selenium浏览器 self.pro.quit() 1234567891011121314151617181920 以上为爬虫文件中的构造2. ```python class WangyiPipeline(object): # 在爬虫开始运行时进行文件的创建 def open_spider(self, spider): self.fp = open('./news.txt', 'w', encoding='utf-8') # 对提交的item数据进行处理 def process_item(self, item, spider): title = item['title'] content = item['content'] # print(item) # 将item数据进行存储 self.fp.write('[' + 'title' + ':' + title + ',' + 'content' + ':' + content + ']' + '\\n') return item def close_spider(self, spider): # 爬虫结束时,关闭打开的文件 self.fp.close() 以上为管道中的代码 ```python from scrapy import signals from scrapy.http import HtmlResponse from time import sleep class WangyiDownloaderMiddleware(object): # 捕获响应数据并进行检测 def process_response(self, request, response, spider): # 抽取爬虫文件中的模拟浏览器 pro = spider.pro # 对请求对象进行解析,当对象为主页面的时候,无视 # spider参数为实例化的爬虫类 # 可以直接调用爬虫类的一些方法 if request.url in spider.cls_url_list: # 通过模拟浏览器发起请求,绕开动态加载 pro.get(request.url) # 等待一秒,确保数据加载完成 sleep(1) # 获取页面源码 page_text = pro.page_source # 实例化HtmlResponse对象 # url 请求的url对象 # body 要返回的页面源码数据 # encoding 设定页面源码的编码格式 # request 请求不变 new_response = HtmlResponse(url=request.url,body=page_text,encoding='utf-8',request=request) # 将准备好的数据返回 return new_response return response 12345678910111213 以上为中间件中的数据 4. 在这三种模块的辅助之下,可以实现对网站数据的深度爬取,必要时,可以添加页码数据,从而确保这个爬虫可以精确定位到每一个分类下的所有数据的每一页数据,也就是说,使用scrapy爬虫,可以直接实现对整个目标网站的覆盖性爬取数据 5. 同时,由于scrapy默认是异步运行的,这种形态的爬虫,工作效率比起requests要高得多3. ### UserAgent伪装 1. ```python from fake_useragent import UserAgent a = UserAgent() print(a.Chrome)","link":"/2022/11/30/Study-notes-Python-Spider-scrapy%E8%AF%A6%E8%A7%A3/"},{"title":"redis概览","text":"redis数据库安装与使用: 前往github下载要使用的版本的压缩包 解压缩文件 cd到安装目录 运行redis-server 将安装目录添加到系统环境变量中 运行redis-server 再次打开一个cmd窗口,输入redis-cli即可进入redis环境 keys * 查看当前所有数据 端口: 127.0.0.1:6379 redis数据类型: list llen 获取列表长度 lpush 在表头添加数据 rpush 在表尾添加数据 lpop 从表头弹出数据 rpop 从表尾弹出数据 rpoplpush 从表尾弹出数据并添加到表头 lrange listName num num 从xx到xx的所有数据,支持负数 lindex listName num 指定下标获取对应的值 linsert key (before) value newvalue 在某个值之前添加另一个新的数值 linsert key (after) value newvalue 在某个值之后添加另一个新的值 lrem key n value 删除某个值,n为次数 当n &gt; 0,则删除n个对应的值,且删除顺序从左往右 当n &lt; 0,删除n个对应的值,且删除顺序从右往左 当n = 0,则删除所有符合条件的数据 string 二进制安全的键值对,以key:value的形式存在,也就是说,可以存储二进制文件 string单个大小最大为512M set set设定键值对 get根据键获取对应的值 append 在键对应的值后面追加新的数据,同时具备set的概念,如果追加的目标是一个不存在的键,将会自动创建出来 strlen 获取键对应的字符串的长度 setnx 对一个键进行赋值,如果该键存在,则不会有任何效果,如果该键并不存在,则进行创建并赋值 incr 对键对应的值进行+1操作,前提是这个值是纯粹的数字 decr 对键对应的值进行-1操作,前提是这个值不是纯数字 incrby keysName num 对键所对应的值进行加法操作, decrby keysName num 对键所对应的值进行减法操作 hash zset redis基础操作: keys * 查看所有的数据 exists name 查看符合该名称的数据总量 del name 删除目标数据 expire name succes 为一个数据设置过期时间,时间单位默认为秒,需输入数字 ttl name 查看该对象还有多久过期 dbsize 查看当前正在使用的库中的数据的数量 Flushdb 清空当前的库 Flushall 清空所有的库","link":"/2022/11/30/Study-notes-Python-Spider-redis%E6%A6%82%E8%A7%88/"},{"title":"Xpath与懒加载","text":"xpath模块使用: 安装模块: ```pip install xpathpip install lxml 1234567891011121314151617181920212223242. 开始使用 1. ```python from lxml import etree # 从lxml导入etree包 res = etree.parse('./baidu.html',parser=etree.HTMLParser(encoding=&quot;utf-8&quot;)) # 创建新的etree实例 # 第一个参数为要匹配的文件 # 第二个参数为要使用的编码方式 # etree.HTMLParser(encoding='utf-8) # xpath表达式的使用方式 # 1-标签匹配 result_1 = res.xpath('//body/div')[0] # 2-索引匹配 # 索引定位时的索引并不是从0开始,而是从1开始的 result_2 = res.xpath('//body/div[1]') # 3-属性匹配 result_2 = res.xpath('//body/div[@class=&quot;song&quot;]') result_2 = res.xpath('//body/div[@id=&quot;song&quot;]') print(result_1) print(result_2) 要注意的是,xpath使用时需要传入要使用的编码方式,详情请看上方的代码块 ```pythonresult_2 = res.xpath(‘/html/body/div’) 层层定位的标签属性,并不局限于三层用这种方法进行定位传回的数据将会精确到单个目标,result_2 = res.xpath(‘//body/div’) 使用两个//将会进行全局匹配,并不在局限于精确的目标,也就是说的,结果返回的数据不一定是head标签中的数据,也不一定是是body中的数据,这样匹配的将会是全局的数据result_2 = res.xpath(‘/html/body/div[1]’) 使用索引后返回的,将会是对应索引位置的数据result_2 = res.xpath(‘//div[1]’) 使用//与索引返回的,是全局中每一个大标签中所有符合条件的数据之中的第一个和正常python中的索引概念不同的是,这里索引并不是从0开始进行计算的 ,而是从开始计算的也就是说,res.xpath(‘//div[1]’)等同于res.xpath(‘/html/body/div’)[0]result_2 = res.xpath(‘//div[1]/text()’) 获取数据中的直系文本信息这里的text不同于在python中,需要加上()result_3 = res.xpath(‘//div[1]//text()’) //获取的是全部的文本内容result_2 = res.xpath(‘//a[1]/@href’) @+属性名,获取对应属性的值1234567891011121314151617181920212223242526272829303132334. 使用xpath直接从获取到的网页中提取数据5. ```python from lxml import etree # # res = etree.parse('./baidu.html',parser=etree.HTMLParser(encoding=&quot;utf-8&quot;)) # result_2 = res.xpath('//a/text()') # result_2 = res.xpath('//a[1]/@href') # # print(result_2) from requests import Session session = Session() url = 'https://www.baidu.com/s' Headers = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36&quot;, } params = { &quot;wd&quot; : '666' } response = session.get(url=url,params=params,headers=Headers) response.encoding = 'utf8' # 上述为正常的获取页面过程 est = etree.HTML(response.text) # 创建etree对象 # 这里使用的解析方式不再是parse,而是使用HTML # parse的使用场景为,对文件类型的数据进行解析 a_list = est.xpath('//a//text()') print(a_list) 当使用xpath匹配到的对象的时候,同样可以使用xpath方法 ```python 当使用匹配到的对象的时候li_list = est.xpath(‘//div//li)’)a_list = []for li in li_list: a_list += li.xpath('./div/a') for a in a_list: href = a.xpath('./@href') print(href) 当进行第二次xpath时,匹配的公式为’./tagName’./意味着从当前的对象的下面开始匹配,将这个对象作为根节点,而不是html如果在这里使用//tagName,则意味着还是从html标签开始进行匹配123456789108. last()函数9. ```python li_list = est.xpath('//div//li[last()])') # last()在xpath中的意思是最后一个,当然, li_list = est.xpath('//div//li[last()-1])') # last()-1就是倒数第二个 contains() ```python contains()函数,匹配属性值中包含某个值的对象12345612. postion()13. ```python li_list = est.xpath('//div//li[postion()&gt;5])') # postion()函数,按照范围进行取值 懒加载:在页面中,设定好的属性中,要求只有当对应的属性被观测到的时候才会变成正常的属性,也就是说,当没有被观测到的时候,都将会是另外一个 在被观测到之前,该属性的存在毫无意义,但当他在页面上的可视化区域出现的时候,将会被激活,并变更为另一种可以使用的形态 我愿称之为量子态 在被可视化页面观测到之前,该属性即存在但又不存在,只有当进入可视化页面的时候,才会发生属性变化,成为存在的数据 而但我们使用requests模块进行球球的时候,对于页面的代码而言,他们仍然处于没有观测的状态,所以,当你想要匹配这种代码的时候,就不能使用正常的做法进行匹配, 而是需要找到在为观测状态下的目标代码,根据其特征进行匹配 俗称—高维干涉 (ˉ▽￣～)","link":"/2022/11/30/Study-notes-Python-Spider-xpath%E4%B8%8E%E6%87%92%E5%8A%A0%E8%BD%BD/"},{"title":"Selenium详解","text":"selenium详解： web自动化测试用的框架，通多代码实现对浏览器的控制，如： 打开网页，点击网页中的元素，实现鼠标滚动等操作 概念：selenium是一个基于浏览器自动化的一个模块 环境安装： 下载selenium模块 selenium和爬虫之间的关联是什么？ 便捷的获取页面中动态加载的数据 实现模拟登录 基本操作： 谷歌浏览器驱动程序下载地址 将下载好的驱动程序放入谷歌浏览器文件包以及python。scripts中 在系统环境中添加谷歌浏览器 实例化某一款浏览器对象 ```pythonfrom selenium import webdriver 进行了环境配置之后，可以不写要使用的驱动程序，它会自动调用对应的浏览器驱动程序bro = webdriver.Chrome() 指定一个urlurl = ‘https://www.baidu.com'bro.get(url) 找到要传入的目标的标签123456789101112131415161718- 关于对象捕获 - ```python # xpath方法捕获详情 from selenium import webdriver import time es = webdriver.Chrome() es.get('https://www.baidu.com/') search_input = es.find_element_by_xpath('//input[@id=&quot;kw&quot;]') btn = es.find_element_by_xpath('//input[@id=&quot;su&quot;]') btn.click() time.sleep(3) shouye = es.find_elements_by_partial_link_text('首页')[0] shouye.click() ```pythonid捕获方法详情from selenium import webdriverimport timees = webdriver.Chrome()es.get(‘https://www.baidu.com/')search_input = es.find_element_by_id(‘kw’)search_input.send_keys(‘俺也一样’)btn = es.find_elements_by_id(‘su’)[0]btn.click()time.sleep(3)shouye = es.find_elements_by_partial_link_text(‘首页’)[0]shouye.click()1234567 - 这两种方法都是精确定位，在没有id的情况下，就可以适当的选择xpath进行捕获- selenium执行js语句 - ```python es.execute_script('window.scrollTo(0,document.body.clientHeight)') 关闭浏览器 ```pythones.quit()1234567891011121314151617181920212223242526272829303132333435## selenium的爬虫应用，对页面源码进行抽取：1. 使用requests时，我们需要考虑浏览器页面的动态加载所带来的差异，这些动态加载的数据在正常情况下无法用requests进行直接获取2. 在使用sesnium时，我们所能获得的页面数据实际上就是所有的数据，动态加载得到数据同样会存在于其中，也就是说，selenium对于网页源码的获取更加全面与简便```pythonfrom selenium import webdriverfrom lxml import etreeimport timeimport jsones = webdriver.Chrome()es.get(url='https://xueqiu.com/')time.sleep(3)res_text = es.page_sourcetree = etree.HTML(res_text)div_list = tree.xpath('//*[@id=&quot;app&quot;]/div[3]/div[1]/div[2]/div[2]/div[1]/div[@class=&quot;AnonymousHome_home__timeline__item_3vU&quot;]')json_dict = []for i in div_list: a_dict = {} a_dict['title'] = i.xpath('./h3/a/text()') a_dict['content'] = i.xpath('./p/text()')[0] a_dict['username'] = i.xpath('./div/div/a[@class=&quot;AnonymousHome_user-name_3wN&quot;]/text()')[0] a_dict['company'] = i.xpath('./div/div/span[1]/text()')[0] a_dict['creat_time'] = i.xpath('./div/div/span[2]/text()')[0] a_dict['count'] = i.xpath('./div/div[2]/text()')[0] json_dict.append(a_dict)with open('./xueqiu.json', 'w', encoding='utf-8') as f: f.write(json.dumps(json_dict,ensure_ascii=False,indent=4))time.sleep(5)es.quit() 在使用selenium时，有时候会遇到子页面的情况，通常，这种页面基于iframe存在，遇到这种情况时，我们需要对获取到的主页面源码进行处理，让他能够直接匹配子页面 ```python swithc_to 将目标iframe注册到页面es.switch_to.frame(‘iframeResult’) 1234567891011121314151617181920212223242526 ## selenium动作链：- 创建动作链 - 在我们需要对页面进行一连串的动作的时候，如： 拖动页面上的块，拖拽图片，挪动方块等，就需要使用到动作链 - ```python from selenium import webdriver from lxml import etree import time import json from selenium.webdriver import ActionChains es = webdriver.Chrome() es.get(url='https://www.runoob.com/try/try.php?filename=jqueryui-api-droppable') # 从网页中找到对应的iframe的id time.sleep(3) es.switch_to.frame('iframeResult') p = es.find_element_by_id('draggable') action = ActionChains(es) action.click_and_hold(p) for i in range(5): # move_by_offset()方法，第一个参数为水平移动的距离，第二个参数为垂直移动的距离 # perform作用是立即执行动作，也就是说，创建好就会进行执行 action.move_by_offset(10,3).perform() action.release() 上述为示范性代码 获取图片 无头浏览器： ​ phantomJS是一款无可视化界面的浏览器 ```pythonfrom selenium import webdriver 导入chrome的options中的Options对象from selenium.webdriver.chrome.options import Optionsimport time 创建一个实例化的Options对象chrome_options = Options() 为这个对象设置全性的状态chrome_options.add_argument(‘–headless’)chrome_options.add_argument(‘–disable-gpu’) 在绑定浏览器的时候对chrome_options进行重新的赋值es = webdriver.Chrome(chrome_options=chrome_options)es.get(‘https://www.baidu.com/')print(es.page_source) 123456783. 无可视化界面的作用是取消每次运行时打开的网页，但是依然保留浏览器访问的效果，也就是说，不需要每次都打开一个浏览器界面4. 这样的话，省去了很多不必要的资源占用5. ``` # save_screenshot 对页面进行截图并保存起来 es.save_screenshot('./2.png') 关于截屏与图片点击:123456789101112131415161718192021222324252627282930313233343536373839404142434445from time import sleepfrom lxml import etreefrom pil import Imagefrom selenium.webdriver import Chrome, ChromeOptionsfrom selenium.webdriver.chrome.options import Optionsfrom selenium.webdriver import ActionChainsoption = ChromeOptions()# option.add_experimental_option('excludeSwitches',['enable-automation'])chrome_option = Options()# chrome_option.add_argument('--headless')# chrome_option.add_argument('--dosable-gpu')es = Chrome(options=option,chrome_options=chrome_option)es.get('https://www.baidu.com/')sleep(5)es.save_screenshot('./day05/1.png')img = es.find_element_by_xpath('//div[@id=&quot;lg&quot;]/img')sleep(3)location = img.locationsize = img.sizerangle = (int(location['x']),int(location['y']),int(location['x'] + size['width']),int(location['y'] + size['height']))print(rangle)i = Image.open('./day05/1.png')frame = i.crop(rangle)frame.save('./day05/frame.png')# 构建数据流对图片进行点击操作action = ActionChains(es)# move_to_element_with_offset() 第一个参数是作为目标的图片,第二个参数和第三个参数分别为横向便宜和纵向偏移# click() 方法,进行点击# perform() 将设定好的数据流立即执行action.move_to_element_with_offset(img,5,5).click().perform()sleep(3)es.quit()","link":"/2022/11/30/Study-notes-Python-Spider-selenium%E8%AF%A6%E8%A7%A3/"},{"title":"代理IP与代理服务器","text":"代理IP 代理IP与代理服务器: 代理IP分类 透明代理IP 服务器知道你在使用代理IP,并且知道你的真实IP 匿名IP 服务器知道你在使用代理IP,但不知道你的真正IP 高匿名IP 服务器不知道你在使用代理IP,并且不知道你的真正IP 代理IP分为两种: 基于接口 获取固定数量的代理IP 这些IP地址时固定的,固定数量,如果你的请求数量过多的话,同样又被发现的风险 基于隧道 这种代理IP基于云端服务器存在,拥有一个云端的IP池 每次使用时,都会从庞大的IP池中随即调用代理IP 因此,基于隧道的代理IP在使用时几乎不会出现重复使用的IP 相比较固定数量的基于接口IP,更加的安全 相比较起来,使用基于隧道的代理IP更加好用,但是相对的是,基于隧道的代理IP价格更加昂贵 ```pythonimport requests url = ‘http://httpbin.org/get' proxies = { &quot;http&quot;: &quot;http://60.167.103.60:9999&quot;, &quot;https&quot;: &quot;https://183.166.138.137:9999&quot; }res = requests.get(url=url,proxies=proxies)print(res.text) 1234567891011124. 在进行高频请求的时候,如果你的爬虫被检测了出来,将会被返回一个特殊的错误:ConectionPool 1. 这个错误的意思就是说: 你已经被发现了,并且你用来进行请求的IP已经被禁掉了5. ```python proxies = [ {&quot;http&quot;: &quot;http://60.167.103.60:9999&quot;}, {&quot;http&quot;: &quot;http://60.167.103.60:9999&quot;}, {&quot;http&quot;: &quot;http://60.167.103.60:9999&quot;}, {&quot;http&quot;: &quot;http://60.167.103.60:9999&quot;} ] 代理ip池写法,创建一个列表,存储准备好的IP,然后使用随机模块进行随即调用,这样可以确保每次都会随机抽取一个代理ip来发送请求","link":"/2022/11/30/Study-notes-Python-Spider-%E4%BB%A3%E7%90%86IP/"},{"title":"动态获取数据","text":"动态获取数据 在一些网页中,进行ajax请求的时候需要传递参数,有些参数是特殊的动态参数,这些参数通常隐藏在页面中,只需要进行对应的匹配就能够进行获取 在一些网页中,cookie的获取可能是在执行ajax请求进行动态加载的时候才传递过来的,因此,想要获取对应的cookie,就需要是用Session模块在进行ajax请求的时候进行获取,才能拿到需要使用的cookie","link":"/2022/11/30/Study-notes-Python-Spider-%E5%8A%A8%E6%80%81%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE/"},{"title":"分布式爬虫实现原理与运行机制","text":"分布式爬虫 什么是分布式爬虫: 基于多台电脑组建一个分布式机群,然后让集群中的每一台电脑执行同一组程序,然后让他们对同一个网站的数据进行分布爬取 使用分布式爬虫的目的: 提升数据爬取效率 如何实现分布式爬取: 基于 scrapy + redis 的形式实现分布式爬虫 scrapy结合scrapy-radis组件实现分布式 原生的scrapy是无法实现分布式爬虫的 调度器无法实现共享,从而导致出现重复请求 管道无法实现共享,不利于数据汇总 scrapy-redis组件的作用: 提供可以被共享的调度器和管道 环境配置: 安装redis数据库 pip install scrapy_redis 编码流程: 创建一个工程 创建一个爬虫文件 spider 和 CrawlSpider 都可以使用 修改爬虫文件 导入组件 ```pythonfrom scrapy_redis.spider import RedisSpider,RedisCrawlSpider1234567891011121314 - 将创建好的爬虫文件类的父类修改为新导入的scrapy_redis的子类 - 将start_url替换掉,换成redis_key- ###### 修改settings - 指定管道 - ```python # 开启可被共享的管道 ITEM_PIPELINES = { 'scrapy_redis.pipelines.RedisPipeline': 300 } 指定调度器 ```python 指定使用可被共享的调度器增加了一个去重容器类的配置,作用使用 Redis的set集合来存储请求的指纹数据,从而实现请求去重的持久化对重复的请求对象去重DUPEFILTER_CLASS = ‘scrapy_redis.dupefilter.RFPDupeFilter’ 使用scrapy_redis组件自己的调度器SCHEDULER = ‘scrapy_redis.scheduler.Scheduler’ 配置调度器是否持久化,也就是说在爬虫运行结束之后要不要清除Redis中请求队列和去重指纹的set这样实际上已经实现了增量式,也就是说,再次运行的时候,将不会将已经获取过的数据重复的获取,增量式的概念就是:# 在创建爬虫之后,每次运行都会进行对比,紧紧对网站的更新内容进行爬取,而不会进行重复数据的重复获取,减少内存占用,同时也更加合理 SCHEDULER_PERSIST = True 1234567- 置定redis服务 - ```python # REDIS_HOST = 'redis 服务器地址' REDIS_HOST = '127.0.0.1' REDIS_PORT = 6379 redis配置文件redis.windows.conf bind 127.0.0.1 将这行代码注释掉 接触127.0.0.1绑定,使其他电脑可以访问本机redis protected-mode yes 将yes改为no,关闭redis数据保护,关闭后允许其他电脑修改本机redis 数据库 携带配置文件启动redis服务 redis-server ./redis.windows.conf 启动redis客户端 运行爬虫文件 cd ./spiders scrapy runspider spiderName.py 向调度器放入起始的 url 队列的位置 队列存储在队列中 lpush key value 按照爬虫中设定好的目标redis队列名称放入起始url 当你把起始url放入之后,将会自动开始爬取 爬虫文件运行之后将会把获取到的数据自动存储到redis中 数据存储到 spiderName:items, response存储到 spiderName:response 注意,在进行分布式的时候,redis 的ip地址必须保持精确,也就是说,想要做分布式最好使用一个内网连接机群","link":"/2022/11/30/Study-notes-Python-Spider-%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E4%B8%8E%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6/"},{"title":"文件操作","text":"文件操作 json文件:1234567import jsonlist_1 = []with open('./001.json', 'w', encoding='utf-8') as f: for i in range(100): list_1.append({'name':'我是%d'% i,'age': 66}) f.write(json.dumps(list_1,ensure_ascii=False,indent=4)) csvf文件:123456789101112import csvwith open('data.csv', 'w', encoding='utf-8') as csvf: # 构建csv文件基础属性,内容的分隔符 writer = csv.writer(csvf.delimiter=',') # 写入文件,传入的参数是一个列表 writer.writerow(['title','comment', 'good', 'bad']) with open('data.csv', 'a', encoding='utf-8') as csvf: # 构建csv文件基础属性,内容的分隔符 writer = csv.writer(csvf.delimiter=',') # 写入文件,传入的参数是一个列表 writer.writerow(['title','comment', 'good', 'bad'])","link":"/2022/11/30/Study-notes-Python-Spider-%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/"},{"title":"爬虫概念基础","text":"初次接触爬虫: 爬虫概念: 通过编写程序模拟浏览器上网,然后让其去互联网上爬取/抓取数据的过程 浏览器是一种原始的爬虫工具 关于网络爬虫分类: 通用爬虫 固定的获取一整张页面的数据,如title和url这样的通用数据,但是通常情况下,roots.txt都会对通用爬虫进行限制 通用爬虫获取到的数据并不精确 常用于浏览器的搜索引擎 聚焦爬虫 聚焦爬虫只获取特定页面的特定数据,其他数据弃之不用,与通用爬虫相反,聚焦爬虫获取的并不一定是通用的属性,更多的是某些页面特有的一些数据,如 小说网站的小说内容,就需要将无用的内容剔除,并进行排版之后存储到对应的txt文件中,这也是一些盗版网站提供的小说下载的方式 建立在通用爬虫之上 增量式爬虫 用来监视网站数据更新的情况,以便获得网页中最新更新的数据 爬虫的风险 爬虫干扰了被访问网站的正常运营; 爬虫抓取了受法律保护的特定类型的数据或信息 规避风险的方法 严格遵守网站设置的robots协议 在规避反爬虫措施时,需要优化自己的代码,避免干扰被访问网站的正常运行 在使用或传播抓取到的信息时,应审查所抓取的内容,如发现属于用户的个人信息、隐私或者涉及他人商业机密的,应即时停止并删除 网络协议1. OSI七层模型 12345678910111213141516171819202122232425262728293031323334- 应用层 - 应用层协议 - HTTP - 超文本传输协议 - Hyper Text Transfer Protocol - HTTPS - Hyper Text Transfer Protocol over Secure Socket Layer - 在Http的及穿上添加了SSL安全套接层,简称HTTPS - HTTP与HTTPS协议区别 - Https协议需要到ca申请证书,收费 - http是超文本传输协议,是明文传输,https是具有安全性的ssl加密传输协议 - http和https使用的是完全不同的连接方式,用的端口也不一样,前者80,后者443 - htto链接时无状态的,https协议是由SLL+HTTP协议共同构建的加密传输协议,比http协议安全 - FTP- 表示层- 会话层- 传输层 - 传输层协议 - TCP:是一种面向连接的可靠的,基于字节流的传输层通信协议 - 有序性:数据包标号,判断数据包的正确次序 - 正确性:使用checksum函数检查数据包是否虽坏,发送接收时都会计算校验 - 可靠性:发送端有超时重发,并由确认机制识别错误和数据的丢失 - 可控性:滑动窗口协议与拥塞控制算法控制数据包的发送速度 - UDP:用户数据报协议,面向无连接的传输层协议,传输不可靠 - 无连接:数据可能丢失或损坏 - 报文小:传输速度快 - 吞吐量大的网络传输,可以在一定程度上承受数据丢失- 网络层 - 网络层协议IP- 数据链路层 - 数据链路层协议ARP- 物理层 - 物理层协议––—-以太网协议 2. 五层模型 123456789- 应用层 - 应用层 - - 表示层 - 会话层- 传输层- 网络层- 数据链路层- 物理层 3. 四层模型服务器常见端口 mysql : 关系型数据库,端口:3306 MongoDB : 非关系型数据库,端口:27017 Redis : 非关系型数据库,端口:6379 ssh : Secure Shell的缩写,用于远程登录会话,端口:22 ftp : File Transfer Protocol的缩写,即文件传输协议,端口:21","link":"/2022/11/30/Study-notes-Python-Spider-%E7%88%AC%E8%99%AB%E6%A6%82%E5%BF%B5%E5%9F%BA%E7%A1%80/"},{"title":"增量式爬虫使用","text":"[TOC] 1—增量式爬虫简介 概念:​ 检测网站数据更新情况 核心:​ 去重 要点::​ 深度爬取类型的网站中需要对详情页的url进行记录和检测 记录:​ 将爬取过的详情页的url进行记录保存 ​ 将url存储到redis中去 检测:​ 在对某一个详情页的url发请求之前先要到记录表中进行查看,该url是否存在,如果存在的话,意味着这个url已经被爬取过了 2—使用流程及案例深度爬取类型的网站使用增量式爬虫爬取数据:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom redis import Redisfrom ..items import ZlsMovieproItemclass ZlsSpider(CrawlSpider): name = 'zls' conn = Redis() # 设置初试url start_urls = ['https://www.4567tv.tv/index.php/vod/show/id/8.html'] rules = ( Rule(LinkExtractor(allow=r'id/8/page/\\d+\\.html'), callback='parse_item', follow=True), ) def parse_item(self, response): # 进行数据解析,准备就进行深度爬取, # 因为这里的深度解析涉及两层页面,所以需要进行手动发送请求来确保更加精确 li_list = response.xpath('//div[1]/div/div/div/div[2]/ul/li') for li in li_list: movie_name = li.xpath('./div/div/h4/a/text()').extract_first() detail_url = 'https://www.4567tv.tv' + li.xpath( './div/div/h4/a/@href').extract_first() # 进行数据插入,通过sadd的返回值来辨别数据是否重复 # 将获取到的详情页的url存储到redis中 # 在执行sadd操作的时候,如果要插入的数据已经存在,将会返回0,并且插入失败,如果不存在,返回1,插入成功 ex = self.conn.sadd('movie_url_data', detail_url) if ex == 1: print('哦~一块新鲜的奶酪!') item = ZlsMovieproItem() item['movie_name'] = movie_name 将获取到的电影名称存储到item并进行传递 yield scrapy.Request(url=detail_url, callback=self.detail_parse, meta={'item': item}) else: print('腐朽的味道!!!') def detail_parse(self, response): item = response.meta['item'] detail_text = response.xpath( '//div[1]/div/div/div/div[2]/p[5]/span[3]/text()').extract_first() item['detail_text'] = detail_text print(item) yield item 不需要深度爬取的网站的注意点:数据指纹: ​ 一组数据的唯一标识 ​ 在数据库中构建指纹集合,换句话说,就是创建一个绝对不会重复的id,来确保数据爬取的时候不会重复获取已有的数据 3—核心要点","link":"/2022/11/30/Study-notes-Python-Spider-%E5%A2%9E%E9%87%8F%E5%BC%8F%E7%88%AC%E8%99%AB%E4%BD%BF%E7%94%A8/"},{"title":"关于创建虚拟环境","text":"创建虚拟环境 安装虚拟环境管理包 pip install virtualenvwrapper-win 创建虚拟环境，创建成功会自动切换该虚拟环境下 mkvirtulenv 虚拟环境名（name） 进入虚拟环境 workon name 在虚拟环境下安装包或模块，安装前要进入该虚拟环境 pip install 包名，框名 卸载包，pip uninstall 包名 查看虚拟机装了那些包 pip list 下载requeste pip install requests 删除虚拟环境 rmvirtualenv name 导出当前虚拟环境下所有的安装包和模板 pip freeze &gt; requirements.txt 一次性安装依赖，pip install -r requirements.txt 退出虚拟环境,deactivate","link":"/2022/11/30/Study-notes-Python-Spider-%E7%95%AA%E5%A4%96-%E5%85%B3%E4%BA%8E%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"title":"百度AI项目解析与使用","text":"百度AI 百度ai是开放性的库,在其中实现了包括文本识别,语言识别,声音识别,指纹识别,虹膜识别等各式各样基于人工智能AI的识别方法: 12345678from aip import AipNlp&quot;&quot;&quot; 你的 APPID AK SK &quot;&quot;&quot;APP_ID = '18141048'API_KEY = 'CPKSpk7Up1aATfxAaP9xln1O'SECRET_KEY = 'miR2GGKYfx214xEGUO0i83cgFmD3T1Z3'client = AipNlp(APP_ID, API_KEY, SECRET_KEY) 它的库是开放的,使用方法同样是开放的,所以我们在进行时使用的时候,可以直接调用对应的aip库中的识别方法,就可以将我们的数据分门别类地识别出来, 提取关键字分类 ```pythonclient.topic(title, content);文章分类,从标题与内容中提取关键字进行分类title,content都是必填项判断词义相似度123456789101112131415- ```python client.wordSimEmbedding(word1, word2); # 判断两个对象的相似度 # 这个结果判断的其实是两个对象的词义的相似度 # 返回结果为: ''' { &quot;score&quot;: 0.456862, # score 为该对象的相似度, &quot;words&quot;: { &quot;word_1&quot;: &quot;北京&quot;, &quot;word_2&quot;: &quot;上海&quot; } }''' 判断文本相似度 ```pythontext1 = “浙富股份” text2 = “万事通自考网” “”” 调用短文本相似度 “””client.simnet(text1, text2); “”” 如果有可选参数 “””options = {}options[“model”] = “CNN” “”” 带参数调用短文本相似度 “””client.simnet(text1, text2, options) ‘’’ 这个结果匹配的是文本的相似度,也就是说,并不在乎其中的意义,比较的是其中的内容{ &quot;log_id&quot;: 12345, &quot;texts&quot;:{ &quot;text_1&quot;:&quot;浙富股份&quot;, &quot;text_2&quot;:&quot;万事通自考网&quot; }, &quot;score&quot;:0.3300237655639648 //相似度结果 },‘’’ 12345678910111213141516171819202122232425262728293031323334353637383940- ### 抽取关键字- ```python title = &quot;iphone手机出现“白苹果”原因及解决办法，用苹果手机的可以看下&quot; content = &quot;如果下面的方法还是没有解决你的问题建议来我们门店看下成都市锦江区红星路三段99号银石广场24层01室。&quot; &quot;&quot;&quot; 调用文章标签 &quot;&quot;&quot; client.keyword(title, content); # 提取标题与文章中的关键字信息,也就说文字节点,这一点能够方便我们更快的定位想要的信息,以及进行更加精确的信息分类 ''' { &quot;log_id&quot;: 4457308639853058292, &quot;items&quot;: [ { &quot;score&quot;: 0.997762, &quot;tag&quot;: &quot;iphone&quot; }, { &quot;score&quot;: 0.861775, &quot;tag&quot;: &quot;手机&quot; }, { &quot;score&quot;: 0.845657, &quot;tag&quot;: &quot;苹果&quot; }, { &quot;score&quot;: 0.83649, &quot;tag&quot;: &quot;苹果公司&quot; }, { &quot;score&quot;: 0.797243, &quot;tag&quot;: &quot;数码&quot; } ] } ''' 错别字纠正 ```pythontext = “百度是一家人工只能公司” “”” 调用文本纠错 “””client.ecnet(text); 返回结果将会把错误信息与正确的信息封装到item中,并返回,根据这些信息可以将原来的数据进行修正,减少错别字‘’’{ &quot;log_id&quot;: 6770395607901559829, &quot;item&quot;: { &quot;vec_fragment&quot;: [ { &quot;ori_frag&quot;: &quot;只能&quot;, &quot;begin_pos&quot;: 21, &quot;correct_frag&quot;: &quot;智能&quot;, &quot;end_pos&quot;: 27 } ], &quot;score&quot;: 0.875169, &quot;correct_query&quot;: &quot;百度是一家人工智能公司&quot; }, &quot;text&quot;: &quot;百度是一家人工只能公司&quot; }‘’’ 1234567891011121314151617181920212223242526272829303132333435- ### 文字情绪识别- ```python text = &quot;本来今天高高兴兴&quot; &quot;&quot;&quot; 调用对话情绪识别接口 &quot;&quot;&quot; client.emotion(text); &quot;&quot;&quot; 如果有可选参数 &quot;&quot;&quot; options = {} options[&quot;scene&quot;] = &quot;talk&quot; &quot;&quot;&quot; 带参数调用对话情绪识别接口 &quot;&quot;&quot; client.emotion(text, options) # neutral 非强烈负面情绪 # pessimistic 强烈负面情绪 ''' { &quot;log_id&quot;: 4258005459150262970, &quot;text&quot;: &quot;本来今天高高兴兴&quot;, &quot;items&quot;: [ { &quot;prob&quot;: 0.998619, &quot;label&quot;: &quot;neutral&quot; }, { &quot;prob&quot;: 0.00138141, &quot;label&quot;: &quot;pessimistic&quot; }, ] } '''","link":"/2022/11/30/Study-notes-Python-Spider-%E7%99%BE%E5%BA%A6AI%E9%A1%B9%E7%9B%AE%E8%A7%A3%E6%9E%90%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"title":"移动端数据爬取","text":"移动端数据爬取 fiddler: fiddler抓包工具使用详解: 在启动fiddler之后,可以通过访问localhost:8888来进行访问 8888为fiddler工具中设置的端口号 集体位置为:","link":"/2022/11/30/Study-notes-Python-Spider-%E7%A7%BB%E5%8A%A8%E7%AB%AF%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96%E4%B8%8E%E8%A7%A3%E6%9E%90/"},{"title":"","text":"","link":"/2022/11/30/Study-notes-JavaScript-JavaScript-001-%E5%9F%BA%E7%A1%80/"},{"title":"进程线程协程与爬虫","text":"异步多进程爬虫实现: 12345678910111213from multiprocessing.dummy import Poolimport requests# 创建线程池pool = Pool(3)# 准备网址列表url_list = ['www.baidu.com','www.baidu.com','www.baidu.com']# 创建操作函数def get_requests(url): # 函数返回获取到的网页源码的数据 return requests.get(url).text# 将返回的页面源码数据接收request_text_list = pool.map(get_requests,url_list) 单线程多任务异步协程:正常协程实现:12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 单协程写法import asyncioimport timeasync func(url): print('start') time.sleep(3) print('stop') f = func(url)task = asynic.ensure_future(f)loop = asyncio.get_event_loop()loop.run_until+complete(task)# 多重协程写法# 多线程时,需要设置线程挂起,参考下列代码import asyncioasync func(url): print('start') # 在需要进行挂起的位置进行线程等待声明,即await await asyncio.sleep(3) print('stop') urls = [ 'www.baidu.com', 'www.baidu.com', 'www.baidu.com', 'www.baidu.com', 'www.baidu.com' ]tasks = []for url in urls: f = func(url) task = asynic.ensure_future(f) # 在创建好新的写成之后,将其添加到协程列表中 tasks.append(task)# 创建事件循环loop = asyncio.get_event_loop()# 将协程列表注册到事件循环中去# 在注册列表类型时，需要甚至线程挂起，所以需要使用asyncio。wait进行特殊声明loop.run_until+complete(asyncio.wait(tasks)) 在使用协程进行网页爬取的时候，是无法使用await挂起的，也就是说，requests并不支持挂起：在这个时候，需要使用支持await的请求模块。也就是aiohttp。12345678910111213141516171819202122232425import aiohttpimport asyncioimport timeasync def func(url): # 构建aiohttp请求 # 每一次请求都需要对 async with aiohttp.ClientSession() as a: async with await a.get(url=url) as response: result = await response.text() print(result) return resulturls = [ 'https://www.baidu.com', 'https://www.baidu.com', 'https://www.baidu.com']tasks = []for url in urls: f = func(url) task = asyncio.ensure_future(f) tasks.append(task)loop = asyncio.get_event_loop()loop.run_until_complete(asyncio.wait(tasks)) 在aiohttp中，和requests一样有get和post请求 这些请求的参数基本相同， 唯一不同的地方在于代理IP 在requests中，代理IP的类型是一个字典类型的数据， 但是在这里，代理IP的类型是一个字符串 同时参数名为proxy 在通常情况下,不要使用asyncio!!!在通常情况下,不要使用asyncio!!!在通常情况下,不要使用asyncio!!!在没有足够充足的代理与完善的伪装的时候,一定不要使用asyncio!协程的恐怖性能足以让任何网站的程序发现,即使没有发现,过于高频的访问也足以击溃网站了!","link":"/2022/11/30/Study-notes-Python-Spider-%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%8D%8F%E7%A8%8B/"}],"tags":[{"name":"技术","slug":"技术","link":"/tags/%E6%8A%80%E6%9C%AF/"},{"name":"基础","slug":"基础","link":"/tags/%E5%9F%BA%E7%A1%80/"},{"name":"教程","slug":"教程","link":"/tags/%E6%95%99%E7%A8%8B/"},{"name":"Scala","slug":"Scala","link":"/tags/Scala/"},{"name":"Spark","slug":"Spark","link":"/tags/Spark/"},{"name":"Big Data","slug":"Big-Data","link":"/tags/Big-Data/"},{"name":"LOL","slug":"LOL","link":"/tags/LOL/"},{"name":"Warframe","slug":"Warframe","link":"/tags/Warframe/"},{"name":"幻想","slug":"幻想","link":"/tags/%E5%B9%BB%E6%83%B3/"},{"name":"星际战甲","slug":"星际战甲","link":"/tags/%E6%98%9F%E9%99%85%E6%88%98%E7%94%B2/"},{"name":"游戏","slug":"游戏","link":"/tags/%E6%B8%B8%E6%88%8F/"},{"name":"永劫无间","slug":"永劫无间","link":"/tags/%E6%B0%B8%E5%8A%AB%E6%97%A0%E9%97%B4/"},{"name":"车万","slug":"车万","link":"/tags/%E8%BD%A6%E4%B8%87/"},{"name":"随笔","slug":"随笔","link":"/tags/%E9%9A%8F%E7%AC%94/"},{"name":"灵感","slug":"灵感","link":"/tags/%E7%81%B5%E6%84%9F/"},{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Flask","slug":"Flask","link":"/tags/Flask/"},{"name":"Django","slug":"Django","link":"/tags/Django/"},{"name":"ajax","slug":"ajax","link":"/tags/ajax/"},{"name":"DRF","slug":"DRF","link":"/tags/DRF/"},{"name":"MD5","slug":"MD5","link":"/tags/MD5/"},{"name":"ORM","slug":"ORM","link":"/tags/ORM/"},{"name":"Vue","slug":"Vue","link":"/tags/Vue/"},{"name":"跨域","slug":"跨域","link":"/tags/%E8%B7%A8%E5%9F%9F/"},{"name":"JavaScript","slug":"JavaScript","link":"/tags/JavaScript/"},{"name":"matplotlib","slug":"matplotlib","link":"/tags/matplotlib/"},{"name":"Pandas","slug":"Pandas","link":"/tags/Pandas/"},{"name":"Numpy","slug":"Numpy","link":"/tags/Numpy/"},{"name":"Spider","slug":"Spider","link":"/tags/Spider/"},{"name":"GIT","slug":"GIT","link":"/tags/GIT/"},{"name":"MongoDB","slug":"MongoDB","link":"/tags/MongoDB/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"}],"categories":[{"name":"Hexo","slug":"Hexo","link":"/categories/Hexo/"},{"name":"scala","slug":"scala","link":"/categories/scala/"},{"name":"设计稿","slug":"设计稿","link":"/categories/%E8%AE%BE%E8%AE%A1%E7%A8%BF/"},{"name":"永劫无间英雄设计活动","slug":"永劫无间英雄设计活动","link":"/categories/%E6%B0%B8%E5%8A%AB%E6%97%A0%E9%97%B4%E8%8B%B1%E9%9B%84%E8%AE%BE%E8%AE%A1%E6%B4%BB%E5%8A%A8/"},{"name":"草稿箱","slug":"草稿箱","link":"/categories/%E8%8D%89%E7%A8%BF%E7%AE%B1/"},{"name":"VTD","slug":"VTD","link":"/categories/VTD/"},{"name":"C++","slug":"C","link":"/categories/C/"},{"name":"日常","slug":"日常","link":"/categories/%E6%97%A5%E5%B8%B8/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Django","slug":"Django","link":"/categories/Django/"},{"name":"Django DRF","slug":"Django-DRF","link":"/categories/Django-DRF/"},{"name":"JavaScript","slug":"JavaScript","link":"/categories/JavaScript/"}]}